{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "887ab6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422e23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb2284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'Bahia - Consumo de Cimento (t)'\n",
    "split_index = 203 #Referente aos 230 anos de input  \n",
    "train_split = split_index + 1 - 12*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc652c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Bahia - value</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Bahia - IDH</th>\n",
       "      <th>Bahia - Produção de Cimento (t)</th>\n",
       "      <th>Bahia - PIB - Estadual</th>\n",
       "      <th>Bahia - PIB - Construção Civil</th>\n",
       "      <th>Bahia - PIB - Per Capita</th>\n",
       "      <th>Bahia - PIB - Preços de Mercado</th>\n",
       "      <th>Bahia - Desemprego</th>\n",
       "      <th>Bahia - Consumo de Cimento (t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-1</td>\n",
       "      <td>0.299858</td>\n",
       "      <td>0.724032</td>\n",
       "      <td>11.520143</td>\n",
       "      <td>1.639718</td>\n",
       "      <td>1.036534</td>\n",
       "      <td>1.772069e+08</td>\n",
       "      <td>7.330309e+06</td>\n",
       "      <td>0.969649</td>\n",
       "      <td>0.669899</td>\n",
       "      <td>39.798880</td>\n",
       "      <td>1.317344e+08</td>\n",
       "      <td>8.384593e+06</td>\n",
       "      <td>8.566149</td>\n",
       "      <td>1.216359e+08</td>\n",
       "      <td>8.348779</td>\n",
       "      <td>151.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-2</td>\n",
       "      <td>0.301903</td>\n",
       "      <td>0.690297</td>\n",
       "      <td>11.189862</td>\n",
       "      <td>1.378899</td>\n",
       "      <td>0.993449</td>\n",
       "      <td>1.773884e+08</td>\n",
       "      <td>7.335910e+06</td>\n",
       "      <td>0.950783</td>\n",
       "      <td>0.670210</td>\n",
       "      <td>39.480034</td>\n",
       "      <td>1.318964e+08</td>\n",
       "      <td>8.391946e+06</td>\n",
       "      <td>8.569210</td>\n",
       "      <td>1.216914e+08</td>\n",
       "      <td>8.342979</td>\n",
       "      <td>138.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-3</td>\n",
       "      <td>0.303709</td>\n",
       "      <td>0.669681</td>\n",
       "      <td>10.820792</td>\n",
       "      <td>1.924317</td>\n",
       "      <td>0.973020</td>\n",
       "      <td>1.775699e+08</td>\n",
       "      <td>7.341511e+06</td>\n",
       "      <td>0.938332</td>\n",
       "      <td>0.670521</td>\n",
       "      <td>39.400256</td>\n",
       "      <td>1.320584e+08</td>\n",
       "      <td>8.399299e+06</td>\n",
       "      <td>8.572270</td>\n",
       "      <td>1.217469e+08</td>\n",
       "      <td>8.337179</td>\n",
       "      <td>135.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-4</td>\n",
       "      <td>0.305311</td>\n",
       "      <td>0.660494</td>\n",
       "      <td>10.417840</td>\n",
       "      <td>1.331174</td>\n",
       "      <td>0.940489</td>\n",
       "      <td>1.777514e+08</td>\n",
       "      <td>7.347112e+06</td>\n",
       "      <td>0.926401</td>\n",
       "      <td>0.670831</td>\n",
       "      <td>39.417185</td>\n",
       "      <td>1.322204e+08</td>\n",
       "      <td>8.406652e+06</td>\n",
       "      <td>8.575331</td>\n",
       "      <td>1.218023e+08</td>\n",
       "      <td>8.331379</td>\n",
       "      <td>126.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-5</td>\n",
       "      <td>0.306860</td>\n",
       "      <td>0.648337</td>\n",
       "      <td>9.959690</td>\n",
       "      <td>1.736072</td>\n",
       "      <td>0.917493</td>\n",
       "      <td>1.779329e+08</td>\n",
       "      <td>7.352713e+06</td>\n",
       "      <td>0.951683</td>\n",
       "      <td>0.671142</td>\n",
       "      <td>39.479943</td>\n",
       "      <td>1.323824e+08</td>\n",
       "      <td>8.414005e+06</td>\n",
       "      <td>8.578392</td>\n",
       "      <td>1.218578e+08</td>\n",
       "      <td>8.325579</td>\n",
       "      <td>137.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2022-8</td>\n",
       "      <td>0.597113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.069163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>366.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2022-9</td>\n",
       "      <td>0.596178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.752943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>346.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2022-10</td>\n",
       "      <td>0.594662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.537361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>347.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2022-11</td>\n",
       "      <td>0.592436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.971241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2022-12</td>\n",
       "      <td>0.589305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.857141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310.845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Bahia - value  \\\n",
       "0       2003-1       0.299858   \n",
       "1       2003-2       0.301903   \n",
       "2       2003-3       0.303709   \n",
       "3       2003-4       0.305311   \n",
       "4       2003-5       0.306860   \n",
       "..         ...            ...   \n",
       "235     2022-8       0.597113   \n",
       "236     2022-9       0.596178   \n",
       "237    2022-10       0.594662   \n",
       "238    2022-11       0.592436   \n",
       "239    2022-12       0.589305   \n",
       "\n",
       "      IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "0                                            0.724032   \n",
       "1                                            0.690297   \n",
       "2                                            0.669681   \n",
       "3                                            0.660494   \n",
       "4                                            0.648337   \n",
       "..                                                ...   \n",
       "235                                               NaN   \n",
       "236                                               NaN   \n",
       "237                                               NaN   \n",
       "238                                               NaN   \n",
       "239                                               NaN   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI     População  \\\n",
       "0                        11.520143        1.639718  1.036534  1.772069e+08   \n",
       "1                        11.189862        1.378899  0.993449  1.773884e+08   \n",
       "2                        10.820792        1.924317  0.973020  1.775699e+08   \n",
       "3                        10.417840        1.331174  0.940489  1.777514e+08   \n",
       "4                         9.959690        1.736072  0.917493  1.779329e+08   \n",
       "..                             ...             ...       ...           ...   \n",
       "235                            NaN             NaN       NaN           NaN   \n",
       "236                            NaN             NaN       NaN           NaN   \n",
       "237                            NaN             NaN       NaN           NaN   \n",
       "238                            NaN             NaN       NaN           NaN   \n",
       "239                            NaN             NaN       NaN           NaN   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  Bahia - IDH  \\\n",
       "0                              7.330309e+06   0.969649     0.669899   \n",
       "1                              7.335910e+06   0.950783     0.670210   \n",
       "2                              7.341511e+06   0.938332     0.670521   \n",
       "3                              7.347112e+06   0.926401     0.670831   \n",
       "4                              7.352713e+06   0.951683     0.671142   \n",
       "..                                      ...        ...          ...   \n",
       "235                                     NaN        NaN          NaN   \n",
       "236                                     NaN        NaN          NaN   \n",
       "237                                     NaN        NaN          NaN   \n",
       "238                                     NaN        NaN          NaN   \n",
       "239                                     NaN        NaN          NaN   \n",
       "\n",
       "     Bahia - Produção de Cimento (t)  Bahia - PIB - Estadual  \\\n",
       "0                          39.798880            1.317344e+08   \n",
       "1                          39.480034            1.318964e+08   \n",
       "2                          39.400256            1.320584e+08   \n",
       "3                          39.417185            1.322204e+08   \n",
       "4                          39.479943            1.323824e+08   \n",
       "..                               ...                     ...   \n",
       "235                       106.069163                     NaN   \n",
       "236                       105.752943                     NaN   \n",
       "237                       105.537361                     NaN   \n",
       "238                       104.971241                     NaN   \n",
       "239                       104.857141                     NaN   \n",
       "\n",
       "     Bahia - PIB - Construção Civil  Bahia - PIB - Per Capita  \\\n",
       "0                      8.384593e+06                  8.566149   \n",
       "1                      8.391946e+06                  8.569210   \n",
       "2                      8.399299e+06                  8.572270   \n",
       "3                      8.406652e+06                  8.575331   \n",
       "4                      8.414005e+06                  8.578392   \n",
       "..                              ...                       ...   \n",
       "235                             NaN                       NaN   \n",
       "236                             NaN                       NaN   \n",
       "237                             NaN                       NaN   \n",
       "238                             NaN                       NaN   \n",
       "239                             NaN                       NaN   \n",
       "\n",
       "     Bahia - PIB - Preços de Mercado  Bahia - Desemprego  \\\n",
       "0                       1.216359e+08            8.348779   \n",
       "1                       1.216914e+08            8.342979   \n",
       "2                       1.217469e+08            8.337179   \n",
       "3                       1.218023e+08            8.331379   \n",
       "4                       1.218578e+08            8.325579   \n",
       "..                               ...                 ...   \n",
       "235                              NaN                 NaN   \n",
       "236                              NaN                 NaN   \n",
       "237                              NaN                 NaN   \n",
       "238                              NaN                 NaN   \n",
       "239                              NaN                 NaN   \n",
       "\n",
       "     Bahia - Consumo de Cimento (t)  \n",
       "0                           151.297  \n",
       "1                           138.707  \n",
       "2                           135.009  \n",
       "3                           126.554  \n",
       "4                           137.331  \n",
       "..                              ...  \n",
       "235                         366.305  \n",
       "236                         346.042  \n",
       "237                         347.901  \n",
       "238                         310.845  \n",
       "239                         310.845  \n",
       "\n",
       "[240 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('2003_mo_model_input_BA.csv')\n",
    "data = data[[col for col in data.columns if col != subject] + [subject]] #Seta consumo (target) para a coluna final\n",
    "data =data.drop([' NFSP - Fluxo Mensal (Milhões de reais)'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9a79f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bahia - value</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Bahia - IDH</th>\n",
       "      <th>Bahia - Produção de Cimento (t)</th>\n",
       "      <th>Bahia - PIB - Estadual</th>\n",
       "      <th>Bahia - PIB - Construção Civil</th>\n",
       "      <th>Bahia - PIB - Per Capita</th>\n",
       "      <th>Bahia - PIB - Preços de Mercado</th>\n",
       "      <th>Bahia - Desemprego</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.436732</td>\n",
       "      <td>2.782450</td>\n",
       "      <td>4.506880</td>\n",
       "      <td>2.067266</td>\n",
       "      <td>2.574314</td>\n",
       "      <td>-2.064648</td>\n",
       "      <td>-2.469876</td>\n",
       "      <td>3.184489</td>\n",
       "      <td>-2.195846</td>\n",
       "      <td>-1.793459</td>\n",
       "      <td>-1.766933</td>\n",
       "      <td>-0.695996</td>\n",
       "      <td>-2.315693</td>\n",
       "      <td>-2.238131</td>\n",
       "      <td>-0.944845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.404185</td>\n",
       "      <td>2.407943</td>\n",
       "      <td>4.328460</td>\n",
       "      <td>1.285816</td>\n",
       "      <td>2.334870</td>\n",
       "      <td>-2.037913</td>\n",
       "      <td>-2.431875</td>\n",
       "      <td>3.029073</td>\n",
       "      <td>-2.161313</td>\n",
       "      <td>-1.806616</td>\n",
       "      <td>-1.747868</td>\n",
       "      <td>-0.654222</td>\n",
       "      <td>-2.273375</td>\n",
       "      <td>-2.196868</td>\n",
       "      <td>-0.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.375442</td>\n",
       "      <td>2.179073</td>\n",
       "      <td>4.129086</td>\n",
       "      <td>2.919965</td>\n",
       "      <td>2.221334</td>\n",
       "      <td>-2.011179</td>\n",
       "      <td>-2.393874</td>\n",
       "      <td>2.926505</td>\n",
       "      <td>-2.126781</td>\n",
       "      <td>-1.809908</td>\n",
       "      <td>-1.728803</td>\n",
       "      <td>-0.612447</td>\n",
       "      <td>-2.231056</td>\n",
       "      <td>-2.155605</td>\n",
       "      <td>-0.948156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.349941</td>\n",
       "      <td>2.077086</td>\n",
       "      <td>3.911409</td>\n",
       "      <td>1.142823</td>\n",
       "      <td>2.040542</td>\n",
       "      <td>-1.984445</td>\n",
       "      <td>-2.355872</td>\n",
       "      <td>2.828220</td>\n",
       "      <td>-2.092248</td>\n",
       "      <td>-1.809210</td>\n",
       "      <td>-1.709739</td>\n",
       "      <td>-0.570673</td>\n",
       "      <td>-2.188738</td>\n",
       "      <td>-2.114342</td>\n",
       "      <td>-0.949811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.325290</td>\n",
       "      <td>1.942128</td>\n",
       "      <td>3.663912</td>\n",
       "      <td>2.355956</td>\n",
       "      <td>1.912744</td>\n",
       "      <td>-1.957710</td>\n",
       "      <td>-2.317871</td>\n",
       "      <td>3.036493</td>\n",
       "      <td>-2.057715</td>\n",
       "      <td>-1.806620</td>\n",
       "      <td>-1.690674</td>\n",
       "      <td>-0.528898</td>\n",
       "      <td>-2.146420</td>\n",
       "      <td>-2.073079</td>\n",
       "      <td>-0.951467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.851312</td>\n",
       "      <td>-0.689886</td>\n",
       "      <td>-0.724085</td>\n",
       "      <td>-1.189161</td>\n",
       "      <td>3.148408</td>\n",
       "      <td>1.378950</td>\n",
       "      <td>0.117681</td>\n",
       "      <td>0.370628</td>\n",
       "      <td>1.041952</td>\n",
       "      <td>0.619029</td>\n",
       "      <td>0.865987</td>\n",
       "      <td>-1.416165</td>\n",
       "      <td>0.555618</td>\n",
       "      <td>0.463875</td>\n",
       "      <td>1.107985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1.873607</td>\n",
       "      <td>-0.441954</td>\n",
       "      <td>-0.736434</td>\n",
       "      <td>-1.296499</td>\n",
       "      <td>3.219670</td>\n",
       "      <td>1.391539</td>\n",
       "      <td>0.092456</td>\n",
       "      <td>0.427775</td>\n",
       "      <td>1.026783</td>\n",
       "      <td>0.635352</td>\n",
       "      <td>0.848481</td>\n",
       "      <td>-1.404857</td>\n",
       "      <td>0.531641</td>\n",
       "      <td>0.443334</td>\n",
       "      <td>1.106555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1.901718</td>\n",
       "      <td>-0.132782</td>\n",
       "      <td>-0.738433</td>\n",
       "      <td>-1.444029</td>\n",
       "      <td>3.421082</td>\n",
       "      <td>1.404128</td>\n",
       "      <td>0.067231</td>\n",
       "      <td>0.538287</td>\n",
       "      <td>1.011615</td>\n",
       "      <td>0.652040</td>\n",
       "      <td>0.830975</td>\n",
       "      <td>-1.393550</td>\n",
       "      <td>0.507664</td>\n",
       "      <td>0.422793</td>\n",
       "      <td>1.105126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1.928136</td>\n",
       "      <td>0.084061</td>\n",
       "      <td>-0.738236</td>\n",
       "      <td>-1.444143</td>\n",
       "      <td>3.373840</td>\n",
       "      <td>1.416717</td>\n",
       "      <td>0.042006</td>\n",
       "      <td>0.666156</td>\n",
       "      <td>0.996447</td>\n",
       "      <td>0.638794</td>\n",
       "      <td>0.813469</td>\n",
       "      <td>-1.382243</td>\n",
       "      <td>0.483687</td>\n",
       "      <td>0.402252</td>\n",
       "      <td>1.103697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1.972896</td>\n",
       "      <td>0.162956</td>\n",
       "      <td>-0.746642</td>\n",
       "      <td>-1.717057</td>\n",
       "      <td>3.497364</td>\n",
       "      <td>1.429307</td>\n",
       "      <td>0.016781</td>\n",
       "      <td>0.767681</td>\n",
       "      <td>0.981278</td>\n",
       "      <td>0.637852</td>\n",
       "      <td>0.795963</td>\n",
       "      <td>-1.370936</td>\n",
       "      <td>0.459711</td>\n",
       "      <td>0.381711</td>\n",
       "      <td>1.102267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bahia - value   IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "0        -1.436732                                          2.782450   \n",
       "1        -1.404185                                          2.407943   \n",
       "2        -1.375442                                          2.179073   \n",
       "3        -1.349941                                          2.077086   \n",
       "4        -1.325290                                          1.942128   \n",
       "..             ...                                               ...   \n",
       "199       1.851312                                         -0.689886   \n",
       "200       1.873607                                         -0.441954   \n",
       "201       1.901718                                         -0.132782   \n",
       "202       1.928136                                          0.084061   \n",
       "203       1.972896                                          0.162956   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI  População  \\\n",
       "0                         4.506880        2.067266  2.574314  -2.064648   \n",
       "1                         4.328460        1.285816  2.334870  -2.037913   \n",
       "2                         4.129086        2.919965  2.221334  -2.011179   \n",
       "3                         3.911409        1.142823  2.040542  -1.984445   \n",
       "4                         3.663912        2.355956  1.912744  -1.957710   \n",
       "..                             ...             ...       ...        ...   \n",
       "199                      -0.724085       -1.189161  3.148408   1.378950   \n",
       "200                      -0.736434       -1.296499  3.219670   1.391539   \n",
       "201                      -0.738433       -1.444029  3.421082   1.404128   \n",
       "202                      -0.738236       -1.444143  3.373840   1.416717   \n",
       "203                      -0.746642       -1.717057  3.497364   1.429307   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  Bahia - IDH  \\\n",
       "0                                 -2.469876   3.184489    -2.195846   \n",
       "1                                 -2.431875   3.029073    -2.161313   \n",
       "2                                 -2.393874   2.926505    -2.126781   \n",
       "3                                 -2.355872   2.828220    -2.092248   \n",
       "4                                 -2.317871   3.036493    -2.057715   \n",
       "..                                      ...        ...          ...   \n",
       "199                                0.117681   0.370628     1.041952   \n",
       "200                                0.092456   0.427775     1.026783   \n",
       "201                                0.067231   0.538287     1.011615   \n",
       "202                                0.042006   0.666156     0.996447   \n",
       "203                                0.016781   0.767681     0.981278   \n",
       "\n",
       "     Bahia - Produção de Cimento (t)  Bahia - PIB - Estadual  \\\n",
       "0                          -1.793459               -1.766933   \n",
       "1                          -1.806616               -1.747868   \n",
       "2                          -1.809908               -1.728803   \n",
       "3                          -1.809210               -1.709739   \n",
       "4                          -1.806620               -1.690674   \n",
       "..                               ...                     ...   \n",
       "199                         0.619029                0.865987   \n",
       "200                         0.635352                0.848481   \n",
       "201                         0.652040                0.830975   \n",
       "202                         0.638794                0.813469   \n",
       "203                         0.637852                0.795963   \n",
       "\n",
       "     Bahia - PIB - Construção Civil  Bahia - PIB - Per Capita  \\\n",
       "0                         -0.695996                 -2.315693   \n",
       "1                         -0.654222                 -2.273375   \n",
       "2                         -0.612447                 -2.231056   \n",
       "3                         -0.570673                 -2.188738   \n",
       "4                         -0.528898                 -2.146420   \n",
       "..                              ...                       ...   \n",
       "199                       -1.416165                  0.555618   \n",
       "200                       -1.404857                  0.531641   \n",
       "201                       -1.393550                  0.507664   \n",
       "202                       -1.382243                  0.483687   \n",
       "203                       -1.370936                  0.459711   \n",
       "\n",
       "     Bahia - PIB - Preços de Mercado  Bahia - Desemprego  \n",
       "0                          -2.238131           -0.944845  \n",
       "1                          -2.196868           -0.946500  \n",
       "2                          -2.155605           -0.948156  \n",
       "3                          -2.114342           -0.949811  \n",
       "4                          -2.073079           -0.951467  \n",
       "..                               ...                 ...  \n",
       "199                         0.463875            1.107985  \n",
       "200                         0.443334            1.106555  \n",
       "201                         0.422793            1.105126  \n",
       "202                         0.402252            1.103697  \n",
       "203                         0.381711            1.102267  \n",
       "\n",
       "[204 rows x 15 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = data.iloc[:split_index + 1,1:-1]\n",
    "# input_data = data.iloc[:split_index + 1,:]\n",
    "input_data = (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb83d26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      138.707\n",
       "1      135.009\n",
       "2      126.554\n",
       "3      137.331\n",
       "4      118.680\n",
       "        ...   \n",
       "235    346.042\n",
       "236    347.901\n",
       "237    310.845\n",
       "238    310.845\n",
       "239        NaN\n",
       "Name: Bahia - Consumo de Cimento (t), Length: 240, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shift para prever futuro e não presente\n",
    "target_data = data[subject].shift(-1)\n",
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bdb2b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bahia - value</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Bahia - IDH</th>\n",
       "      <th>Bahia - Produção de Cimento (t)</th>\n",
       "      <th>Bahia - PIB - Estadual</th>\n",
       "      <th>Bahia - PIB - Construção Civil</th>\n",
       "      <th>Bahia - PIB - Per Capita</th>\n",
       "      <th>Bahia - PIB - Preços de Mercado</th>\n",
       "      <th>Bahia - Desemprego</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.436732</td>\n",
       "      <td>2.782450</td>\n",
       "      <td>4.506880</td>\n",
       "      <td>2.067266</td>\n",
       "      <td>2.574314</td>\n",
       "      <td>-2.064648</td>\n",
       "      <td>-2.469876</td>\n",
       "      <td>3.184489</td>\n",
       "      <td>-2.195846</td>\n",
       "      <td>-1.793459</td>\n",
       "      <td>-1.766933</td>\n",
       "      <td>-0.695996</td>\n",
       "      <td>-2.315693</td>\n",
       "      <td>-2.238131</td>\n",
       "      <td>-0.944845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.404185</td>\n",
       "      <td>2.407943</td>\n",
       "      <td>4.328460</td>\n",
       "      <td>1.285816</td>\n",
       "      <td>2.334870</td>\n",
       "      <td>-2.037913</td>\n",
       "      <td>-2.431875</td>\n",
       "      <td>3.029073</td>\n",
       "      <td>-2.161313</td>\n",
       "      <td>-1.806616</td>\n",
       "      <td>-1.747868</td>\n",
       "      <td>-0.654222</td>\n",
       "      <td>-2.273375</td>\n",
       "      <td>-2.196868</td>\n",
       "      <td>-0.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.375442</td>\n",
       "      <td>2.179073</td>\n",
       "      <td>4.129086</td>\n",
       "      <td>2.919965</td>\n",
       "      <td>2.221334</td>\n",
       "      <td>-2.011179</td>\n",
       "      <td>-2.393874</td>\n",
       "      <td>2.926505</td>\n",
       "      <td>-2.126781</td>\n",
       "      <td>-1.809908</td>\n",
       "      <td>-1.728803</td>\n",
       "      <td>-0.612447</td>\n",
       "      <td>-2.231056</td>\n",
       "      <td>-2.155605</td>\n",
       "      <td>-0.948156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.349941</td>\n",
       "      <td>2.077086</td>\n",
       "      <td>3.911409</td>\n",
       "      <td>1.142823</td>\n",
       "      <td>2.040542</td>\n",
       "      <td>-1.984445</td>\n",
       "      <td>-2.355872</td>\n",
       "      <td>2.828220</td>\n",
       "      <td>-2.092248</td>\n",
       "      <td>-1.809210</td>\n",
       "      <td>-1.709739</td>\n",
       "      <td>-0.570673</td>\n",
       "      <td>-2.188738</td>\n",
       "      <td>-2.114342</td>\n",
       "      <td>-0.949811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.325290</td>\n",
       "      <td>1.942128</td>\n",
       "      <td>3.663912</td>\n",
       "      <td>2.355956</td>\n",
       "      <td>1.912744</td>\n",
       "      <td>-1.957710</td>\n",
       "      <td>-2.317871</td>\n",
       "      <td>3.036493</td>\n",
       "      <td>-2.057715</td>\n",
       "      <td>-1.806620</td>\n",
       "      <td>-1.690674</td>\n",
       "      <td>-0.528898</td>\n",
       "      <td>-2.146420</td>\n",
       "      <td>-2.073079</td>\n",
       "      <td>-0.951467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.005306</td>\n",
       "      <td>-0.742853</td>\n",
       "      <td>-0.619453</td>\n",
       "      <td>0.617009</td>\n",
       "      <td>-0.970725</td>\n",
       "      <td>0.980624</td>\n",
       "      <td>0.764972</td>\n",
       "      <td>-1.322806</td>\n",
       "      <td>1.339087</td>\n",
       "      <td>0.469766</td>\n",
       "      <td>1.163205</td>\n",
       "      <td>-1.340370</td>\n",
       "      <td>1.005749</td>\n",
       "      <td>1.008566</td>\n",
       "      <td>1.160245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.029532</td>\n",
       "      <td>-0.787367</td>\n",
       "      <td>-0.611176</td>\n",
       "      <td>0.459632</td>\n",
       "      <td>-0.841670</td>\n",
       "      <td>0.991608</td>\n",
       "      <td>0.753473</td>\n",
       "      <td>-1.355523</td>\n",
       "      <td>1.336451</td>\n",
       "      <td>0.457435</td>\n",
       "      <td>1.164562</td>\n",
       "      <td>-1.364998</td>\n",
       "      <td>1.008863</td>\n",
       "      <td>1.002613</td>\n",
       "      <td>1.155001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.057198</td>\n",
       "      <td>-0.757940</td>\n",
       "      <td>-0.599627</td>\n",
       "      <td>0.493659</td>\n",
       "      <td>-0.838453</td>\n",
       "      <td>1.002592</td>\n",
       "      <td>0.741973</td>\n",
       "      <td>-1.376175</td>\n",
       "      <td>1.333816</td>\n",
       "      <td>0.454716</td>\n",
       "      <td>1.165920</td>\n",
       "      <td>-1.389626</td>\n",
       "      <td>1.011977</td>\n",
       "      <td>0.996661</td>\n",
       "      <td>1.149757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.082730</td>\n",
       "      <td>-0.917469</td>\n",
       "      <td>-0.589237</td>\n",
       "      <td>0.617024</td>\n",
       "      <td>-1.006001</td>\n",
       "      <td>1.013577</td>\n",
       "      <td>0.730473</td>\n",
       "      <td>-1.414146</td>\n",
       "      <td>1.331180</td>\n",
       "      <td>0.446427</td>\n",
       "      <td>1.167277</td>\n",
       "      <td>-1.414254</td>\n",
       "      <td>1.015092</td>\n",
       "      <td>0.990708</td>\n",
       "      <td>1.144512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.112662</td>\n",
       "      <td>-0.980136</td>\n",
       "      <td>-0.584931</td>\n",
       "      <td>0.302239</td>\n",
       "      <td>-1.088872</td>\n",
       "      <td>1.024561</td>\n",
       "      <td>0.718973</td>\n",
       "      <td>-1.444336</td>\n",
       "      <td>1.328544</td>\n",
       "      <td>0.406571</td>\n",
       "      <td>1.168635</td>\n",
       "      <td>-1.438882</td>\n",
       "      <td>1.018206</td>\n",
       "      <td>0.984756</td>\n",
       "      <td>1.139268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bahia - value   IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "0        -1.436732                                          2.782450   \n",
       "1        -1.404185                                          2.407943   \n",
       "2        -1.375442                                          2.179073   \n",
       "3        -1.349941                                          2.077086   \n",
       "4        -1.325290                                          1.942128   \n",
       "..             ...                                               ...   \n",
       "163       1.005306                                         -0.742853   \n",
       "164       1.029532                                         -0.787367   \n",
       "165       1.057198                                         -0.757940   \n",
       "166       1.082730                                         -0.917469   \n",
       "167       1.112662                                         -0.980136   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI  População  \\\n",
       "0                         4.506880        2.067266  2.574314  -2.064648   \n",
       "1                         4.328460        1.285816  2.334870  -2.037913   \n",
       "2                         4.129086        2.919965  2.221334  -2.011179   \n",
       "3                         3.911409        1.142823  2.040542  -1.984445   \n",
       "4                         3.663912        2.355956  1.912744  -1.957710   \n",
       "..                             ...             ...       ...        ...   \n",
       "163                      -0.619453        0.617009 -0.970725   0.980624   \n",
       "164                      -0.611176        0.459632 -0.841670   0.991608   \n",
       "165                      -0.599627        0.493659 -0.838453   1.002592   \n",
       "166                      -0.589237        0.617024 -1.006001   1.013577   \n",
       "167                      -0.584931        0.302239 -1.088872   1.024561   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  Bahia - IDH  \\\n",
       "0                                 -2.469876   3.184489    -2.195846   \n",
       "1                                 -2.431875   3.029073    -2.161313   \n",
       "2                                 -2.393874   2.926505    -2.126781   \n",
       "3                                 -2.355872   2.828220    -2.092248   \n",
       "4                                 -2.317871   3.036493    -2.057715   \n",
       "..                                      ...        ...          ...   \n",
       "163                                0.764972  -1.322806     1.339087   \n",
       "164                                0.753473  -1.355523     1.336451   \n",
       "165                                0.741973  -1.376175     1.333816   \n",
       "166                                0.730473  -1.414146     1.331180   \n",
       "167                                0.718973  -1.444336     1.328544   \n",
       "\n",
       "     Bahia - Produção de Cimento (t)  Bahia - PIB - Estadual  \\\n",
       "0                          -1.793459               -1.766933   \n",
       "1                          -1.806616               -1.747868   \n",
       "2                          -1.809908               -1.728803   \n",
       "3                          -1.809210               -1.709739   \n",
       "4                          -1.806620               -1.690674   \n",
       "..                               ...                     ...   \n",
       "163                         0.469766                1.163205   \n",
       "164                         0.457435                1.164562   \n",
       "165                         0.454716                1.165920   \n",
       "166                         0.446427                1.167277   \n",
       "167                         0.406571                1.168635   \n",
       "\n",
       "     Bahia - PIB - Construção Civil  Bahia - PIB - Per Capita  \\\n",
       "0                         -0.695996                 -2.315693   \n",
       "1                         -0.654222                 -2.273375   \n",
       "2                         -0.612447                 -2.231056   \n",
       "3                         -0.570673                 -2.188738   \n",
       "4                         -0.528898                 -2.146420   \n",
       "..                              ...                       ...   \n",
       "163                       -1.340370                  1.005749   \n",
       "164                       -1.364998                  1.008863   \n",
       "165                       -1.389626                  1.011977   \n",
       "166                       -1.414254                  1.015092   \n",
       "167                       -1.438882                  1.018206   \n",
       "\n",
       "     Bahia - PIB - Preços de Mercado  Bahia - Desemprego  \n",
       "0                          -2.238131           -0.944845  \n",
       "1                          -2.196868           -0.946500  \n",
       "2                          -2.155605           -0.948156  \n",
       "3                          -2.114342           -0.949811  \n",
       "4                          -2.073079           -0.951467  \n",
       "..                               ...                 ...  \n",
       "163                         1.008566            1.160245  \n",
       "164                         1.002613            1.155001  \n",
       "165                         0.996661            1.149757  \n",
       "166                         0.990708            1.144512  \n",
       "167                         0.984756            1.139268  \n",
       "\n",
       "[168 rows x 15 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input para treinamento\n",
    "train_input = input_data.iloc[:train_split]\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a9e1f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      138.707\n",
       "1      135.009\n",
       "2      126.554\n",
       "3      137.331\n",
       "4      118.680\n",
       "        ...   \n",
       "163    280.074\n",
       "164    277.938\n",
       "165    275.802\n",
       "166    273.666\n",
       "167    271.530\n",
       "Name: Bahia - Consumo de Cimento (t), Length: 168, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alvo para treinamento\n",
    "train_target = target_data.iloc[:train_split]\n",
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b094360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bahia - value</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Bahia - IDH</th>\n",
       "      <th>Bahia - Produção de Cimento (t)</th>\n",
       "      <th>Bahia - PIB - Estadual</th>\n",
       "      <th>Bahia - PIB - Construção Civil</th>\n",
       "      <th>Bahia - PIB - Per Capita</th>\n",
       "      <th>Bahia - PIB - Preços de Mercado</th>\n",
       "      <th>Bahia - Desemprego</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.142199</td>\n",
       "      <td>-1.032458</td>\n",
       "      <td>-0.585256</td>\n",
       "      <td>-0.505289</td>\n",
       "      <td>-1.202612</td>\n",
       "      <td>1.035545</td>\n",
       "      <td>0.707473</td>\n",
       "      <td>-1.446117</td>\n",
       "      <td>1.325909</td>\n",
       "      <td>0.407445</td>\n",
       "      <td>1.169992</td>\n",
       "      <td>-1.463511</td>\n",
       "      <td>1.021321</td>\n",
       "      <td>0.978804</td>\n",
       "      <td>1.134024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.169669</td>\n",
       "      <td>-1.077854</td>\n",
       "      <td>-0.581056</td>\n",
       "      <td>-0.373612</td>\n",
       "      <td>-1.139987</td>\n",
       "      <td>1.046449</td>\n",
       "      <td>0.692399</td>\n",
       "      <td>-1.526274</td>\n",
       "      <td>1.318823</td>\n",
       "      <td>0.400025</td>\n",
       "      <td>1.166430</td>\n",
       "      <td>-1.470219</td>\n",
       "      <td>1.023238</td>\n",
       "      <td>0.968444</td>\n",
       "      <td>1.133050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.197459</td>\n",
       "      <td>-1.036724</td>\n",
       "      <td>-0.574769</td>\n",
       "      <td>-0.291560</td>\n",
       "      <td>-1.053534</td>\n",
       "      <td>1.057352</td>\n",
       "      <td>0.677324</td>\n",
       "      <td>-1.610341</td>\n",
       "      <td>1.311738</td>\n",
       "      <td>0.398398</td>\n",
       "      <td>1.162868</td>\n",
       "      <td>-1.476927</td>\n",
       "      <td>1.025155</td>\n",
       "      <td>0.958085</td>\n",
       "      <td>1.132076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.231686</td>\n",
       "      <td>-1.013400</td>\n",
       "      <td>-0.567021</td>\n",
       "      <td>-0.450118</td>\n",
       "      <td>-0.932974</td>\n",
       "      <td>1.068256</td>\n",
       "      <td>0.662249</td>\n",
       "      <td>-1.601301</td>\n",
       "      <td>1.304652</td>\n",
       "      <td>0.379182</td>\n",
       "      <td>1.159306</td>\n",
       "      <td>-1.483635</td>\n",
       "      <td>1.027072</td>\n",
       "      <td>0.947726</td>\n",
       "      <td>1.131102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.268038</td>\n",
       "      <td>-1.059228</td>\n",
       "      <td>-0.557842</td>\n",
       "      <td>-0.909054</td>\n",
       "      <td>-0.763756</td>\n",
       "      <td>1.079159</td>\n",
       "      <td>0.647175</td>\n",
       "      <td>-1.618284</td>\n",
       "      <td>1.297567</td>\n",
       "      <td>0.384352</td>\n",
       "      <td>1.155744</td>\n",
       "      <td>-1.490343</td>\n",
       "      <td>1.028989</td>\n",
       "      <td>0.937367</td>\n",
       "      <td>1.130127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.299588</td>\n",
       "      <td>-1.170032</td>\n",
       "      <td>-0.548012</td>\n",
       "      <td>0.387442</td>\n",
       "      <td>-0.644652</td>\n",
       "      <td>1.090063</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>-1.602348</td>\n",
       "      <td>1.290482</td>\n",
       "      <td>0.367059</td>\n",
       "      <td>1.152182</td>\n",
       "      <td>-1.497051</td>\n",
       "      <td>1.030906</td>\n",
       "      <td>0.927007</td>\n",
       "      <td>1.129153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.338328</td>\n",
       "      <td>-1.115621</td>\n",
       "      <td>-0.537244</td>\n",
       "      <td>-0.937589</td>\n",
       "      <td>-0.541601</td>\n",
       "      <td>1.100966</td>\n",
       "      <td>0.617026</td>\n",
       "      <td>-1.686659</td>\n",
       "      <td>1.283396</td>\n",
       "      <td>0.363065</td>\n",
       "      <td>1.148620</td>\n",
       "      <td>-1.503759</td>\n",
       "      <td>1.032823</td>\n",
       "      <td>0.916648</td>\n",
       "      <td>1.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.376421</td>\n",
       "      <td>-1.182159</td>\n",
       "      <td>-0.524149</td>\n",
       "      <td>-1.212466</td>\n",
       "      <td>-0.565747</td>\n",
       "      <td>1.111870</td>\n",
       "      <td>0.601951</td>\n",
       "      <td>-1.688184</td>\n",
       "      <td>1.276311</td>\n",
       "      <td>0.360415</td>\n",
       "      <td>1.145058</td>\n",
       "      <td>-1.510467</td>\n",
       "      <td>1.034740</td>\n",
       "      <td>0.906289</td>\n",
       "      <td>1.127205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.409717</td>\n",
       "      <td>-1.270623</td>\n",
       "      <td>-0.519647</td>\n",
       "      <td>-0.499856</td>\n",
       "      <td>-0.536256</td>\n",
       "      <td>1.122773</td>\n",
       "      <td>0.586877</td>\n",
       "      <td>-1.692173</td>\n",
       "      <td>1.269226</td>\n",
       "      <td>0.359113</td>\n",
       "      <td>1.141496</td>\n",
       "      <td>-1.517175</td>\n",
       "      <td>1.036658</td>\n",
       "      <td>0.895930</td>\n",
       "      <td>1.126231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.441251</td>\n",
       "      <td>-1.321931</td>\n",
       "      <td>-0.510389</td>\n",
       "      <td>-0.120028</td>\n",
       "      <td>-0.544036</td>\n",
       "      <td>1.133677</td>\n",
       "      <td>0.571802</td>\n",
       "      <td>-1.690760</td>\n",
       "      <td>1.262140</td>\n",
       "      <td>0.367682</td>\n",
       "      <td>1.137934</td>\n",
       "      <td>-1.523883</td>\n",
       "      <td>1.038575</td>\n",
       "      <td>0.885571</td>\n",
       "      <td>1.125257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.453870</td>\n",
       "      <td>-1.339532</td>\n",
       "      <td>-0.493794</td>\n",
       "      <td>0.302226</td>\n",
       "      <td>-0.455816</td>\n",
       "      <td>1.144580</td>\n",
       "      <td>0.556728</td>\n",
       "      <td>-1.754852</td>\n",
       "      <td>1.255055</td>\n",
       "      <td>0.362771</td>\n",
       "      <td>1.134372</td>\n",
       "      <td>-1.530591</td>\n",
       "      <td>1.040492</td>\n",
       "      <td>0.875211</td>\n",
       "      <td>1.124283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.466416</td>\n",
       "      <td>-1.177993</td>\n",
       "      <td>-0.482543</td>\n",
       "      <td>-1.133998</td>\n",
       "      <td>-0.341841</td>\n",
       "      <td>1.155483</td>\n",
       "      <td>0.541653</td>\n",
       "      <td>-1.791166</td>\n",
       "      <td>1.247969</td>\n",
       "      <td>0.368449</td>\n",
       "      <td>1.130810</td>\n",
       "      <td>-1.537299</td>\n",
       "      <td>1.042409</td>\n",
       "      <td>0.864852</td>\n",
       "      <td>1.123309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.480970</td>\n",
       "      <td>-1.259339</td>\n",
       "      <td>-0.476661</td>\n",
       "      <td>-1.443985</td>\n",
       "      <td>-0.404575</td>\n",
       "      <td>1.166387</td>\n",
       "      <td>0.526579</td>\n",
       "      <td>-1.758016</td>\n",
       "      <td>1.240884</td>\n",
       "      <td>0.387991</td>\n",
       "      <td>1.127248</td>\n",
       "      <td>-1.544007</td>\n",
       "      <td>1.044326</td>\n",
       "      <td>0.854493</td>\n",
       "      <td>1.122335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.495507</td>\n",
       "      <td>-1.295690</td>\n",
       "      <td>-0.477030</td>\n",
       "      <td>-1.094983</td>\n",
       "      <td>-0.459546</td>\n",
       "      <td>1.176757</td>\n",
       "      <td>0.507219</td>\n",
       "      <td>-1.773309</td>\n",
       "      <td>1.233154</td>\n",
       "      <td>0.400455</td>\n",
       "      <td>1.115688</td>\n",
       "      <td>-1.539950</td>\n",
       "      <td>1.017587</td>\n",
       "      <td>0.833924</td>\n",
       "      <td>1.121973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.503196</td>\n",
       "      <td>-1.375339</td>\n",
       "      <td>-0.480370</td>\n",
       "      <td>-1.222235</td>\n",
       "      <td>-0.278469</td>\n",
       "      <td>1.187127</td>\n",
       "      <td>0.487859</td>\n",
       "      <td>-1.769159</td>\n",
       "      <td>1.225425</td>\n",
       "      <td>0.405571</td>\n",
       "      <td>1.104128</td>\n",
       "      <td>-1.535892</td>\n",
       "      <td>0.990848</td>\n",
       "      <td>0.813354</td>\n",
       "      <td>1.121611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.512115</td>\n",
       "      <td>-1.486072</td>\n",
       "      <td>-0.485497</td>\n",
       "      <td>-1.257837</td>\n",
       "      <td>-0.319575</td>\n",
       "      <td>1.197496</td>\n",
       "      <td>0.468498</td>\n",
       "      <td>-1.788744</td>\n",
       "      <td>1.217695</td>\n",
       "      <td>0.398503</td>\n",
       "      <td>1.092568</td>\n",
       "      <td>-1.531834</td>\n",
       "      <td>0.964108</td>\n",
       "      <td>0.792785</td>\n",
       "      <td>1.121249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.525662</td>\n",
       "      <td>-1.642545</td>\n",
       "      <td>-0.493698</td>\n",
       "      <td>-1.292177</td>\n",
       "      <td>-0.288531</td>\n",
       "      <td>1.207866</td>\n",
       "      <td>0.449138</td>\n",
       "      <td>-1.818878</td>\n",
       "      <td>1.209966</td>\n",
       "      <td>0.408478</td>\n",
       "      <td>1.081008</td>\n",
       "      <td>-1.527776</td>\n",
       "      <td>0.937369</td>\n",
       "      <td>0.772216</td>\n",
       "      <td>1.120887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.523051</td>\n",
       "      <td>-1.696141</td>\n",
       "      <td>-0.503979</td>\n",
       "      <td>-1.370259</td>\n",
       "      <td>-0.279873</td>\n",
       "      <td>1.218236</td>\n",
       "      <td>0.429778</td>\n",
       "      <td>-1.814437</td>\n",
       "      <td>1.202236</td>\n",
       "      <td>0.451828</td>\n",
       "      <td>1.069448</td>\n",
       "      <td>-1.523719</td>\n",
       "      <td>0.910630</td>\n",
       "      <td>0.751646</td>\n",
       "      <td>1.120525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.525659</td>\n",
       "      <td>-1.924292</td>\n",
       "      <td>-0.519678</td>\n",
       "      <td>-1.295050</td>\n",
       "      <td>-0.169438</td>\n",
       "      <td>1.228606</td>\n",
       "      <td>0.410418</td>\n",
       "      <td>-1.797383</td>\n",
       "      <td>1.194507</td>\n",
       "      <td>0.469481</td>\n",
       "      <td>1.057888</td>\n",
       "      <td>-1.519661</td>\n",
       "      <td>0.883891</td>\n",
       "      <td>0.731077</td>\n",
       "      <td>1.120163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.537596</td>\n",
       "      <td>-1.972576</td>\n",
       "      <td>-0.537386</td>\n",
       "      <td>-1.148806</td>\n",
       "      <td>0.237913</td>\n",
       "      <td>1.238976</td>\n",
       "      <td>0.391058</td>\n",
       "      <td>-1.790206</td>\n",
       "      <td>1.186777</td>\n",
       "      <td>0.484754</td>\n",
       "      <td>1.046328</td>\n",
       "      <td>-1.515603</td>\n",
       "      <td>0.857152</td>\n",
       "      <td>0.710508</td>\n",
       "      <td>1.119801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.551512</td>\n",
       "      <td>-1.832285</td>\n",
       "      <td>-0.553461</td>\n",
       "      <td>-1.222716</td>\n",
       "      <td>0.559735</td>\n",
       "      <td>1.249345</td>\n",
       "      <td>0.371698</td>\n",
       "      <td>-1.629944</td>\n",
       "      <td>1.179048</td>\n",
       "      <td>0.486517</td>\n",
       "      <td>1.034768</td>\n",
       "      <td>-1.511546</td>\n",
       "      <td>0.830413</td>\n",
       "      <td>0.689938</td>\n",
       "      <td>1.119439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.571909</td>\n",
       "      <td>-1.767518</td>\n",
       "      <td>-0.577640</td>\n",
       "      <td>-1.370262</td>\n",
       "      <td>0.802696</td>\n",
       "      <td>1.259715</td>\n",
       "      <td>0.352338</td>\n",
       "      <td>-1.383366</td>\n",
       "      <td>1.171318</td>\n",
       "      <td>0.500299</td>\n",
       "      <td>1.023208</td>\n",
       "      <td>-1.507488</td>\n",
       "      <td>0.803674</td>\n",
       "      <td>0.669369</td>\n",
       "      <td>1.119077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.589949</td>\n",
       "      <td>-1.688436</td>\n",
       "      <td>-0.606406</td>\n",
       "      <td>-1.283594</td>\n",
       "      <td>1.106124</td>\n",
       "      <td>1.270085</td>\n",
       "      <td>0.332977</td>\n",
       "      <td>-1.227161</td>\n",
       "      <td>1.163589</td>\n",
       "      <td>0.498868</td>\n",
       "      <td>1.011648</td>\n",
       "      <td>-1.503430</td>\n",
       "      <td>0.776935</td>\n",
       "      <td>0.648800</td>\n",
       "      <td>1.118715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1.614723</td>\n",
       "      <td>-1.350739</td>\n",
       "      <td>-0.629137</td>\n",
       "      <td>-1.449457</td>\n",
       "      <td>1.364723</td>\n",
       "      <td>1.280455</td>\n",
       "      <td>0.313617</td>\n",
       "      <td>-1.127517</td>\n",
       "      <td>1.155859</td>\n",
       "      <td>0.499306</td>\n",
       "      <td>1.000088</td>\n",
       "      <td>-1.499373</td>\n",
       "      <td>0.750195</td>\n",
       "      <td>0.628230</td>\n",
       "      <td>1.118353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.641795</td>\n",
       "      <td>-1.335646</td>\n",
       "      <td>-0.644481</td>\n",
       "      <td>-1.429372</td>\n",
       "      <td>1.795882</td>\n",
       "      <td>1.290825</td>\n",
       "      <td>0.294257</td>\n",
       "      <td>-0.993281</td>\n",
       "      <td>1.148130</td>\n",
       "      <td>0.540294</td>\n",
       "      <td>0.988529</td>\n",
       "      <td>-1.495315</td>\n",
       "      <td>0.723456</td>\n",
       "      <td>0.607661</td>\n",
       "      <td>1.117991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1.672065</td>\n",
       "      <td>-1.219493</td>\n",
       "      <td>-0.656384</td>\n",
       "      <td>-1.694292</td>\n",
       "      <td>2.156500</td>\n",
       "      <td>1.303414</td>\n",
       "      <td>0.269032</td>\n",
       "      <td>-0.748672</td>\n",
       "      <td>1.132961</td>\n",
       "      <td>0.528474</td>\n",
       "      <td>0.971023</td>\n",
       "      <td>-1.484008</td>\n",
       "      <td>0.699479</td>\n",
       "      <td>0.587120</td>\n",
       "      <td>1.116561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1.708758</td>\n",
       "      <td>-1.109104</td>\n",
       "      <td>-0.667513</td>\n",
       "      <td>-1.370264</td>\n",
       "      <td>2.328120</td>\n",
       "      <td>1.316003</td>\n",
       "      <td>0.243807</td>\n",
       "      <td>-0.529379</td>\n",
       "      <td>1.117793</td>\n",
       "      <td>0.538242</td>\n",
       "      <td>0.953517</td>\n",
       "      <td>-1.472700</td>\n",
       "      <td>0.675503</td>\n",
       "      <td>0.566579</td>\n",
       "      <td>1.115132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1.749791</td>\n",
       "      <td>-1.210267</td>\n",
       "      <td>-0.680761</td>\n",
       "      <td>-1.222734</td>\n",
       "      <td>2.527565</td>\n",
       "      <td>1.328592</td>\n",
       "      <td>0.218582</td>\n",
       "      <td>-0.408128</td>\n",
       "      <td>1.102625</td>\n",
       "      <td>0.541863</td>\n",
       "      <td>0.936011</td>\n",
       "      <td>-1.461393</td>\n",
       "      <td>0.651526</td>\n",
       "      <td>0.546038</td>\n",
       "      <td>1.113703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.776281</td>\n",
       "      <td>-1.155379</td>\n",
       "      <td>-0.691791</td>\n",
       "      <td>-1.469841</td>\n",
       "      <td>2.881734</td>\n",
       "      <td>1.341182</td>\n",
       "      <td>0.193357</td>\n",
       "      <td>-0.074438</td>\n",
       "      <td>1.087457</td>\n",
       "      <td>0.559874</td>\n",
       "      <td>0.918505</td>\n",
       "      <td>-1.450086</td>\n",
       "      <td>0.627549</td>\n",
       "      <td>0.525497</td>\n",
       "      <td>1.112273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.802829</td>\n",
       "      <td>-1.066718</td>\n",
       "      <td>-0.703014</td>\n",
       "      <td>-1.326682</td>\n",
       "      <td>2.923240</td>\n",
       "      <td>1.353771</td>\n",
       "      <td>0.168132</td>\n",
       "      <td>0.315163</td>\n",
       "      <td>1.072288</td>\n",
       "      <td>0.568669</td>\n",
       "      <td>0.900999</td>\n",
       "      <td>-1.438779</td>\n",
       "      <td>0.603572</td>\n",
       "      <td>0.504957</td>\n",
       "      <td>1.110844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.829211</td>\n",
       "      <td>-0.849380</td>\n",
       "      <td>-0.712461</td>\n",
       "      <td>-1.222733</td>\n",
       "      <td>3.089057</td>\n",
       "      <td>1.366360</td>\n",
       "      <td>0.142906</td>\n",
       "      <td>0.349253</td>\n",
       "      <td>1.057120</td>\n",
       "      <td>0.597643</td>\n",
       "      <td>0.883493</td>\n",
       "      <td>-1.427472</td>\n",
       "      <td>0.579595</td>\n",
       "      <td>0.484416</td>\n",
       "      <td>1.109414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.851312</td>\n",
       "      <td>-0.689886</td>\n",
       "      <td>-0.724085</td>\n",
       "      <td>-1.189161</td>\n",
       "      <td>3.148408</td>\n",
       "      <td>1.378950</td>\n",
       "      <td>0.117681</td>\n",
       "      <td>0.370628</td>\n",
       "      <td>1.041952</td>\n",
       "      <td>0.619029</td>\n",
       "      <td>0.865987</td>\n",
       "      <td>-1.416165</td>\n",
       "      <td>0.555618</td>\n",
       "      <td>0.463875</td>\n",
       "      <td>1.107985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1.873607</td>\n",
       "      <td>-0.441954</td>\n",
       "      <td>-0.736434</td>\n",
       "      <td>-1.296499</td>\n",
       "      <td>3.219670</td>\n",
       "      <td>1.391539</td>\n",
       "      <td>0.092456</td>\n",
       "      <td>0.427775</td>\n",
       "      <td>1.026783</td>\n",
       "      <td>0.635352</td>\n",
       "      <td>0.848481</td>\n",
       "      <td>-1.404857</td>\n",
       "      <td>0.531641</td>\n",
       "      <td>0.443334</td>\n",
       "      <td>1.106555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1.901718</td>\n",
       "      <td>-0.132782</td>\n",
       "      <td>-0.738433</td>\n",
       "      <td>-1.444029</td>\n",
       "      <td>3.421082</td>\n",
       "      <td>1.404128</td>\n",
       "      <td>0.067231</td>\n",
       "      <td>0.538287</td>\n",
       "      <td>1.011615</td>\n",
       "      <td>0.652040</td>\n",
       "      <td>0.830975</td>\n",
       "      <td>-1.393550</td>\n",
       "      <td>0.507664</td>\n",
       "      <td>0.422793</td>\n",
       "      <td>1.105126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1.928136</td>\n",
       "      <td>0.084061</td>\n",
       "      <td>-0.738236</td>\n",
       "      <td>-1.444143</td>\n",
       "      <td>3.373840</td>\n",
       "      <td>1.416717</td>\n",
       "      <td>0.042006</td>\n",
       "      <td>0.666156</td>\n",
       "      <td>0.996447</td>\n",
       "      <td>0.638794</td>\n",
       "      <td>0.813469</td>\n",
       "      <td>-1.382243</td>\n",
       "      <td>0.483687</td>\n",
       "      <td>0.402252</td>\n",
       "      <td>1.103697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1.972896</td>\n",
       "      <td>0.162956</td>\n",
       "      <td>-0.746642</td>\n",
       "      <td>-1.717057</td>\n",
       "      <td>3.497364</td>\n",
       "      <td>1.429307</td>\n",
       "      <td>0.016781</td>\n",
       "      <td>0.767681</td>\n",
       "      <td>0.981278</td>\n",
       "      <td>0.637852</td>\n",
       "      <td>0.795963</td>\n",
       "      <td>-1.370936</td>\n",
       "      <td>0.459711</td>\n",
       "      <td>0.381711</td>\n",
       "      <td>1.102267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bahia - value   IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "168       1.142199                                         -1.032458   \n",
       "169       1.169669                                         -1.077854   \n",
       "170       1.197459                                         -1.036724   \n",
       "171       1.231686                                         -1.013400   \n",
       "172       1.268038                                         -1.059228   \n",
       "173       1.299588                                         -1.170032   \n",
       "174       1.338328                                         -1.115621   \n",
       "175       1.376421                                         -1.182159   \n",
       "176       1.409717                                         -1.270623   \n",
       "177       1.441251                                         -1.321931   \n",
       "178       1.453870                                         -1.339532   \n",
       "179       1.466416                                         -1.177993   \n",
       "180       1.480970                                         -1.259339   \n",
       "181       1.495507                                         -1.295690   \n",
       "182       1.503196                                         -1.375339   \n",
       "183       1.512115                                         -1.486072   \n",
       "184       1.525662                                         -1.642545   \n",
       "185       1.523051                                         -1.696141   \n",
       "186       1.525659                                         -1.924292   \n",
       "187       1.537596                                         -1.972576   \n",
       "188       1.551512                                         -1.832285   \n",
       "189       1.571909                                         -1.767518   \n",
       "190       1.589949                                         -1.688436   \n",
       "191       1.614723                                         -1.350739   \n",
       "192       1.641795                                         -1.335646   \n",
       "193       1.672065                                         -1.219493   \n",
       "194       1.708758                                         -1.109104   \n",
       "195       1.749791                                         -1.210267   \n",
       "196       1.776281                                         -1.155379   \n",
       "197       1.802829                                         -1.066718   \n",
       "198       1.829211                                         -0.849380   \n",
       "199       1.851312                                         -0.689886   \n",
       "200       1.873607                                         -0.441954   \n",
       "201       1.901718                                         -0.132782   \n",
       "202       1.928136                                          0.084061   \n",
       "203       1.972896                                          0.162956   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI  População  \\\n",
       "168                      -0.585256       -0.505289 -1.202612   1.035545   \n",
       "169                      -0.581056       -0.373612 -1.139987   1.046449   \n",
       "170                      -0.574769       -0.291560 -1.053534   1.057352   \n",
       "171                      -0.567021       -0.450118 -0.932974   1.068256   \n",
       "172                      -0.557842       -0.909054 -0.763756   1.079159   \n",
       "173                      -0.548012        0.387442 -0.644652   1.090063   \n",
       "174                      -0.537244       -0.937589 -0.541601   1.100966   \n",
       "175                      -0.524149       -1.212466 -0.565747   1.111870   \n",
       "176                      -0.519647       -0.499856 -0.536256   1.122773   \n",
       "177                      -0.510389       -0.120028 -0.544036   1.133677   \n",
       "178                      -0.493794        0.302226 -0.455816   1.144580   \n",
       "179                      -0.482543       -1.133998 -0.341841   1.155483   \n",
       "180                      -0.476661       -1.443985 -0.404575   1.166387   \n",
       "181                      -0.477030       -1.094983 -0.459546   1.176757   \n",
       "182                      -0.480370       -1.222235 -0.278469   1.187127   \n",
       "183                      -0.485497       -1.257837 -0.319575   1.197496   \n",
       "184                      -0.493698       -1.292177 -0.288531   1.207866   \n",
       "185                      -0.503979       -1.370259 -0.279873   1.218236   \n",
       "186                      -0.519678       -1.295050 -0.169438   1.228606   \n",
       "187                      -0.537386       -1.148806  0.237913   1.238976   \n",
       "188                      -0.553461       -1.222716  0.559735   1.249345   \n",
       "189                      -0.577640       -1.370262  0.802696   1.259715   \n",
       "190                      -0.606406       -1.283594  1.106124   1.270085   \n",
       "191                      -0.629137       -1.449457  1.364723   1.280455   \n",
       "192                      -0.644481       -1.429372  1.795882   1.290825   \n",
       "193                      -0.656384       -1.694292  2.156500   1.303414   \n",
       "194                      -0.667513       -1.370264  2.328120   1.316003   \n",
       "195                      -0.680761       -1.222734  2.527565   1.328592   \n",
       "196                      -0.691791       -1.469841  2.881734   1.341182   \n",
       "197                      -0.703014       -1.326682  2.923240   1.353771   \n",
       "198                      -0.712461       -1.222733  3.089057   1.366360   \n",
       "199                      -0.724085       -1.189161  3.148408   1.378950   \n",
       "200                      -0.736434       -1.296499  3.219670   1.391539   \n",
       "201                      -0.738433       -1.444029  3.421082   1.404128   \n",
       "202                      -0.738236       -1.444143  3.373840   1.416717   \n",
       "203                      -0.746642       -1.717057  3.497364   1.429307   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  Bahia - IDH  \\\n",
       "168                                0.707473  -1.446117     1.325909   \n",
       "169                                0.692399  -1.526274     1.318823   \n",
       "170                                0.677324  -1.610341     1.311738   \n",
       "171                                0.662249  -1.601301     1.304652   \n",
       "172                                0.647175  -1.618284     1.297567   \n",
       "173                                0.632100  -1.602348     1.290482   \n",
       "174                                0.617026  -1.686659     1.283396   \n",
       "175                                0.601951  -1.688184     1.276311   \n",
       "176                                0.586877  -1.692173     1.269226   \n",
       "177                                0.571802  -1.690760     1.262140   \n",
       "178                                0.556728  -1.754852     1.255055   \n",
       "179                                0.541653  -1.791166     1.247969   \n",
       "180                                0.526579  -1.758016     1.240884   \n",
       "181                                0.507219  -1.773309     1.233154   \n",
       "182                                0.487859  -1.769159     1.225425   \n",
       "183                                0.468498  -1.788744     1.217695   \n",
       "184                                0.449138  -1.818878     1.209966   \n",
       "185                                0.429778  -1.814437     1.202236   \n",
       "186                                0.410418  -1.797383     1.194507   \n",
       "187                                0.391058  -1.790206     1.186777   \n",
       "188                                0.371698  -1.629944     1.179048   \n",
       "189                                0.352338  -1.383366     1.171318   \n",
       "190                                0.332977  -1.227161     1.163589   \n",
       "191                                0.313617  -1.127517     1.155859   \n",
       "192                                0.294257  -0.993281     1.148130   \n",
       "193                                0.269032  -0.748672     1.132961   \n",
       "194                                0.243807  -0.529379     1.117793   \n",
       "195                                0.218582  -0.408128     1.102625   \n",
       "196                                0.193357  -0.074438     1.087457   \n",
       "197                                0.168132   0.315163     1.072288   \n",
       "198                                0.142906   0.349253     1.057120   \n",
       "199                                0.117681   0.370628     1.041952   \n",
       "200                                0.092456   0.427775     1.026783   \n",
       "201                                0.067231   0.538287     1.011615   \n",
       "202                                0.042006   0.666156     0.996447   \n",
       "203                                0.016781   0.767681     0.981278   \n",
       "\n",
       "     Bahia - Produção de Cimento (t)  Bahia - PIB - Estadual  \\\n",
       "168                         0.407445                1.169992   \n",
       "169                         0.400025                1.166430   \n",
       "170                         0.398398                1.162868   \n",
       "171                         0.379182                1.159306   \n",
       "172                         0.384352                1.155744   \n",
       "173                         0.367059                1.152182   \n",
       "174                         0.363065                1.148620   \n",
       "175                         0.360415                1.145058   \n",
       "176                         0.359113                1.141496   \n",
       "177                         0.367682                1.137934   \n",
       "178                         0.362771                1.134372   \n",
       "179                         0.368449                1.130810   \n",
       "180                         0.387991                1.127248   \n",
       "181                         0.400455                1.115688   \n",
       "182                         0.405571                1.104128   \n",
       "183                         0.398503                1.092568   \n",
       "184                         0.408478                1.081008   \n",
       "185                         0.451828                1.069448   \n",
       "186                         0.469481                1.057888   \n",
       "187                         0.484754                1.046328   \n",
       "188                         0.486517                1.034768   \n",
       "189                         0.500299                1.023208   \n",
       "190                         0.498868                1.011648   \n",
       "191                         0.499306                1.000088   \n",
       "192                         0.540294                0.988529   \n",
       "193                         0.528474                0.971023   \n",
       "194                         0.538242                0.953517   \n",
       "195                         0.541863                0.936011   \n",
       "196                         0.559874                0.918505   \n",
       "197                         0.568669                0.900999   \n",
       "198                         0.597643                0.883493   \n",
       "199                         0.619029                0.865987   \n",
       "200                         0.635352                0.848481   \n",
       "201                         0.652040                0.830975   \n",
       "202                         0.638794                0.813469   \n",
       "203                         0.637852                0.795963   \n",
       "\n",
       "     Bahia - PIB - Construção Civil  Bahia - PIB - Per Capita  \\\n",
       "168                       -1.463511                  1.021321   \n",
       "169                       -1.470219                  1.023238   \n",
       "170                       -1.476927                  1.025155   \n",
       "171                       -1.483635                  1.027072   \n",
       "172                       -1.490343                  1.028989   \n",
       "173                       -1.497051                  1.030906   \n",
       "174                       -1.503759                  1.032823   \n",
       "175                       -1.510467                  1.034740   \n",
       "176                       -1.517175                  1.036658   \n",
       "177                       -1.523883                  1.038575   \n",
       "178                       -1.530591                  1.040492   \n",
       "179                       -1.537299                  1.042409   \n",
       "180                       -1.544007                  1.044326   \n",
       "181                       -1.539950                  1.017587   \n",
       "182                       -1.535892                  0.990848   \n",
       "183                       -1.531834                  0.964108   \n",
       "184                       -1.527776                  0.937369   \n",
       "185                       -1.523719                  0.910630   \n",
       "186                       -1.519661                  0.883891   \n",
       "187                       -1.515603                  0.857152   \n",
       "188                       -1.511546                  0.830413   \n",
       "189                       -1.507488                  0.803674   \n",
       "190                       -1.503430                  0.776935   \n",
       "191                       -1.499373                  0.750195   \n",
       "192                       -1.495315                  0.723456   \n",
       "193                       -1.484008                  0.699479   \n",
       "194                       -1.472700                  0.675503   \n",
       "195                       -1.461393                  0.651526   \n",
       "196                       -1.450086                  0.627549   \n",
       "197                       -1.438779                  0.603572   \n",
       "198                       -1.427472                  0.579595   \n",
       "199                       -1.416165                  0.555618   \n",
       "200                       -1.404857                  0.531641   \n",
       "201                       -1.393550                  0.507664   \n",
       "202                       -1.382243                  0.483687   \n",
       "203                       -1.370936                  0.459711   \n",
       "\n",
       "     Bahia - PIB - Preços de Mercado  Bahia - Desemprego  \n",
       "168                         0.978804            1.134024  \n",
       "169                         0.968444            1.133050  \n",
       "170                         0.958085            1.132076  \n",
       "171                         0.947726            1.131102  \n",
       "172                         0.937367            1.130127  \n",
       "173                         0.927007            1.129153  \n",
       "174                         0.916648            1.128179  \n",
       "175                         0.906289            1.127205  \n",
       "176                         0.895930            1.126231  \n",
       "177                         0.885571            1.125257  \n",
       "178                         0.875211            1.124283  \n",
       "179                         0.864852            1.123309  \n",
       "180                         0.854493            1.122335  \n",
       "181                         0.833924            1.121973  \n",
       "182                         0.813354            1.121611  \n",
       "183                         0.792785            1.121249  \n",
       "184                         0.772216            1.120887  \n",
       "185                         0.751646            1.120525  \n",
       "186                         0.731077            1.120163  \n",
       "187                         0.710508            1.119801  \n",
       "188                         0.689938            1.119439  \n",
       "189                         0.669369            1.119077  \n",
       "190                         0.648800            1.118715  \n",
       "191                         0.628230            1.118353  \n",
       "192                         0.607661            1.117991  \n",
       "193                         0.587120            1.116561  \n",
       "194                         0.566579            1.115132  \n",
       "195                         0.546038            1.113703  \n",
       "196                         0.525497            1.112273  \n",
       "197                         0.504957            1.110844  \n",
       "198                         0.484416            1.109414  \n",
       "199                         0.463875            1.107985  \n",
       "200                         0.443334            1.106555  \n",
       "201                         0.422793            1.105126  \n",
       "202                         0.402252            1.103697  \n",
       "203                         0.381711            1.102267  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input de test (Ano 2021)\n",
    "test_input = input_data.iloc[train_split:]\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb5a92e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168    213.266\n",
       "169    285.938\n",
       "170    219.576\n",
       "171    267.203\n",
       "172    240.714\n",
       "173    250.101\n",
       "174    277.528\n",
       "175    270.092\n",
       "176    278.146\n",
       "177    257.458\n",
       "178    231.748\n",
       "179    268.336\n",
       "180    223.453\n",
       "181    241.464\n",
       "182    238.901\n",
       "183    191.989\n",
       "184    272.452\n",
       "185    261.009\n",
       "186    292.688\n",
       "187    258.881\n",
       "188    276.879\n",
       "189    255.774\n",
       "190    208.326\n",
       "191    291.428\n",
       "192    249.430\n",
       "193    241.612\n",
       "194    252.303\n",
       "195    281.912\n",
       "196    200.213\n",
       "197    270.511\n",
       "198    281.466\n",
       "199    285.535\n",
       "200    328.259\n",
       "201    298.078\n",
       "202    264.838\n",
       "203    300.311\n",
       "Name: Bahia - Consumo de Cimento (t), dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alvo de test (Ano 2021)\n",
    "test_target = target_data.iloc[train_split:split_index + 1]\n",
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f831f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_splitter(df, div_factor):\n",
    "    split_factor = len(df)//div_factor\n",
    "    positions_to_drop = []\n",
    "    for i in range(split_factor):\n",
    "        pos = df.shape[0] - (i*6 + 1)\n",
    "        positions_to_drop.append(pos)\n",
    "    \n",
    "    return df.drop(positions_to_drop), df.iloc[positions_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a3842bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede neural feed-forward com optmizador Estocástico\n",
    "def neural_network_model(train_input, train_target, want_verbose=1, seed=0):\n",
    "    if seed != 0:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "    # Aṕos 500 epochs sem grandes melhoras no val_loss, interrompe.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=500, \n",
    "                                                      verbose=want_verbose, \n",
    "                                                      restore_best_weights=True,\n",
    "                                                      start_from_epoch=100)\n",
    "    # Método estocástico e learning rate=0.005\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.005)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2048, activation='tanh', input_shape=(train_input.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1024, activation='tanh'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1024, activation='tanh'),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    train, train_val = validation_splitter(train_input, 6)\n",
    "    target,target_val = validation_splitter(train_target, 6)\n",
    "#     display(train)\n",
    "#     display(train_val)\n",
    "#     display(target)\n",
    "#     display(target_val)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')   \n",
    "    history = model.fit(train, \n",
    "                        target, \n",
    "                        epochs=10000,\n",
    "                        validation_data=(train_val, target_val),\n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=want_verbose)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67d5c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_good_seed(train_input, train_target, test_target, test_input):\n",
    "\n",
    "    random_seeds = [random.randint(0, 2**32 - 1) for _ in range(25)]\n",
    "    print(random_seeds)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    winner_seed = None\n",
    "    i = 0\n",
    "    for seed in random_seeds:\n",
    "        print(f\"\\n\\nStep: {i} ___________________________________________\")\n",
    "        i += 1\n",
    "\n",
    "        model, history = neural_network_model(train_input, train_target, want_verbose=0, seed=seed)\n",
    "        current_loss = min(history.history['val_loss'][100:])\n",
    "        print(f\"val_loss: {current_loss}\")\n",
    "\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            winner_seed = seed\n",
    "            print(f\"winner_seed: {winner_seed}\")\n",
    "            if winner_seed == 0.0:\n",
    "                return winner_seed\n",
    "\n",
    "    return winner_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1acb58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93470216, 1864188991, 2125944638, 3839403813, 1099321741, 2105300297, 4114045829, 2208471635, 2119684419, 3278814380, 3039031841, 1811316228, 1080030228, 1631001836, 2725908411, 2701734392, 2865431260, 900984803, 4221230527, 4078569769, 4057710384, 2770560984, 857218811, 1866134998, 3942350069]\n",
      "\n",
      "\n",
      "Step: 0 ___________________________________________\n",
      "val_loss: 168.2988739013672\n",
      "winner_seed: 93470216\n",
      "\n",
      "\n",
      "Step: 1 ___________________________________________\n",
      "val_loss: 156.1582794189453\n",
      "winner_seed: 1864188991\n",
      "\n",
      "\n",
      "Step: 2 ___________________________________________\n",
      "val_loss: 109.13557434082031\n",
      "winner_seed: 2125944638\n",
      "\n",
      "\n",
      "Step: 3 ___________________________________________\n",
      "val_loss: 154.4282684326172\n",
      "\n",
      "\n",
      "Step: 4 ___________________________________________\n",
      "val_loss: 189.16162109375\n",
      "\n",
      "\n",
      "Step: 5 ___________________________________________\n",
      "val_loss: 147.6299591064453\n",
      "\n",
      "\n",
      "Step: 6 ___________________________________________\n",
      "val_loss: 114.7603759765625\n",
      "\n",
      "\n",
      "Step: 7 ___________________________________________\n",
      "val_loss: 171.08494567871094\n",
      "\n",
      "\n",
      "Step: 8 ___________________________________________\n",
      "val_loss: 134.76902770996094\n",
      "\n",
      "\n",
      "Step: 9 ___________________________________________\n",
      "val_loss: 1284.605712890625\n",
      "\n",
      "\n",
      "Step: 10 ___________________________________________\n",
      "val_loss: 150.58786010742188\n",
      "\n",
      "\n",
      "Step: 11 ___________________________________________\n",
      "val_loss: 147.0950164794922\n",
      "\n",
      "\n",
      "Step: 12 ___________________________________________\n",
      "val_loss: 163.37435913085938\n",
      "\n",
      "\n",
      "Step: 13 ___________________________________________\n",
      "val_loss: 137.57008361816406\n",
      "\n",
      "\n",
      "Step: 14 ___________________________________________\n",
      "val_loss: 143.1587677001953\n",
      "\n",
      "\n",
      "Step: 15 ___________________________________________\n",
      "val_loss: 213.1306915283203\n",
      "\n",
      "\n",
      "Step: 16 ___________________________________________\n",
      "val_loss: 162.26622009277344\n",
      "\n",
      "\n",
      "Step: 17 ___________________________________________\n",
      "val_loss: 134.0414276123047\n",
      "\n",
      "\n",
      "Step: 18 ___________________________________________\n",
      "val_loss: 140.24459838867188\n",
      "\n",
      "\n",
      "Step: 19 ___________________________________________\n",
      "val_loss: 172.04721069335938\n",
      "\n",
      "\n",
      "Step: 20 ___________________________________________\n",
      "val_loss: 179.2079315185547\n",
      "\n",
      "\n",
      "Step: 21 ___________________________________________\n",
      "val_loss: 144.26087951660156\n",
      "\n",
      "\n",
      "Step: 22 ___________________________________________\n",
      "val_loss: 166.8972930908203\n",
      "\n",
      "\n",
      "Step: 23 ___________________________________________\n",
      "val_loss: 124.7256088256836\n",
      "\n",
      "\n",
      "Step: 24 ___________________________________________\n",
      "val_loss: 172.5929718017578\n",
      "\n",
      "\n",
      "final_seed: 2125944638\n"
     ]
    }
   ],
   "source": [
    "winner_seed = get_a_good_seed(train_input, train_target, test_target, test_input)\n",
    "print(f\"\\n\\nfinal_seed: {winner_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "903643e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 53349.0625 - val_loss: 46122.2812\n",
      "Epoch 2/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 34046.9062 - val_loss: 34689.5625\n",
      "Epoch 3/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 25713.4883 - val_loss: 13351.8643\n",
      "Epoch 4/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 21137.0859 - val_loss: 13606.6387\n",
      "Epoch 5/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 9991.1172 - val_loss: 9505.7197\n",
      "Epoch 6/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4972.8989 - val_loss: 1701.9733\n",
      "Epoch 7/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1107.8701 - val_loss: 562.3380\n",
      "Epoch 8/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 615.9182 - val_loss: 384.4833\n",
      "Epoch 9/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 433.6516 - val_loss: 689.4791\n",
      "Epoch 10/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 675.5025 - val_loss: 337.8489\n",
      "Epoch 11/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 515.3812 - val_loss: 362.1304\n",
      "Epoch 12/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 517.4027 - val_loss: 403.4361\n",
      "Epoch 13/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 618.8705 - val_loss: 430.6031\n",
      "Epoch 14/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 504.1781 - val_loss: 293.6314\n",
      "Epoch 15/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 497.3264 - val_loss: 321.4798\n",
      "Epoch 16/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 407.6671 - val_loss: 366.1187\n",
      "Epoch 17/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 563.6234 - val_loss: 332.2404\n",
      "Epoch 18/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 473.9894 - val_loss: 539.2648\n",
      "Epoch 19/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 582.5717 - val_loss: 244.9771\n",
      "Epoch 20/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 382.0012 - val_loss: 242.0816\n",
      "Epoch 21/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 409.3018 - val_loss: 889.0884\n",
      "Epoch 22/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 763.7748 - val_loss: 960.4086\n",
      "Epoch 23/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 672.9250 - val_loss: 255.5581\n",
      "Epoch 24/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 461.4150 - val_loss: 361.5820\n",
      "Epoch 25/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 471.9216 - val_loss: 484.6135\n",
      "Epoch 26/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 478.6057 - val_loss: 364.4896\n",
      "Epoch 27/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 416.2509 - val_loss: 233.9384\n",
      "Epoch 28/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 324.4019 - val_loss: 278.1060\n",
      "Epoch 29/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 425.1314 - val_loss: 192.8088\n",
      "Epoch 30/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 399.0040 - val_loss: 238.6167\n",
      "Epoch 31/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 435.1204 - val_loss: 471.2025\n",
      "Epoch 32/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.3423 - val_loss: 226.5441\n",
      "Epoch 33/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 516.4800 - val_loss: 1300.1544\n",
      "Epoch 34/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 854.9907 - val_loss: 617.4770\n",
      "Epoch 35/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 473.4782 - val_loss: 240.2134\n",
      "Epoch 36/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 445.1168 - val_loss: 638.4195\n",
      "Epoch 37/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 583.7204 - val_loss: 774.9019\n",
      "Epoch 38/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 575.1260 - val_loss: 369.3553\n",
      "Epoch 39/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 382.4749 - val_loss: 515.5064\n",
      "Epoch 40/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 443.2409 - val_loss: 243.3027\n",
      "Epoch 41/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 414.3742 - val_loss: 452.6648\n",
      "Epoch 42/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 442.5452 - val_loss: 351.0133\n",
      "Epoch 43/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 450.3806 - val_loss: 369.1489\n",
      "Epoch 44/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 495.0162 - val_loss: 389.3722\n",
      "Epoch 45/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 574.9124 - val_loss: 325.6596\n",
      "Epoch 46/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 433.8521 - val_loss: 512.7114\n",
      "Epoch 47/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 558.5488 - val_loss: 208.2163\n",
      "Epoch 48/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 403.1924 - val_loss: 410.3914\n",
      "Epoch 49/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 443.9985 - val_loss: 755.7603\n",
      "Epoch 50/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 651.9067 - val_loss: 288.7838\n",
      "Epoch 51/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 574.2955 - val_loss: 748.6501\n",
      "Epoch 52/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 730.9445 - val_loss: 542.4859\n",
      "Epoch 53/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 743.2082 - val_loss: 748.2779\n",
      "Epoch 54/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 566.8197 - val_loss: 338.8248\n",
      "Epoch 55/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 541.2132 - val_loss: 449.2318\n",
      "Epoch 56/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 520.2330 - val_loss: 200.4977\n",
      "Epoch 57/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 308.5301 - val_loss: 196.8791\n",
      "Epoch 58/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 484.8793 - val_loss: 330.6396\n",
      "Epoch 59/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 480.5604 - val_loss: 285.7192\n",
      "Epoch 60/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 461.7552 - val_loss: 486.5235\n",
      "Epoch 61/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 471.1470 - val_loss: 305.1750\n",
      "Epoch 62/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 407.4167 - val_loss: 381.9382\n",
      "Epoch 63/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 515.7268 - val_loss: 322.4681\n",
      "Epoch 64/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 420.0268 - val_loss: 298.9049\n",
      "Epoch 65/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 358.1318 - val_loss: 387.0183\n",
      "Epoch 66/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 389.3436 - val_loss: 340.1616\n",
      "Epoch 67/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 402.4420 - val_loss: 303.2624\n",
      "Epoch 68/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 425.3108 - val_loss: 488.1413\n",
      "Epoch 69/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 395.5722 - val_loss: 294.2569\n",
      "Epoch 70/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.9762 - val_loss: 203.5465\n",
      "Epoch 71/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 366.8152 - val_loss: 310.9431\n",
      "Epoch 72/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 382.0554 - val_loss: 338.2661\n",
      "Epoch 73/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 463.9893 - val_loss: 293.3661\n",
      "Epoch 74/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 375.3033 - val_loss: 213.9987\n",
      "Epoch 75/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.1904 - val_loss: 206.3444\n",
      "Epoch 76/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 495.8673 - val_loss: 296.8601\n",
      "Epoch 77/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 592.1541 - val_loss: 358.0112\n",
      "Epoch 78/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 540.9270 - val_loss: 324.0534\n",
      "Epoch 79/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 518.7855 - val_loss: 308.9951\n",
      "Epoch 80/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 425.1756 - val_loss: 199.5612\n",
      "Epoch 81/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 326.6982 - val_loss: 189.4670\n",
      "Epoch 82/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 290.9828 - val_loss: 188.6272\n",
      "Epoch 83/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 425.1520 - val_loss: 252.7412\n",
      "Epoch 84/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 471.1938 - val_loss: 272.2770\n",
      "Epoch 85/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 438.9564 - val_loss: 310.2895\n",
      "Epoch 86/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 483.7518 - val_loss: 389.5735\n",
      "Epoch 87/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 443.5250 - val_loss: 400.8097\n",
      "Epoch 88/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 461.9028 - val_loss: 470.5671\n",
      "Epoch 89/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 509.8645 - val_loss: 322.5477\n",
      "Epoch 90/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 388.7288 - val_loss: 275.7270\n",
      "Epoch 91/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 345.7832 - val_loss: 271.2065\n",
      "Epoch 92/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 480.2594 - val_loss: 317.0547\n",
      "Epoch 93/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 412.5069 - val_loss: 295.9980\n",
      "Epoch 94/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 371.6232 - val_loss: 520.0481\n",
      "Epoch 95/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 393.4816 - val_loss: 295.1695\n",
      "Epoch 96/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.3093 - val_loss: 608.1464\n",
      "Epoch 97/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 625.2704 - val_loss: 458.1606\n",
      "Epoch 98/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 388.8647 - val_loss: 327.4234\n",
      "Epoch 99/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.1824 - val_loss: 241.4578\n",
      "Epoch 100/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 330.3833 - val_loss: 265.4685\n",
      "Epoch 101/10000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 421.3935 - val_loss: 427.9102\n",
      "Epoch 102/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 402.3116 - val_loss: 263.0894\n",
      "Epoch 103/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 403.8469 - val_loss: 496.9870\n",
      "Epoch 104/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 435.2041 - val_loss: 489.2523\n",
      "Epoch 105/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 454.3746 - val_loss: 317.1050\n",
      "Epoch 106/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 418.5348 - val_loss: 524.5688\n",
      "Epoch 107/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 409.1433 - val_loss: 460.1549\n",
      "Epoch 108/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 449.0312 - val_loss: 500.5037\n",
      "Epoch 109/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 470.4170 - val_loss: 357.1798\n",
      "Epoch 110/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 415.5353 - val_loss: 193.6427\n",
      "Epoch 111/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 355.1873 - val_loss: 259.0738\n",
      "Epoch 112/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 385.3139 - val_loss: 333.3910\n",
      "Epoch 113/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 389.3639 - val_loss: 292.5458\n",
      "Epoch 114/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 376.7151 - val_loss: 427.8524\n",
      "Epoch 115/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.0800 - val_loss: 674.1554\n",
      "Epoch 116/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 460.9976 - val_loss: 418.0186\n",
      "Epoch 117/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 409.3171 - val_loss: 573.8658\n",
      "Epoch 118/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 375.5000 - val_loss: 361.2336\n",
      "Epoch 119/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 348.1754 - val_loss: 443.5559\n",
      "Epoch 120/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 360.0242 - val_loss: 181.1794\n",
      "Epoch 121/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.5292 - val_loss: 293.9910\n",
      "Epoch 122/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.9872 - val_loss: 394.9743\n",
      "Epoch 123/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 411.8056 - val_loss: 262.3077\n",
      "Epoch 124/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 450.9206 - val_loss: 253.9255\n",
      "Epoch 125/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.6339 - val_loss: 227.5620\n",
      "Epoch 126/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 349.0805 - val_loss: 267.7843\n",
      "Epoch 127/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 351.4312 - val_loss: 228.3330\n",
      "Epoch 128/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 424.6679 - val_loss: 210.1039\n",
      "Epoch 129/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 405.8554 - val_loss: 507.1025\n",
      "Epoch 130/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 561.9791 - val_loss: 362.5196\n",
      "Epoch 131/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 475.7324 - val_loss: 355.1343\n",
      "Epoch 132/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 417.2511 - val_loss: 311.8433\n",
      "Epoch 133/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 451.3789 - val_loss: 234.9093\n",
      "Epoch 134/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.1830 - val_loss: 266.9960\n",
      "Epoch 135/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 527.3098 - val_loss: 427.9250\n",
      "Epoch 136/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 565.9266 - val_loss: 304.5918\n",
      "Epoch 137/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 439.4531 - val_loss: 206.7379\n",
      "Epoch 138/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 393.9870 - val_loss: 283.5873\n",
      "Epoch 139/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 486.4327 - val_loss: 426.8878\n",
      "Epoch 140/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 569.7884 - val_loss: 331.9697\n",
      "Epoch 141/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 398.5724 - val_loss: 235.2408\n",
      "Epoch 142/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 394.4731 - val_loss: 418.6802\n",
      "Epoch 143/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 427.5036 - val_loss: 269.3773\n",
      "Epoch 144/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 418.3237 - val_loss: 378.8538\n",
      "Epoch 145/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 456.0848 - val_loss: 414.4622\n",
      "Epoch 146/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 537.5908 - val_loss: 279.8179\n",
      "Epoch 147/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 401.0999 - val_loss: 293.5045\n",
      "Epoch 148/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 372.8722 - val_loss: 412.1213\n",
      "Epoch 149/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 376.9185 - val_loss: 312.7607\n",
      "Epoch 150/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.5620 - val_loss: 400.0636\n",
      "Epoch 151/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 415.4651 - val_loss: 300.4999\n",
      "Epoch 152/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 437.7971 - val_loss: 318.3320\n",
      "Epoch 153/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 386.0637 - val_loss: 275.3341\n",
      "Epoch 154/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 377.1610 - val_loss: 281.0767\n",
      "Epoch 155/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 398.0032 - val_loss: 347.1775\n",
      "Epoch 156/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.9278 - val_loss: 294.5645\n",
      "Epoch 157/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 401.7391 - val_loss: 362.8146\n",
      "Epoch 158/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 393.0506 - val_loss: 345.3229\n",
      "Epoch 159/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 470.8242 - val_loss: 353.3344\n",
      "Epoch 160/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 389.1624 - val_loss: 272.0139\n",
      "Epoch 161/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 341.9490 - val_loss: 241.5444\n",
      "Epoch 162/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.0694 - val_loss: 274.1913\n",
      "Epoch 163/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 444.3780 - val_loss: 260.5183\n",
      "Epoch 164/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 552.8311 - val_loss: 561.3952\n",
      "Epoch 165/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 471.2847 - val_loss: 387.5493\n",
      "Epoch 166/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 450.5312 - val_loss: 287.2751\n",
      "Epoch 167/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 372.4285 - val_loss: 241.8176\n",
      "Epoch 168/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 314.9216 - val_loss: 304.7100\n",
      "Epoch 169/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 332.2980 - val_loss: 263.6966\n",
      "Epoch 170/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 423.3600 - val_loss: 227.9041\n",
      "Epoch 171/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 368.8996 - val_loss: 200.9175\n",
      "Epoch 172/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 424.0350 - val_loss: 313.2280\n",
      "Epoch 173/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 434.2881 - val_loss: 348.6131\n",
      "Epoch 174/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 406.4669 - val_loss: 265.3720\n",
      "Epoch 175/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 349.0548 - val_loss: 300.6803\n",
      "Epoch 176/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.3180 - val_loss: 621.8199\n",
      "Epoch 177/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 649.3903 - val_loss: 398.5360\n",
      "Epoch 178/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 460.6653 - val_loss: 390.0911\n",
      "Epoch 179/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 445.5275 - val_loss: 542.8073\n",
      "Epoch 180/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 481.1181 - val_loss: 592.7795\n",
      "Epoch 181/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 422.1571 - val_loss: 320.8922\n",
      "Epoch 182/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 489.1599 - val_loss: 553.7365\n",
      "Epoch 183/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 420.7570 - val_loss: 247.4453\n",
      "Epoch 184/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.7011 - val_loss: 717.7001\n",
      "Epoch 185/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 601.9267 - val_loss: 450.2034\n",
      "Epoch 186/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 485.9605 - val_loss: 266.2341\n",
      "Epoch 187/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 367.6960 - val_loss: 256.8215\n",
      "Epoch 188/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 371.7181 - val_loss: 232.2482\n",
      "Epoch 189/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 518.5491 - val_loss: 323.2620\n",
      "Epoch 190/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 427.2011 - val_loss: 513.5241\n",
      "Epoch 191/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 580.2881 - val_loss: 552.8224\n",
      "Epoch 192/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 625.7711 - val_loss: 415.5992\n",
      "Epoch 193/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 541.6915 - val_loss: 502.2674\n",
      "Epoch 194/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 518.9014 - val_loss: 427.6871\n",
      "Epoch 195/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 515.4156 - val_loss: 426.0206\n",
      "Epoch 196/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 495.0516 - val_loss: 259.8625\n",
      "Epoch 197/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 639.4803 - val_loss: 488.6182\n",
      "Epoch 198/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 548.3055 - val_loss: 447.1786\n",
      "Epoch 199/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 508.9519 - val_loss: 482.2205\n",
      "Epoch 200/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.2269 - val_loss: 197.8698\n",
      "Epoch 201/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 347.5470 - val_loss: 241.6313\n",
      "Epoch 202/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 426.3566 - val_loss: 319.2826\n",
      "Epoch 203/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 395.4113 - val_loss: 188.6912\n",
      "Epoch 204/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 349.0089 - val_loss: 224.8835\n",
      "Epoch 205/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 385.9095 - val_loss: 414.5510\n",
      "Epoch 206/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 444.0379 - val_loss: 293.2033\n",
      "Epoch 207/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 409.8332 - val_loss: 346.4732\n",
      "Epoch 208/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 427.7835 - val_loss: 331.6111\n",
      "Epoch 209/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 435.7456 - val_loss: 362.3875\n",
      "Epoch 210/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 453.9794 - val_loss: 338.4469\n",
      "Epoch 211/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 419.4143 - val_loss: 371.9785\n",
      "Epoch 212/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 437.8343 - val_loss: 341.4120\n",
      "Epoch 213/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 439.9861 - val_loss: 471.2335\n",
      "Epoch 214/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 422.1416 - val_loss: 373.9368\n",
      "Epoch 215/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 469.3333 - val_loss: 526.3364\n",
      "Epoch 216/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 573.0496 - val_loss: 369.1019\n",
      "Epoch 217/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 565.2214 - val_loss: 352.2774\n",
      "Epoch 218/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 518.6966 - val_loss: 361.5152\n",
      "Epoch 219/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 421.5549 - val_loss: 420.4969\n",
      "Epoch 220/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 519.3186 - val_loss: 464.6125\n",
      "Epoch 221/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 532.5164 - val_loss: 438.0812\n",
      "Epoch 222/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 466.5137 - val_loss: 314.0264\n",
      "Epoch 223/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 415.5242 - val_loss: 341.0072\n",
      "Epoch 224/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 444.9048 - val_loss: 344.9221\n",
      "Epoch 225/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 435.2668 - val_loss: 586.8962\n",
      "Epoch 226/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 690.2305 - val_loss: 294.5959\n",
      "Epoch 227/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 483.9184 - val_loss: 340.6596\n",
      "Epoch 228/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 568.2353 - val_loss: 407.2608\n",
      "Epoch 229/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 480.8924 - val_loss: 287.4538\n",
      "Epoch 230/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 433.9532 - val_loss: 393.0596\n",
      "Epoch 231/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 403.6176 - val_loss: 358.3262\n",
      "Epoch 232/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 474.8550 - val_loss: 269.6841\n",
      "Epoch 233/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 459.0059 - val_loss: 328.8614\n",
      "Epoch 234/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 606.0513 - val_loss: 538.9512\n",
      "Epoch 235/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 438.1967 - val_loss: 195.0284\n",
      "Epoch 236/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.3227 - val_loss: 220.4505\n",
      "Epoch 237/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 346.8884 - val_loss: 232.5393\n",
      "Epoch 238/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 445.7400 - val_loss: 206.1608\n",
      "Epoch 239/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 377.2086 - val_loss: 242.6458\n",
      "Epoch 240/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 400.6472 - val_loss: 378.3221\n",
      "Epoch 241/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 445.1152 - val_loss: 352.4145\n",
      "Epoch 242/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 501.4624 - val_loss: 481.4934\n",
      "Epoch 243/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 524.0769 - val_loss: 355.4919\n",
      "Epoch 244/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 442.3044 - val_loss: 436.9250\n",
      "Epoch 245/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 461.1258 - val_loss: 344.2465\n",
      "Epoch 246/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 443.9295 - val_loss: 362.9414\n",
      "Epoch 247/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 432.3536 - val_loss: 263.9060\n",
      "Epoch 248/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 368.6313 - val_loss: 423.6648\n",
      "Epoch 249/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 366.5172 - val_loss: 279.7770\n",
      "Epoch 250/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 389.8394 - val_loss: 167.1855\n",
      "Epoch 251/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 327.1072 - val_loss: 253.8668\n",
      "Epoch 252/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 374.3452 - val_loss: 254.9267\n",
      "Epoch 253/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 429.2528 - val_loss: 307.8251\n",
      "Epoch 254/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 381.0812 - val_loss: 238.4131\n",
      "Epoch 255/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 386.2957 - val_loss: 270.5226\n",
      "Epoch 256/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 394.4626 - val_loss: 217.3978\n",
      "Epoch 257/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 421.6376 - val_loss: 248.9929\n",
      "Epoch 258/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2552.6292 - val_loss: 16507.9004\n",
      "Epoch 259/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4474.5337 - val_loss: 515.1241\n",
      "Epoch 260/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 537.3766 - val_loss: 360.6880\n",
      "Epoch 261/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 493.2392 - val_loss: 315.8076\n",
      "Epoch 262/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 485.0378 - val_loss: 252.3320\n",
      "Epoch 263/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 492.6415 - val_loss: 324.6743\n",
      "Epoch 264/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 493.2683 - val_loss: 297.3629\n",
      "Epoch 265/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 534.9861 - val_loss: 750.0561\n",
      "Epoch 266/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 666.3589 - val_loss: 401.1223\n",
      "Epoch 267/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 515.0663 - val_loss: 391.2000\n",
      "Epoch 268/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 505.6577 - val_loss: 286.8958\n",
      "Epoch 269/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 573.2551 - val_loss: 375.3325\n",
      "Epoch 270/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 517.9329 - val_loss: 590.2154\n",
      "Epoch 271/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 561.2422 - val_loss: 360.3553\n",
      "Epoch 272/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 483.5496 - val_loss: 457.8701\n",
      "Epoch 273/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 436.4896 - val_loss: 373.8979\n",
      "Epoch 274/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 411.3551 - val_loss: 310.0611\n",
      "Epoch 275/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.3327 - val_loss: 267.7448\n",
      "Epoch 276/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.4143 - val_loss: 302.7180\n",
      "Epoch 277/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 362.4711 - val_loss: 239.8230\n",
      "Epoch 278/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.9358 - val_loss: 245.2558\n",
      "Epoch 279/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.7780 - val_loss: 306.4766\n",
      "Epoch 280/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 369.2585 - val_loss: 345.7642\n",
      "Epoch 281/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 367.2415 - val_loss: 332.8535\n",
      "Epoch 282/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 444.2130 - val_loss: 366.7573\n",
      "Epoch 283/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 449.5679 - val_loss: 238.6832\n",
      "Epoch 284/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 404.3719 - val_loss: 382.3467\n",
      "Epoch 285/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 420.0685 - val_loss: 266.9524\n",
      "Epoch 286/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.0365 - val_loss: 266.8741\n",
      "Epoch 287/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 414.7045 - val_loss: 321.6004\n",
      "Epoch 288/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 15740.2021 - val_loss: 7312.8169\n",
      "Epoch 289/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4293.3423 - val_loss: 723.0751\n",
      "Epoch 290/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 731.0245 - val_loss: 430.4196\n",
      "Epoch 291/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 523.1754 - val_loss: 401.1284\n",
      "Epoch 292/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 504.2852 - val_loss: 285.4077\n",
      "Epoch 293/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 468.3658 - val_loss: 305.4523\n",
      "Epoch 294/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 387.7812 - val_loss: 321.2343\n",
      "Epoch 295/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 395.0675 - val_loss: 354.5562\n",
      "Epoch 296/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 490.6547 - val_loss: 322.8699\n",
      "Epoch 297/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 513.1281 - val_loss: 358.1815\n",
      "Epoch 298/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 438.1447 - val_loss: 850.7303\n",
      "Epoch 299/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 589.7314 - val_loss: 333.0490\n",
      "Epoch 300/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 371.1323 - val_loss: 304.4099\n",
      "Epoch 301/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 538.1111 - val_loss: 386.3529\n",
      "Epoch 302/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 408.2890 - val_loss: 286.5911\n",
      "Epoch 303/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 361.4850 - val_loss: 310.8597\n",
      "Epoch 304/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.1770 - val_loss: 243.5730\n",
      "Epoch 305/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.5628 - val_loss: 289.1489\n",
      "Epoch 306/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 367.3832 - val_loss: 154.8355\n",
      "Epoch 307/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 338.0351 - val_loss: 374.7525\n",
      "Epoch 308/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 342.9401 - val_loss: 236.9602\n",
      "Epoch 309/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 372.4098 - val_loss: 250.5642\n",
      "Epoch 310/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 331.7772 - val_loss: 294.4157\n",
      "Epoch 311/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 383.3569 - val_loss: 357.4221\n",
      "Epoch 312/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 362.1762 - val_loss: 208.6982\n",
      "Epoch 313/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 327.8161 - val_loss: 253.8775\n",
      "Epoch 314/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 415.0000 - val_loss: 371.4319\n",
      "Epoch 315/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 367.3710 - val_loss: 534.8921\n",
      "Epoch 316/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 451.6684 - val_loss: 303.4141\n",
      "Epoch 317/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 408.6630 - val_loss: 340.8770\n",
      "Epoch 318/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 455.9684 - val_loss: 291.4306\n",
      "Epoch 319/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 359.3570 - val_loss: 378.0467\n",
      "Epoch 320/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 450.3416 - val_loss: 437.6800\n",
      "Epoch 321/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 418.2013 - val_loss: 234.7765\n",
      "Epoch 322/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 371.8637 - val_loss: 424.3373\n",
      "Epoch 323/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 430.9172 - val_loss: 252.2850\n",
      "Epoch 324/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 422.7325 - val_loss: 399.8398\n",
      "Epoch 325/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 507.0732 - val_loss: 430.6415\n",
      "Epoch 326/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 402.6908 - val_loss: 421.7850\n",
      "Epoch 327/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 331.8990 - val_loss: 338.6129\n",
      "Epoch 328/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 306.3873 - val_loss: 278.9292\n",
      "Epoch 329/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 427.8275 - val_loss: 217.8800\n",
      "Epoch 330/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 358.6319 - val_loss: 165.3925\n",
      "Epoch 331/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 325.8136 - val_loss: 223.4569\n",
      "Epoch 332/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 315.2975 - val_loss: 257.1033\n",
      "Epoch 333/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 380.6501 - val_loss: 222.6454\n",
      "Epoch 334/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 330.4824 - val_loss: 286.7078\n",
      "Epoch 335/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.7861 - val_loss: 300.1741\n",
      "Epoch 336/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 305.3676 - val_loss: 179.0021\n",
      "Epoch 337/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 327.8329 - val_loss: 217.1211\n",
      "Epoch 338/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 381.8299 - val_loss: 458.8958\n",
      "Epoch 339/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.5161 - val_loss: 415.0986\n",
      "Epoch 340/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 376.2869 - val_loss: 297.4662\n",
      "Epoch 341/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 361.5826 - val_loss: 284.2023\n",
      "Epoch 342/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.7736 - val_loss: 226.6089\n",
      "Epoch 343/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 337.7492 - val_loss: 305.1219\n",
      "Epoch 344/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.2391 - val_loss: 375.6987\n",
      "Epoch 345/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 403.3365 - val_loss: 315.2126\n",
      "Epoch 346/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 344.7650 - val_loss: 258.4223\n",
      "Epoch 347/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 346.3979 - val_loss: 301.8078\n",
      "Epoch 348/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 405.1975 - val_loss: 297.0777\n",
      "Epoch 349/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.7911 - val_loss: 314.1751\n",
      "Epoch 350/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 417.8116 - val_loss: 279.0926\n",
      "Epoch 351/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 371.1381 - val_loss: 243.3895\n",
      "Epoch 352/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 361.3489 - val_loss: 256.3155\n",
      "Epoch 353/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 404.8661 - val_loss: 268.2288\n",
      "Epoch 354/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 398.7518 - val_loss: 331.2828\n",
      "Epoch 355/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 352.1402 - val_loss: 321.6983\n",
      "Epoch 356/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 373.5468 - val_loss: 211.5045\n",
      "Epoch 357/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 350.9605 - val_loss: 272.8814\n",
      "Epoch 358/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 340.6630 - val_loss: 249.7526\n",
      "Epoch 359/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.0582 - val_loss: 265.8380\n",
      "Epoch 360/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 369.5866 - val_loss: 334.0838\n",
      "Epoch 361/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 408.4887 - val_loss: 317.5856\n",
      "Epoch 362/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.5938 - val_loss: 250.0425\n",
      "Epoch 363/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 335.2804 - val_loss: 301.4976\n",
      "Epoch 364/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.6763 - val_loss: 278.2905\n",
      "Epoch 365/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 317.2226 - val_loss: 198.0343\n",
      "Epoch 366/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.2450 - val_loss: 223.9674\n",
      "Epoch 367/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 318.9334 - val_loss: 314.1028\n",
      "Epoch 368/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 349.6600 - val_loss: 246.8832\n",
      "Epoch 369/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 304.9519 - val_loss: 225.8084\n",
      "Epoch 370/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.8417 - val_loss: 388.6105\n",
      "Epoch 371/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 663.9775 - val_loss: 300.0880\n",
      "Epoch 372/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 518.1486 - val_loss: 269.2121\n",
      "Epoch 373/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 400.9075 - val_loss: 258.0049\n",
      "Epoch 374/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 335.7479 - val_loss: 240.5058\n",
      "Epoch 375/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 322.3422 - val_loss: 174.4878\n",
      "Epoch 376/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 322.1508 - val_loss: 304.2630\n",
      "Epoch 377/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 390.3322 - val_loss: 286.2603\n",
      "Epoch 378/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.3416 - val_loss: 247.7037\n",
      "Epoch 379/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 305.9386 - val_loss: 259.0415\n",
      "Epoch 380/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 338.8743 - val_loss: 198.1769\n",
      "Epoch 381/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 308.4474 - val_loss: 346.4921\n",
      "Epoch 382/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 504.7974 - val_loss: 209.4933\n",
      "Epoch 383/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 413.9118 - val_loss: 186.4240\n",
      "Epoch 384/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 316.2598 - val_loss: 424.8341\n",
      "Epoch 385/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 427.4205 - val_loss: 250.5217\n",
      "Epoch 386/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 368.5558 - val_loss: 295.3296\n",
      "Epoch 387/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 369.8300 - val_loss: 232.5660\n",
      "Epoch 388/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 347.1510 - val_loss: 319.0916\n",
      "Epoch 389/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 419.7013 - val_loss: 197.9324\n",
      "Epoch 390/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 331.4492 - val_loss: 296.2384\n",
      "Epoch 391/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 390.3943 - val_loss: 249.3757\n",
      "Epoch 392/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 326.3834 - val_loss: 232.9952\n",
      "Epoch 393/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 339.9866 - val_loss: 251.2575\n",
      "Epoch 394/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 287.6669 - val_loss: 246.8913\n",
      "Epoch 395/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 369.4500 - val_loss: 171.8062\n",
      "Epoch 396/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 338.1303 - val_loss: 143.3142\n",
      "Epoch 397/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 328.4693 - val_loss: 109.1356\n",
      "Epoch 398/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 292.7457 - val_loss: 147.4398\n",
      "Epoch 399/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 309.6590 - val_loss: 242.2799\n",
      "Epoch 400/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 868.4482 - val_loss: 2083.4646\n",
      "Epoch 401/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 907.1436 - val_loss: 356.3111\n",
      "Epoch 402/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 641.0659 - val_loss: 464.3459\n",
      "Epoch 403/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 424.2471 - val_loss: 261.7427\n",
      "Epoch 404/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 397.6598 - val_loss: 305.7232\n",
      "Epoch 405/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 414.8928 - val_loss: 446.6406\n",
      "Epoch 406/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 425.2354 - val_loss: 239.3207\n",
      "Epoch 407/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 356.0721 - val_loss: 232.1518\n",
      "Epoch 408/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 375.8632 - val_loss: 186.0383\n",
      "Epoch 409/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.5735 - val_loss: 252.6645\n",
      "Epoch 410/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 358.5812 - val_loss: 316.7747\n",
      "Epoch 411/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 462.4938 - val_loss: 215.6765\n",
      "Epoch 412/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 417.0414 - val_loss: 229.1497\n",
      "Epoch 413/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 394.6753 - val_loss: 188.3767\n",
      "Epoch 414/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 369.0405 - val_loss: 493.5910\n",
      "Epoch 415/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 445.5855 - val_loss: 294.2386\n",
      "Epoch 416/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.2014 - val_loss: 246.8726\n",
      "Epoch 417/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 366.5597 - val_loss: 230.4021\n",
      "Epoch 418/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 346.3618 - val_loss: 269.4085\n",
      "Epoch 419/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 372.6751 - val_loss: 233.1811\n",
      "Epoch 420/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 344.1923 - val_loss: 344.6877\n",
      "Epoch 421/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 468.5826 - val_loss: 200.3712\n",
      "Epoch 422/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 374.5760 - val_loss: 323.0415\n",
      "Epoch 423/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 451.5171 - val_loss: 287.5827\n",
      "Epoch 424/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 406.7055 - val_loss: 288.4139\n",
      "Epoch 425/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 415.6631 - val_loss: 361.8098\n",
      "Epoch 426/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 423.3028 - val_loss: 328.1551\n",
      "Epoch 427/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 452.0309 - val_loss: 383.5360\n",
      "Epoch 428/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 410.3189 - val_loss: 269.0684\n",
      "Epoch 429/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.7018 - val_loss: 241.8095\n",
      "Epoch 430/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 358.5731 - val_loss: 219.9099\n",
      "Epoch 431/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 360.1948 - val_loss: 266.0988\n",
      "Epoch 432/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 324.2255 - val_loss: 209.6616\n",
      "Epoch 433/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 367.8054 - val_loss: 193.9458\n",
      "Epoch 434/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 316.0271 - val_loss: 208.4523\n",
      "Epoch 435/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 308.3093 - val_loss: 207.7753\n",
      "Epoch 436/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 306.0076 - val_loss: 248.5666\n",
      "Epoch 437/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 321.5110 - val_loss: 221.2179\n",
      "Epoch 438/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.8786 - val_loss: 255.2501\n",
      "Epoch 439/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.5365 - val_loss: 203.5300\n",
      "Epoch 440/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 393.8017 - val_loss: 248.9285\n",
      "Epoch 441/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 360.8785 - val_loss: 762.4167\n",
      "Epoch 442/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 959.8683 - val_loss: 342.6027\n",
      "Epoch 443/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 438.8143 - val_loss: 252.8382\n",
      "Epoch 444/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 451.3479 - val_loss: 433.1136\n",
      "Epoch 445/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 444.0197 - val_loss: 271.6358\n",
      "Epoch 446/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.0972 - val_loss: 315.6335\n",
      "Epoch 447/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.3479 - val_loss: 357.1543\n",
      "Epoch 448/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 355.3719 - val_loss: 311.7027\n",
      "Epoch 449/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 380.7169 - val_loss: 269.5667\n",
      "Epoch 450/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 340.8361 - val_loss: 354.5826\n",
      "Epoch 451/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 399.1634 - val_loss: 334.5763\n",
      "Epoch 452/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 358.4892 - val_loss: 337.6570\n",
      "Epoch 453/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 320.8403 - val_loss: 275.0028\n",
      "Epoch 454/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 297.5262 - val_loss: 352.3900\n",
      "Epoch 455/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.1973 - val_loss: 625.0798\n",
      "Epoch 456/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 450.2638 - val_loss: 367.5669\n",
      "Epoch 457/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 364.7872 - val_loss: 360.8879\n",
      "Epoch 458/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 362.0876 - val_loss: 343.1450\n",
      "Epoch 459/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 296.5091 - val_loss: 433.7344\n",
      "Epoch 460/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.9579 - val_loss: 363.1775\n",
      "Epoch 461/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 349.5595 - val_loss: 263.1623\n",
      "Epoch 462/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 395.7404 - val_loss: 272.4965\n",
      "Epoch 463/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 340.6302 - val_loss: 372.5269\n",
      "Epoch 464/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.6246 - val_loss: 282.0827\n",
      "Epoch 465/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 359.8611 - val_loss: 339.2149\n",
      "Epoch 466/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 388.1780 - val_loss: 273.7553\n",
      "Epoch 467/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 351.7348 - val_loss: 316.7656\n",
      "Epoch 468/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 327.2286 - val_loss: 244.5124\n",
      "Epoch 469/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 310.4743 - val_loss: 248.8380\n",
      "Epoch 470/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 321.8301 - val_loss: 279.8866\n",
      "Epoch 471/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 360.4780 - val_loss: 265.8479\n",
      "Epoch 472/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 395.4560 - val_loss: 321.3398\n",
      "Epoch 473/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.3319 - val_loss: 265.8326\n",
      "Epoch 474/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 414.5211 - val_loss: 320.8477\n",
      "Epoch 475/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 342.3692 - val_loss: 266.9911\n",
      "Epoch 476/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 368.0693 - val_loss: 271.3084\n",
      "Epoch 477/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.4303 - val_loss: 253.8668\n",
      "Epoch 478/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 351.4954 - val_loss: 310.0471\n",
      "Epoch 479/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.2343 - val_loss: 299.7009\n",
      "Epoch 480/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 411.8392 - val_loss: 359.6624\n",
      "Epoch 481/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 405.5073 - val_loss: 302.3931\n",
      "Epoch 482/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 373.9694 - val_loss: 399.7205\n",
      "Epoch 483/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.3380 - val_loss: 327.3261\n",
      "Epoch 484/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 382.7959 - val_loss: 285.4464\n",
      "Epoch 485/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 371.9711 - val_loss: 288.1700\n",
      "Epoch 486/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 326.0321 - val_loss: 256.3347\n",
      "Epoch 487/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 356.9511 - val_loss: 209.5492\n",
      "Epoch 488/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.6847 - val_loss: 240.9998\n",
      "Epoch 489/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.2973 - val_loss: 243.5062\n",
      "Epoch 490/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 405.0911 - val_loss: 423.2614\n",
      "Epoch 491/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.9517 - val_loss: 212.0516\n",
      "Epoch 492/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 317.9662 - val_loss: 347.5038\n",
      "Epoch 493/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 349.1979 - val_loss: 211.3265\n",
      "Epoch 494/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 335.4771 - val_loss: 329.7332\n",
      "Epoch 495/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 404.8994 - val_loss: 447.3445\n",
      "Epoch 496/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 401.3084 - val_loss: 585.3076\n",
      "Epoch 497/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 403.8933 - val_loss: 391.3809\n",
      "Epoch 498/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 428.1086 - val_loss: 350.8831\n",
      "Epoch 499/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 368.2175 - val_loss: 357.7829\n",
      "Epoch 500/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 366.3938 - val_loss: 299.3808\n",
      "Epoch 501/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 421.3782 - val_loss: 341.5740\n",
      "Epoch 502/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 399.1807 - val_loss: 326.7893\n",
      "Epoch 503/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 370.4872 - val_loss: 310.3259\n",
      "Epoch 504/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.7045 - val_loss: 342.3203\n",
      "Epoch 505/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 369.7336 - val_loss: 273.5363\n",
      "Epoch 506/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.5391 - val_loss: 239.1301\n",
      "Epoch 507/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 315.3180 - val_loss: 260.1807\n",
      "Epoch 508/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 332.9726 - val_loss: 333.7000\n",
      "Epoch 509/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 364.5794 - val_loss: 212.2253\n",
      "Epoch 510/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 360.2790 - val_loss: 275.6286\n",
      "Epoch 511/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 348.3358 - val_loss: 258.4128\n",
      "Epoch 512/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 322.9901 - val_loss: 287.6980\n",
      "Epoch 513/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 358.0150 - val_loss: 245.3096\n",
      "Epoch 514/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.0405 - val_loss: 256.4697\n",
      "Epoch 515/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 369.4592 - val_loss: 354.2153\n",
      "Epoch 516/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 438.0246 - val_loss: 231.4870\n",
      "Epoch 517/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 441.5153 - val_loss: 230.7268\n",
      "Epoch 518/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 385.2359 - val_loss: 284.2656\n",
      "Epoch 519/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.9849 - val_loss: 231.7530\n",
      "Epoch 520/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 384.7919 - val_loss: 268.8935\n",
      "Epoch 521/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 354.4206 - val_loss: 268.5920\n",
      "Epoch 522/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 651.6146 - val_loss: 1256.2634\n",
      "Epoch 523/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 740.0106 - val_loss: 308.7985\n",
      "Epoch 524/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 517.3499 - val_loss: 276.6199\n",
      "Epoch 525/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 389.0659 - val_loss: 342.7567\n",
      "Epoch 526/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.7191 - val_loss: 450.0379\n",
      "Epoch 527/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 409.0088 - val_loss: 372.1884\n",
      "Epoch 528/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 393.0089 - val_loss: 360.3511\n",
      "Epoch 529/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 417.5496 - val_loss: 421.1973\n",
      "Epoch 530/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 386.8114 - val_loss: 402.2961\n",
      "Epoch 531/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 370.5126 - val_loss: 337.5196\n",
      "Epoch 532/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 459.0401 - val_loss: 258.6256\n",
      "Epoch 533/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.5165 - val_loss: 233.6228\n",
      "Epoch 534/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 312.4024 - val_loss: 348.7720\n",
      "Epoch 535/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 332.3346 - val_loss: 317.6601\n",
      "Epoch 536/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 338.7514 - val_loss: 247.8053\n",
      "Epoch 537/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.1895 - val_loss: 280.9890\n",
      "Epoch 538/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 297.0649 - val_loss: 258.4789\n",
      "Epoch 539/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1318.9772 - val_loss: 784.8759\n",
      "Epoch 540/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 525.0160 - val_loss: 433.0822\n",
      "Epoch 541/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 492.7921 - val_loss: 890.8643\n",
      "Epoch 542/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 564.7635 - val_loss: 391.8152\n",
      "Epoch 543/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 386.0382 - val_loss: 309.0066\n",
      "Epoch 544/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 413.1000 - val_loss: 327.2564\n",
      "Epoch 545/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 411.2187 - val_loss: 393.2026\n",
      "Epoch 546/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 408.6051 - val_loss: 287.1995\n",
      "Epoch 547/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 329.3747 - val_loss: 398.0330\n",
      "Epoch 548/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 400.4310 - val_loss: 391.8560\n",
      "Epoch 549/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 431.2590 - val_loss: 250.3276\n",
      "Epoch 550/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 348.9744 - val_loss: 239.1181\n",
      "Epoch 551/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 332.0973 - val_loss: 245.7302\n",
      "Epoch 552/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 421.3737 - val_loss: 279.4687\n",
      "Epoch 553/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 380.3689 - val_loss: 233.2668\n",
      "Epoch 554/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.3913 - val_loss: 228.7940\n",
      "Epoch 555/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 338.1095 - val_loss: 271.6756\n",
      "Epoch 556/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 359.6779 - val_loss: 381.5338\n",
      "Epoch 557/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 404.3345 - val_loss: 361.4172\n",
      "Epoch 558/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 384.4603 - val_loss: 423.0190\n",
      "Epoch 559/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 413.3940 - val_loss: 297.4281\n",
      "Epoch 560/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 360.1466 - val_loss: 291.5625\n",
      "Epoch 561/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.6329 - val_loss: 284.2596\n",
      "Epoch 562/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 391.2695 - val_loss: 447.1796\n",
      "Epoch 563/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 387.3125 - val_loss: 256.7027\n",
      "Epoch 564/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 310.7832 - val_loss: 249.4195\n",
      "Epoch 565/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 339.4767 - val_loss: 272.4842\n",
      "Epoch 566/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 330.9609 - val_loss: 229.3539\n",
      "Epoch 567/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 377.1134 - val_loss: 239.5229\n",
      "Epoch 568/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 346.5679 - val_loss: 254.5354\n",
      "Epoch 569/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 307.9602 - val_loss: 253.0853\n",
      "Epoch 570/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 298.5360 - val_loss: 247.5428\n",
      "Epoch 571/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 340.8070 - val_loss: 201.9222\n",
      "Epoch 572/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.7185 - val_loss: 196.0446\n",
      "Epoch 573/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.1971 - val_loss: 249.5805\n",
      "Epoch 574/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 316.7782 - val_loss: 261.8128\n",
      "Epoch 575/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 355.0896 - val_loss: 267.4836\n",
      "Epoch 576/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 366.3483 - val_loss: 307.1853\n",
      "Epoch 577/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 373.6597 - val_loss: 220.1738\n",
      "Epoch 578/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 376.5806 - val_loss: 290.9832\n",
      "Epoch 579/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 510.7433 - val_loss: 246.3856\n",
      "Epoch 580/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 403.8871 - val_loss: 238.5513\n",
      "Epoch 581/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 404.0263 - val_loss: 161.5415\n",
      "Epoch 582/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 423.4426 - val_loss: 198.7882\n",
      "Epoch 583/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 375.4255 - val_loss: 203.1616\n",
      "Epoch 584/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 373.6310 - val_loss: 318.4644\n",
      "Epoch 585/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 367.8976 - val_loss: 301.9056\n",
      "Epoch 586/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 359.2719 - val_loss: 289.6059\n",
      "Epoch 587/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 360.2405 - val_loss: 178.9553\n",
      "Epoch 588/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 314.1981 - val_loss: 167.0909\n",
      "Epoch 589/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 468.9156 - val_loss: 5164.2964\n",
      "Epoch 590/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 2254.8904 - val_loss: 382.4203\n",
      "Epoch 591/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 450.4284 - val_loss: 404.3770\n",
      "Epoch 592/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 396.5164 - val_loss: 344.2343\n",
      "Epoch 593/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 412.8097 - val_loss: 210.4365\n",
      "Epoch 594/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 329.8860 - val_loss: 205.8806\n",
      "Epoch 595/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 324.2351 - val_loss: 264.2886\n",
      "Epoch 596/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 397.7640 - val_loss: 204.6245\n",
      "Epoch 597/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 364.9828 - val_loss: 227.7626\n",
      "Epoch 598/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 378.2951 - val_loss: 450.6401\n",
      "Epoch 599/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 390.8472 - val_loss: 307.3187\n",
      "Epoch 600/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 354.5036 - val_loss: 306.7441\n",
      "Epoch 601/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 382.3424 - val_loss: 313.2974\n",
      "Epoch 602/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.7781 - val_loss: 300.4627\n",
      "Epoch 603/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 303.5407 - val_loss: 322.6993\n",
      "Epoch 604/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 329.3723 - val_loss: 331.0836\n",
      "Epoch 605/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 328.7227 - val_loss: 306.3075\n",
      "Epoch 606/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 324.4629 - val_loss: 252.0395\n",
      "Epoch 607/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.4509 - val_loss: 371.9587\n",
      "Epoch 608/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 316.1107 - val_loss: 448.5562\n",
      "Epoch 609/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 306.0982 - val_loss: 357.4601\n",
      "Epoch 610/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 314.8586 - val_loss: 337.9345\n",
      "Epoch 611/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 309.9775 - val_loss: 254.8825\n",
      "Epoch 612/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 321.5913 - val_loss: 279.9424\n",
      "Epoch 613/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 312.4975 - val_loss: 273.3125\n",
      "Epoch 614/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 343.3275 - val_loss: 237.1422\n",
      "Epoch 615/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 407.3946 - val_loss: 344.2212\n",
      "Epoch 616/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 439.6611 - val_loss: 427.4743\n",
      "Epoch 617/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 418.7977 - val_loss: 282.1372\n",
      "Epoch 618/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 391.6969 - val_loss: 327.3369\n",
      "Epoch 619/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 417.2456 - val_loss: 259.3538\n",
      "Epoch 620/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 356.8936 - val_loss: 229.3479\n",
      "Epoch 621/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 383.6989 - val_loss: 200.7799\n",
      "Epoch 622/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 354.4611 - val_loss: 193.7152\n",
      "Epoch 623/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 354.0266 - val_loss: 254.6551\n",
      "Epoch 624/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 340.2610 - val_loss: 276.4889\n",
      "Epoch 625/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 349.1040 - val_loss: 295.2705\n",
      "Epoch 626/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.8526 - val_loss: 237.3129\n",
      "Epoch 627/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 427.1756 - val_loss: 374.3631\n",
      "Epoch 628/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 441.7744 - val_loss: 170.0813\n",
      "Epoch 629/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 330.7119 - val_loss: 251.4628\n",
      "Epoch 630/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 309.1832 - val_loss: 281.4476\n",
      "Epoch 631/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 342.1815 - val_loss: 278.4582\n",
      "Epoch 632/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 306.8509 - val_loss: 263.8978\n",
      "Epoch 633/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 327.0013 - val_loss: 310.0683\n",
      "Epoch 634/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 338.7462 - val_loss: 310.4761\n",
      "Epoch 635/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.1202 - val_loss: 262.5963\n",
      "Epoch 636/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 337.7959 - val_loss: 337.2017\n",
      "Epoch 637/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 462.4468 - val_loss: 364.0821\n",
      "Epoch 638/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 514.8020 - val_loss: 247.6743\n",
      "Epoch 639/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 396.9121 - val_loss: 267.4349\n",
      "Epoch 640/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 408.1276 - val_loss: 233.1698\n",
      "Epoch 641/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 368.4767 - val_loss: 401.1354\n",
      "Epoch 642/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 484.9948 - val_loss: 352.3260\n",
      "Epoch 643/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 429.0506 - val_loss: 406.3062\n",
      "Epoch 644/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 482.5558 - val_loss: 333.4378\n",
      "Epoch 645/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 422.6650 - val_loss: 342.3879\n",
      "Epoch 646/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 404.0969 - val_loss: 433.1109\n",
      "Epoch 647/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 398.5283 - val_loss: 303.9765\n",
      "Epoch 648/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 406.0818 - val_loss: 280.4086\n",
      "Epoch 649/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 412.1255 - val_loss: 291.1748\n",
      "Epoch 650/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 414.4722 - val_loss: 313.4372\n",
      "Epoch 651/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 391.9552 - val_loss: 250.1485\n",
      "Epoch 652/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 449.1769 - val_loss: 228.4650\n",
      "Epoch 653/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 408.9781 - val_loss: 349.5665\n",
      "Epoch 654/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 453.1831 - val_loss: 324.5258\n",
      "Epoch 655/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 366.2016 - val_loss: 300.3979\n",
      "Epoch 656/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 289.5405 - val_loss: 276.2109\n",
      "Epoch 657/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.9699 - val_loss: 270.3309\n",
      "Epoch 658/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.2191 - val_loss: 261.5434\n",
      "Epoch 659/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 302.5076 - val_loss: 250.9336\n",
      "Epoch 660/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 297.5366 - val_loss: 272.9254\n",
      "Epoch 661/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 322.3917 - val_loss: 262.9561\n",
      "Epoch 662/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 325.9933 - val_loss: 333.8168\n",
      "Epoch 663/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.5316 - val_loss: 286.3548\n",
      "Epoch 664/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 316.5903 - val_loss: 248.4707\n",
      "Epoch 665/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 304.2549 - val_loss: 349.7573\n",
      "Epoch 666/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.6921 - val_loss: 301.9939\n",
      "Epoch 667/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 350.8912 - val_loss: 336.9671\n",
      "Epoch 668/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.6960 - val_loss: 304.5512\n",
      "Epoch 669/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 337.4782 - val_loss: 307.2117\n",
      "Epoch 670/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 306.9827 - val_loss: 306.3353\n",
      "Epoch 671/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 316.7166 - val_loss: 346.3563\n",
      "Epoch 672/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 362.0864 - val_loss: 306.4226\n",
      "Epoch 673/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 351.8937 - val_loss: 354.2513\n",
      "Epoch 674/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.3914 - val_loss: 338.1801\n",
      "Epoch 675/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 388.0026 - val_loss: 293.4551\n",
      "Epoch 676/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 375.6572 - val_loss: 319.3503\n",
      "Epoch 677/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 376.5240 - val_loss: 346.5777\n",
      "Epoch 678/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.5401 - val_loss: 334.2179\n",
      "Epoch 679/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 326.7794 - val_loss: 258.8212\n",
      "Epoch 680/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 358.4015 - val_loss: 252.2178\n",
      "Epoch 681/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 362.8356 - val_loss: 318.4393\n",
      "Epoch 682/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 351.9455 - val_loss: 300.9536\n",
      "Epoch 683/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 356.8044 - val_loss: 305.7261\n",
      "Epoch 684/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 322.0508 - val_loss: 254.8458\n",
      "Epoch 685/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 312.5790 - val_loss: 223.3050\n",
      "Epoch 686/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 318.5391 - val_loss: 383.5185\n",
      "Epoch 687/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 335.8482 - val_loss: 270.5262\n",
      "Epoch 688/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 297.6792 - val_loss: 270.6402\n",
      "Epoch 689/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 312.2147 - val_loss: 208.1923\n",
      "Epoch 690/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.2076 - val_loss: 318.8206\n",
      "Epoch 691/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.2817 - val_loss: 230.6290\n",
      "Epoch 692/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 303.8807 - val_loss: 236.7144\n",
      "Epoch 693/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 283.1367 - val_loss: 282.9882\n",
      "Epoch 694/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 285.5760 - val_loss: 261.6703\n",
      "Epoch 695/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 342.3736 - val_loss: 241.0600\n",
      "Epoch 696/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 382.0447 - val_loss: 306.6537\n",
      "Epoch 697/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.4591 - val_loss: 324.2134\n",
      "Epoch 698/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.8087 - val_loss: 281.7992\n",
      "Epoch 699/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.3792 - val_loss: 268.3719\n",
      "Epoch 700/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 321.6122 - val_loss: 278.0235\n",
      "Epoch 701/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 335.5383 - val_loss: 225.1746\n",
      "Epoch 702/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.9829 - val_loss: 180.9836\n",
      "Epoch 703/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 304.0265 - val_loss: 170.9341\n",
      "Epoch 704/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 299.3863 - val_loss: 175.9702\n",
      "Epoch 705/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 401.4539 - val_loss: 198.8876\n",
      "Epoch 706/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 326.2325 - val_loss: 270.3702\n",
      "Epoch 707/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 370.6404 - val_loss: 336.9047\n",
      "Epoch 708/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 390.1541 - val_loss: 301.5642\n",
      "Epoch 709/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 325.3531 - val_loss: 330.8180\n",
      "Epoch 710/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.9334 - val_loss: 249.7233\n",
      "Epoch 711/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.8330 - val_loss: 233.8969\n",
      "Epoch 712/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.7796 - val_loss: 267.6877\n",
      "Epoch 713/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 424.9243 - val_loss: 946.4660\n",
      "Epoch 714/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 559.9481 - val_loss: 208.9773\n",
      "Epoch 715/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.0428 - val_loss: 281.3235\n",
      "Epoch 716/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 382.2213 - val_loss: 350.5332\n",
      "Epoch 717/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 373.7701 - val_loss: 257.2859\n",
      "Epoch 718/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 351.9492 - val_loss: 279.0495\n",
      "Epoch 719/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 352.3361 - val_loss: 342.0096\n",
      "Epoch 720/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 352.9141 - val_loss: 356.8012\n",
      "Epoch 721/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 354.5150 - val_loss: 249.8097\n",
      "Epoch 722/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 328.1955 - val_loss: 275.6278\n",
      "Epoch 723/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 317.8264 - val_loss: 311.6887\n",
      "Epoch 724/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.5101 - val_loss: 237.7793\n",
      "Epoch 725/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 322.3426 - val_loss: 254.5009\n",
      "Epoch 726/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 316.7880 - val_loss: 260.8953\n",
      "Epoch 727/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 287.2772 - val_loss: 258.6946\n",
      "Epoch 728/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 310.2406 - val_loss: 297.0124\n",
      "Epoch 729/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 292.9237 - val_loss: 314.1576\n",
      "Epoch 730/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 332.2627 - val_loss: 277.7205\n",
      "Epoch 731/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.4161 - val_loss: 293.9209\n",
      "Epoch 732/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 325.0096 - val_loss: 277.8554\n",
      "Epoch 733/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 340.8477 - val_loss: 258.7489\n",
      "Epoch 734/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 289.8372 - val_loss: 458.2634\n",
      "Epoch 735/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 318.6461 - val_loss: 293.0400\n",
      "Epoch 736/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.0651 - val_loss: 308.4424\n",
      "Epoch 737/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.1674 - val_loss: 382.4764\n",
      "Epoch 738/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 392.5892 - val_loss: 276.1758\n",
      "Epoch 739/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 329.9946 - val_loss: 252.6992\n",
      "Epoch 740/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 347.5660 - val_loss: 318.2719\n",
      "Epoch 741/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 325.3459 - val_loss: 323.3666\n",
      "Epoch 742/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 375.9503 - val_loss: 256.1086\n",
      "Epoch 743/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 327.1451 - val_loss: 314.2582\n",
      "Epoch 744/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 446.0839 - val_loss: 290.2185\n",
      "Epoch 745/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 339.4178 - val_loss: 277.1797\n",
      "Epoch 746/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.6234 - val_loss: 330.6684\n",
      "Epoch 747/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 359.8570 - val_loss: 441.2109\n",
      "Epoch 748/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 408.1553 - val_loss: 362.5016\n",
      "Epoch 749/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 360.9753 - val_loss: 295.5283\n",
      "Epoch 750/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 350.0049 - val_loss: 273.9730\n",
      "Epoch 751/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 331.4278 - val_loss: 237.5440\n",
      "Epoch 752/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 349.7148 - val_loss: 274.2283\n",
      "Epoch 753/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 350.4722 - val_loss: 253.4235\n",
      "Epoch 754/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 307.4767 - val_loss: 247.6060\n",
      "Epoch 755/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 362.4028 - val_loss: 253.2666\n",
      "Epoch 756/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 347.3717 - val_loss: 269.9062\n",
      "Epoch 757/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.0083 - val_loss: 250.1310\n",
      "Epoch 758/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 380.0727 - val_loss: 292.2086\n",
      "Epoch 759/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 325.6138 - val_loss: 313.4378\n",
      "Epoch 760/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.5767 - val_loss: 258.1498\n",
      "Epoch 761/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 346.6088 - val_loss: 249.8669\n",
      "Epoch 762/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 345.6252 - val_loss: 358.6837\n",
      "Epoch 763/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 403.9376 - val_loss: 263.1342\n",
      "Epoch 764/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 350.6052 - val_loss: 273.6500\n",
      "Epoch 765/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 390.2045 - val_loss: 279.5591\n",
      "Epoch 766/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 371.8586 - val_loss: 326.5857\n",
      "Epoch 767/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 386.7599 - val_loss: 248.8878\n",
      "Epoch 768/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 415.0808 - val_loss: 345.7637\n",
      "Epoch 769/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 406.1704 - val_loss: 273.3645\n",
      "Epoch 770/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 361.5286 - val_loss: 359.5460\n",
      "Epoch 771/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 388.4398 - val_loss: 315.6338\n",
      "Epoch 772/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 369.7631 - val_loss: 231.4364\n",
      "Epoch 773/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 390.3501 - val_loss: 257.7090\n",
      "Epoch 774/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 353.8938 - val_loss: 260.7340\n",
      "Epoch 775/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 355.1583 - val_loss: 289.4968\n",
      "Epoch 776/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.8669 - val_loss: 263.5122\n",
      "Epoch 777/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 318.4054 - val_loss: 281.2847\n",
      "Epoch 778/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 356.8444 - val_loss: 283.6977\n",
      "Epoch 779/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.7117 - val_loss: 238.2434\n",
      "Epoch 780/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 366.4948 - val_loss: 290.7229\n",
      "Epoch 781/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 358.2167 - val_loss: 191.4508\n",
      "Epoch 782/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.7480 - val_loss: 335.8157\n",
      "Epoch 783/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 361.9315 - val_loss: 229.5339\n",
      "Epoch 784/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 362.7165 - val_loss: 313.7154\n",
      "Epoch 785/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 348.3308 - val_loss: 248.5162\n",
      "Epoch 786/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 359.7555 - val_loss: 258.0753\n",
      "Epoch 787/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.6752 - val_loss: 243.1444\n",
      "Epoch 788/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 362.2953 - val_loss: 344.2623\n",
      "Epoch 789/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 372.9055 - val_loss: 287.8242\n",
      "Epoch 790/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 302.5141 - val_loss: 214.6053\n",
      "Epoch 791/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.2097 - val_loss: 256.6603\n",
      "Epoch 792/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 382.0566 - val_loss: 256.5986\n",
      "Epoch 793/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.4138 - val_loss: 257.5386\n",
      "Epoch 794/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 339.4923 - val_loss: 274.0915\n",
      "Epoch 795/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 376.2477 - val_loss: 294.2817\n",
      "Epoch 796/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 361.7482 - val_loss: 274.1813\n",
      "Epoch 797/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 353.3001 - val_loss: 254.6825\n",
      "Epoch 798/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 359.0804 - val_loss: 261.0202\n",
      "Epoch 799/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 331.7330 - val_loss: 341.2247\n",
      "Epoch 800/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 355.8271 - val_loss: 317.6133\n",
      "Epoch 801/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 326.7685 - val_loss: 330.6851\n",
      "Epoch 802/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.1508 - val_loss: 269.1477\n",
      "Epoch 803/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 342.3703 - val_loss: 275.5567\n",
      "Epoch 804/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 315.8797 - val_loss: 275.8268\n",
      "Epoch 805/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.7213 - val_loss: 234.3426\n",
      "Epoch 806/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 342.8365 - val_loss: 238.4530\n",
      "Epoch 807/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 304.0974 - val_loss: 195.9464\n",
      "Epoch 808/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 284.9534 - val_loss: 328.4604\n",
      "Epoch 809/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 356.0560 - val_loss: 270.4585\n",
      "Epoch 810/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 323.2706 - val_loss: 254.6739\n",
      "Epoch 811/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 338.1322 - val_loss: 370.5898\n",
      "Epoch 812/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 332.2098 - val_loss: 270.0241\n",
      "Epoch 813/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 319.8958 - val_loss: 244.6243\n",
      "Epoch 814/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 318.0266 - val_loss: 285.5152\n",
      "Epoch 815/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 350.7363 - val_loss: 319.2967\n",
      "Epoch 816/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.3839 - val_loss: 315.7996\n",
      "Epoch 817/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.8903 - val_loss: 290.6634\n",
      "Epoch 818/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 368.6096 - val_loss: 258.8475\n",
      "Epoch 819/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.2848 - val_loss: 237.7092\n",
      "Epoch 820/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 312.5973 - val_loss: 279.5685\n",
      "Epoch 821/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 329.2547 - val_loss: 327.8405\n",
      "Epoch 822/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 414.8379 - val_loss: 303.6969\n",
      "Epoch 823/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 317.4607 - val_loss: 264.9451\n",
      "Epoch 824/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 336.5502 - val_loss: 283.9130\n",
      "Epoch 825/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 332.4336 - val_loss: 260.0679\n",
      "Epoch 826/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 326.2226 - val_loss: 266.9116\n",
      "Epoch 827/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.5305 - val_loss: 361.6376\n",
      "Epoch 828/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 374.3443 - val_loss: 374.6084\n",
      "Epoch 829/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 399.9634 - val_loss: 210.3148\n",
      "Epoch 830/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 345.3468 - val_loss: 231.9737\n",
      "Epoch 831/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 315.9656 - val_loss: 186.8456\n",
      "Epoch 832/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.5689 - val_loss: 243.3508\n",
      "Epoch 833/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 370.6923 - val_loss: 265.2877\n",
      "Epoch 834/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 351.8186 - val_loss: 273.3683\n",
      "Epoch 835/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 364.7461 - val_loss: 255.5547\n",
      "Epoch 836/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 354.6151 - val_loss: 285.8242\n",
      "Epoch 837/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 303.6997 - val_loss: 286.2169\n",
      "Epoch 838/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 340.8891 - val_loss: 288.5936\n",
      "Epoch 839/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.1297 - val_loss: 213.3547\n",
      "Epoch 840/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 296.3066 - val_loss: 311.3614\n",
      "Epoch 841/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.7493 - val_loss: 268.2662\n",
      "Epoch 842/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 350.7549 - val_loss: 261.8089\n",
      "Epoch 843/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 319.8278 - val_loss: 260.9515\n",
      "Epoch 844/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 295.2177 - val_loss: 258.3391\n",
      "Epoch 845/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 323.3036 - val_loss: 253.6810\n",
      "Epoch 846/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 325.9863 - val_loss: 224.3732\n",
      "Epoch 847/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 354.0461 - val_loss: 240.1676\n",
      "Epoch 848/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 351.6967 - val_loss: 275.2790\n",
      "Epoch 849/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 338.1427 - val_loss: 275.4938\n",
      "Epoch 850/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 317.8031 - val_loss: 293.9548\n",
      "Epoch 851/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 331.2863 - val_loss: 283.5262\n",
      "Epoch 852/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 354.5257 - val_loss: 369.3749\n",
      "Epoch 853/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.3492 - val_loss: 273.8568\n",
      "Epoch 854/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 326.9510 - val_loss: 322.0119\n",
      "Epoch 855/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 309.9294 - val_loss: 255.0202\n",
      "Epoch 856/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 308.3892 - val_loss: 338.7770\n",
      "Epoch 857/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 354.0331 - val_loss: 395.1293\n",
      "Epoch 858/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 379.1643 - val_loss: 363.9008\n",
      "Epoch 859/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.6753 - val_loss: 334.7122\n",
      "Epoch 860/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 347.0712 - val_loss: 388.4904\n",
      "Epoch 861/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 352.3679 - val_loss: 371.6732\n",
      "Epoch 862/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 356.1729 - val_loss: 391.5095\n",
      "Epoch 863/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 325.9810 - val_loss: 336.0334\n",
      "Epoch 864/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 339.0859 - val_loss: 303.8228\n",
      "Epoch 865/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 347.7553 - val_loss: 321.7939\n",
      "Epoch 866/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 352.8624 - val_loss: 337.1835\n",
      "Epoch 867/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 374.8390 - val_loss: 317.1042\n",
      "Epoch 868/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 367.2618 - val_loss: 306.8154\n",
      "Epoch 869/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 397.8515 - val_loss: 296.2495\n",
      "Epoch 870/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 365.6636 - val_loss: 242.2386\n",
      "Epoch 871/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 344.0467 - val_loss: 293.0612\n",
      "Epoch 872/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 350.0170 - val_loss: 263.6314\n",
      "Epoch 873/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 368.4127 - val_loss: 219.2668\n",
      "Epoch 874/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 320.5902 - val_loss: 184.1049\n",
      "Epoch 875/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 341.3761 - val_loss: 206.1053\n",
      "Epoch 876/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 320.6580 - val_loss: 178.5542\n",
      "Epoch 877/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 304.7982 - val_loss: 234.8736\n",
      "Epoch 878/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 300.3846 - val_loss: 256.3231\n",
      "Epoch 879/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 357.7228 - val_loss: 549.9434\n",
      "Epoch 880/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 450.8295 - val_loss: 296.5731\n",
      "Epoch 881/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 326.5385 - val_loss: 254.2755\n",
      "Epoch 882/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 321.4484 - val_loss: 278.3839\n",
      "Epoch 883/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 333.8632 - val_loss: 243.0506\n",
      "Epoch 884/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 296.4974 - val_loss: 219.0691\n",
      "Epoch 885/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 327.3485 - val_loss: 261.4963\n",
      "Epoch 886/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 334.0426 - val_loss: 187.8481\n",
      "Epoch 887/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 301.7101 - val_loss: 154.9688\n",
      "Epoch 888/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 307.0754 - val_loss: 234.3629\n",
      "Epoch 889/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 295.6973 - val_loss: 243.3661\n",
      "Epoch 890/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 308.7856 - val_loss: 212.6760\n",
      "Epoch 891/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 275.3762 - val_loss: 179.9341\n",
      "Epoch 892/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 288.8979 - val_loss: 165.2890\n",
      "Epoch 893/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 343.4912 - val_loss: 301.0938\n",
      "Epoch 894/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 363.7528 - val_loss: 350.1401\n",
      "Epoch 895/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 373.5797 - val_loss: 234.0998\n",
      "Epoch 896/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 313.8258 - val_loss: 332.8406\n",
      "Epoch 897/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 316.0625Restoring model weights from the end of the best epoch: 397.\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 294.8351 - val_loss: 227.6285\n",
      "Epoch 897: early stopping\n"
     ]
    }
   ],
   "source": [
    "trained_model, history = neural_network_model(train_input, train_target, want_verbose=1, seed=winner_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "867771c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_mape_calculator(model, test_input, test_target, start_target):\n",
    "    \n",
    "    errors = []\n",
    "    error_percent = []\n",
    "    \n",
    "    for i in range(len(test_target)):\n",
    "        prediction = model.predict(test_input.iloc[i:i+1])\n",
    "        error = np.abs(prediction - test_target[start_target + i])\n",
    "        errors.append(error)\n",
    "        error_percent.append(error / test_target[start_target + i])\n",
    "        print(f\"Month-{i + 1} - Error: {error}\")\n",
    "    \n",
    "    mae = np.mean(errors)\n",
    "    mape = np.mean(error_percent) \n",
    "\n",
    "    return errors, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09914e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "Month-1 - Error: [[64.80965]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-2 - Error: [[7.8657837]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-3 - Error: [[58.49611]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-4 - Error: [[10.869293]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-5 - Error: [[37.35756]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-6 - Error: [[27.974869]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-7 - Error: [[5.062378]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-8 - Error: [[22.417236]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-9 - Error: [[14.459473]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-10 - Error: [[35.19406]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-11 - Error: [[60.893997]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-12 - Error: [[18.251923]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-13 - Error: [[62.312378]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-14 - Error: [[44.783646]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-15 - Error: [[41.80838]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-16 - Error: [[87.25871]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-17 - Error: [[14.599335]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-18 - Error: [[16.398773]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-19 - Error: [[120.57747]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-20 - Error: [[326.2077]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Month-21 - Error: [[346.18912]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Month-22 - Error: [[325.74164]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-23 - Error: [[279.59436]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Month-24 - Error: [[362.7655]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-25 - Error: [[320.77576]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-26 - Error: [[312.95807]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-27 - Error: [[323.6491]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-28 - Error: [[353.25812]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-29 - Error: [[271.5595]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-30 - Error: [[360.68045]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-31 - Error: [[373.22168]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Month-32 - Error: [[377.29065]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-33 - Error: [[420.01465]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-34 - Error: [[389.83368]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-35 - Error: [[356.59332]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Month-36 - Error: [[392.06665]]\n"
     ]
    }
   ],
   "source": [
    "errors, mae, mape = mae_mape_calculator(trained_model, test_input, test_target, 168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16807ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.54974"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.70035625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mae)\n",
    "display(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "447b6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_mae_mape_calculator(model, test_input, test_target, start_target):\n",
    "    \n",
    "    errors = []\n",
    "    error_percent = []\n",
    "    \n",
    "    target_sum = 0\n",
    "    prediction_sum = 0\n",
    "    \n",
    "    for i in range(len(test_target)):\n",
    "        if i % 12 == 0 and i != 0:\n",
    "            error = np.abs(target_sum - prediction_sum)\n",
    "            errors.append(error)\n",
    "            error_percent.append(error / target_sum)\n",
    "            print(f\"Ano-{i%12}: |Prediction{prediction_sum} - Target[{target_sum}]| =  Error: {error}; MAPE:{abs(prediction_sum - target_sum)/target_sum}\")\n",
    "            target_sum = 0\n",
    "            prediction_sum = 0\n",
    "            \n",
    "        prediction = model.predict(test_input.iloc[i:i+1])\n",
    "        target_sum += test_target[start_target + i]\n",
    "        prediction_sum += prediction\n",
    "        \n",
    "    error = np.abs(target_sum - prediction_sum)\n",
    "    errors.append(error)\n",
    "    error_percent.append(error / target_sum)\n",
    "    print(f\"Ano-{i%12}: |Prediction{prediction_sum} - Target[{target_sum}]| =  Error: {error}; MAPE:{abs(prediction_sum - target_sum)/target_sum}\")\n",
    "        \n",
    "    mae = np.mean(errors)\n",
    "    mape = np.mean(error_percent) \n",
    "\n",
    "    return errors, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04d784f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ano-0: |Prediction[[3408.0269]] - Target[3060.1059999999998]| =  Error: [[347.9209]]; MAPE:[[0.1136957]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ano-0: |Prediction[[1457.3331]] - Target[3013.2439999999997]| =  Error: [[1555.9108]]; MAPE:[[0.5163574]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ano-11: |Prediction[[-997.43365]] - Target[3254.4680000000003]| =  Error: [[4251.902]]; MAPE:[[1.3064814]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[347.9209]], dtype=float32),\n",
       " array([[1555.9108]], dtype=float32),\n",
       " array([[4251.902]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2051.9111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.64551145"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors, mae, mape = year_mae_mape_calculator(trained_model, test_input, test_target, 168)\n",
    "display(errors)\n",
    "display(mae)\n",
    "display(mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1949ad",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
