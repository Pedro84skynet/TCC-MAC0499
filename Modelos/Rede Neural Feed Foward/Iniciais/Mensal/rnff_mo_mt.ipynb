{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887ab6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422e23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb2284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'Mato Grosso - Consumo de Cimento (t)'\n",
    "split_index = 191 #Referente aos 230 anos de input  \n",
    "train_split = split_index + 1 -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc652c77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Mato Grosso - Produção de Cimento (t)</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Mato Grosso - Desemprego</th>\n",
       "      <th>Mato Grosso - IDH</th>\n",
       "      <th>Mato Grosso - PIB - Estadual</th>\n",
       "      <th>Mato Grosso - PIB - Construção Civil</th>\n",
       "      <th>Mato Grosso - PIB - Per Capita</th>\n",
       "      <th>Mato Grosso - PIB - Preços de Mercado</th>\n",
       "      <th>Mato Grosso - value</th>\n",
       "      <th>Mato Grosso - Consumo de Cimento (t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-1</td>\n",
       "      <td>50.917240</td>\n",
       "      <td>0.724032</td>\n",
       "      <td>11.520143</td>\n",
       "      <td>1.639718</td>\n",
       "      <td>1.036534</td>\n",
       "      <td>1.772069e+08</td>\n",
       "      <td>7.330309e+06</td>\n",
       "      <td>0.969649</td>\n",
       "      <td>8.297178</td>\n",
       "      <td>0.745147</td>\n",
       "      <td>6.084564e+07</td>\n",
       "      <td>2.670588e+06</td>\n",
       "      <td>16.831332</td>\n",
       "      <td>5.118946e+07</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>47.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-2</td>\n",
       "      <td>50.851586</td>\n",
       "      <td>0.690297</td>\n",
       "      <td>11.189862</td>\n",
       "      <td>1.378899</td>\n",
       "      <td>0.993449</td>\n",
       "      <td>1.773884e+08</td>\n",
       "      <td>7.335910e+06</td>\n",
       "      <td>0.950783</td>\n",
       "      <td>8.291317</td>\n",
       "      <td>0.745308</td>\n",
       "      <td>6.092768e+07</td>\n",
       "      <td>2.672688e+06</td>\n",
       "      <td>16.835967</td>\n",
       "      <td>5.121614e+07</td>\n",
       "      <td>0.333615</td>\n",
       "      <td>45.387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-3</td>\n",
       "      <td>50.970549</td>\n",
       "      <td>0.669681</td>\n",
       "      <td>10.820792</td>\n",
       "      <td>1.924317</td>\n",
       "      <td>0.973020</td>\n",
       "      <td>1.775699e+08</td>\n",
       "      <td>7.341511e+06</td>\n",
       "      <td>0.938332</td>\n",
       "      <td>8.285457</td>\n",
       "      <td>0.745469</td>\n",
       "      <td>6.100973e+07</td>\n",
       "      <td>2.674788e+06</td>\n",
       "      <td>16.840602</td>\n",
       "      <td>5.124282e+07</td>\n",
       "      <td>0.334864</td>\n",
       "      <td>44.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-4</td>\n",
       "      <td>51.049978</td>\n",
       "      <td>0.660494</td>\n",
       "      <td>10.417840</td>\n",
       "      <td>1.331174</td>\n",
       "      <td>0.940489</td>\n",
       "      <td>1.777514e+08</td>\n",
       "      <td>7.347112e+06</td>\n",
       "      <td>0.926401</td>\n",
       "      <td>8.279596</td>\n",
       "      <td>0.745630</td>\n",
       "      <td>6.109177e+07</td>\n",
       "      <td>2.676888e+06</td>\n",
       "      <td>16.845237</td>\n",
       "      <td>5.126950e+07</td>\n",
       "      <td>0.336048</td>\n",
       "      <td>45.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-5</td>\n",
       "      <td>51.567529</td>\n",
       "      <td>0.648337</td>\n",
       "      <td>9.959690</td>\n",
       "      <td>1.736072</td>\n",
       "      <td>0.917493</td>\n",
       "      <td>1.779329e+08</td>\n",
       "      <td>7.352713e+06</td>\n",
       "      <td>0.951683</td>\n",
       "      <td>8.273735</td>\n",
       "      <td>0.745791</td>\n",
       "      <td>6.117381e+07</td>\n",
       "      <td>2.678988e+06</td>\n",
       "      <td>16.849872</td>\n",
       "      <td>5.129619e+07</td>\n",
       "      <td>0.336447</td>\n",
       "      <td>56.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2022-8</td>\n",
       "      <td>119.544326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600607</td>\n",
       "      <td>192.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2022-9</td>\n",
       "      <td>118.223448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598965</td>\n",
       "      <td>183.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2022-10</td>\n",
       "      <td>117.524152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.596866</td>\n",
       "      <td>180.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2022-11</td>\n",
       "      <td>116.430559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.594246</td>\n",
       "      <td>161.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2022-12</td>\n",
       "      <td>116.846011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.591053</td>\n",
       "      <td>161.683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Mato Grosso - Produção de Cimento (t)  \\\n",
       "0       2003-1                              50.917240   \n",
       "1       2003-2                              50.851586   \n",
       "2       2003-3                              50.970549   \n",
       "3       2003-4                              51.049978   \n",
       "4       2003-5                              51.567529   \n",
       "..         ...                                    ...   \n",
       "235     2022-8                             119.544326   \n",
       "236     2022-9                             118.223448   \n",
       "237    2022-10                             117.524152   \n",
       "238    2022-11                             116.430559   \n",
       "239    2022-12                             116.846011   \n",
       "\n",
       "      IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "0                                            0.724032   \n",
       "1                                            0.690297   \n",
       "2                                            0.669681   \n",
       "3                                            0.660494   \n",
       "4                                            0.648337   \n",
       "..                                                ...   \n",
       "235                                               NaN   \n",
       "236                                               NaN   \n",
       "237                                               NaN   \n",
       "238                                               NaN   \n",
       "239                                               NaN   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI     População  \\\n",
       "0                        11.520143        1.639718  1.036534  1.772069e+08   \n",
       "1                        11.189862        1.378899  0.993449  1.773884e+08   \n",
       "2                        10.820792        1.924317  0.973020  1.775699e+08   \n",
       "3                        10.417840        1.331174  0.940489  1.777514e+08   \n",
       "4                         9.959690        1.736072  0.917493  1.779329e+08   \n",
       "..                             ...             ...       ...           ...   \n",
       "235                            NaN             NaN       NaN           NaN   \n",
       "236                            NaN             NaN       NaN           NaN   \n",
       "237                            NaN             NaN       NaN           NaN   \n",
       "238                            NaN             NaN       NaN           NaN   \n",
       "239                            NaN             NaN       NaN           NaN   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  \\\n",
       "0                              7.330309e+06   0.969649   \n",
       "1                              7.335910e+06   0.950783   \n",
       "2                              7.341511e+06   0.938332   \n",
       "3                              7.347112e+06   0.926401   \n",
       "4                              7.352713e+06   0.951683   \n",
       "..                                      ...        ...   \n",
       "235                                     NaN        NaN   \n",
       "236                                     NaN        NaN   \n",
       "237                                     NaN        NaN   \n",
       "238                                     NaN        NaN   \n",
       "239                                     NaN        NaN   \n",
       "\n",
       "     Mato Grosso - Desemprego  Mato Grosso - IDH  \\\n",
       "0                    8.297178           0.745147   \n",
       "1                    8.291317           0.745308   \n",
       "2                    8.285457           0.745469   \n",
       "3                    8.279596           0.745630   \n",
       "4                    8.273735           0.745791   \n",
       "..                        ...                ...   \n",
       "235                       NaN                NaN   \n",
       "236                       NaN                NaN   \n",
       "237                       NaN                NaN   \n",
       "238                       NaN                NaN   \n",
       "239                       NaN                NaN   \n",
       "\n",
       "     Mato Grosso - PIB - Estadual  Mato Grosso - PIB - Construção Civil  \\\n",
       "0                    6.084564e+07                          2.670588e+06   \n",
       "1                    6.092768e+07                          2.672688e+06   \n",
       "2                    6.100973e+07                          2.674788e+06   \n",
       "3                    6.109177e+07                          2.676888e+06   \n",
       "4                    6.117381e+07                          2.678988e+06   \n",
       "..                            ...                                   ...   \n",
       "235                           NaN                                   NaN   \n",
       "236                           NaN                                   NaN   \n",
       "237                           NaN                                   NaN   \n",
       "238                           NaN                                   NaN   \n",
       "239                           NaN                                   NaN   \n",
       "\n",
       "     Mato Grosso - PIB - Per Capita  Mato Grosso - PIB - Preços de Mercado  \\\n",
       "0                         16.831332                           5.118946e+07   \n",
       "1                         16.835967                           5.121614e+07   \n",
       "2                         16.840602                           5.124282e+07   \n",
       "3                         16.845237                           5.126950e+07   \n",
       "4                         16.849872                           5.129619e+07   \n",
       "..                              ...                                    ...   \n",
       "235                             NaN                                    NaN   \n",
       "236                             NaN                                    NaN   \n",
       "237                             NaN                                    NaN   \n",
       "238                             NaN                                    NaN   \n",
       "239                             NaN                                    NaN   \n",
       "\n",
       "     Mato Grosso - value  Mato Grosso - Consumo de Cimento (t)  \n",
       "0               0.331800                                47.470  \n",
       "1               0.333615                                45.387  \n",
       "2               0.334864                                44.907  \n",
       "3               0.336048                                45.467  \n",
       "4               0.336447                                56.246  \n",
       "..                   ...                                   ...  \n",
       "235             0.600607                               192.533  \n",
       "236             0.598965                               183.895  \n",
       "237             0.596866                               180.525  \n",
       "238             0.594246                               161.683  \n",
       "239             0.591053                               161.683  \n",
       "\n",
       "[240 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('2003_mo_model_input_MT.csv')\n",
    "data = data[[col for col in data.columns if col != subject] + [subject]] #Seta consumo (target) para a coluna final\n",
    "data =data.drop([' NFSP - Fluxo Mensal (Milhões de reais)'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9a79f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mato Grosso - Produção de Cimento (t)</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Mato Grosso - Desemprego</th>\n",
       "      <th>Mato Grosso - IDH</th>\n",
       "      <th>Mato Grosso - PIB - Estadual</th>\n",
       "      <th>Mato Grosso - PIB - Construção Civil</th>\n",
       "      <th>Mato Grosso - PIB - Per Capita</th>\n",
       "      <th>Mato Grosso - PIB - Preços de Mercado</th>\n",
       "      <th>Mato Grosso - value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.582351</td>\n",
       "      <td>2.723741</td>\n",
       "      <td>4.398348</td>\n",
       "      <td>2.052494</td>\n",
       "      <td>3.890153</td>\n",
       "      <td>-2.042341</td>\n",
       "      <td>-2.389042</td>\n",
       "      <td>3.122582</td>\n",
       "      <td>1.216878</td>\n",
       "      <td>-2.486031</td>\n",
       "      <td>-1.594683</td>\n",
       "      <td>-2.079312</td>\n",
       "      <td>-1.524159</td>\n",
       "      <td>-1.566766</td>\n",
       "      <td>-1.015691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.585655</td>\n",
       "      <td>2.350880</td>\n",
       "      <td>4.222509</td>\n",
       "      <td>1.242280</td>\n",
       "      <td>3.551840</td>\n",
       "      <td>-2.014760</td>\n",
       "      <td>-2.352139</td>\n",
       "      <td>2.970356</td>\n",
       "      <td>1.204107</td>\n",
       "      <td>-2.439296</td>\n",
       "      <td>-1.579788</td>\n",
       "      <td>-2.049593</td>\n",
       "      <td>-1.511861</td>\n",
       "      <td>-1.551683</td>\n",
       "      <td>-0.977876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.579669</td>\n",
       "      <td>2.123016</td>\n",
       "      <td>4.026019</td>\n",
       "      <td>2.936580</td>\n",
       "      <td>3.391423</td>\n",
       "      <td>-1.987179</td>\n",
       "      <td>-2.315236</td>\n",
       "      <td>2.869895</td>\n",
       "      <td>1.191337</td>\n",
       "      <td>-2.392562</td>\n",
       "      <td>-1.564893</td>\n",
       "      <td>-2.019873</td>\n",
       "      <td>-1.499562</td>\n",
       "      <td>-1.536601</td>\n",
       "      <td>-0.951860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.575671</td>\n",
       "      <td>2.021477</td>\n",
       "      <td>3.811492</td>\n",
       "      <td>1.094024</td>\n",
       "      <td>3.135979</td>\n",
       "      <td>-1.959598</td>\n",
       "      <td>-2.278333</td>\n",
       "      <td>2.773628</td>\n",
       "      <td>1.178566</td>\n",
       "      <td>-2.345827</td>\n",
       "      <td>-1.549998</td>\n",
       "      <td>-1.990154</td>\n",
       "      <td>-1.487263</td>\n",
       "      <td>-1.521519</td>\n",
       "      <td>-0.927186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.549625</td>\n",
       "      <td>1.887113</td>\n",
       "      <td>3.567576</td>\n",
       "      <td>2.351810</td>\n",
       "      <td>2.955412</td>\n",
       "      <td>-1.932017</td>\n",
       "      <td>-2.241431</td>\n",
       "      <td>2.977624</td>\n",
       "      <td>1.165795</td>\n",
       "      <td>-2.299093</td>\n",
       "      <td>-1.535104</td>\n",
       "      <td>-1.960435</td>\n",
       "      <td>-1.474964</td>\n",
       "      <td>-1.506437</td>\n",
       "      <td>-0.918874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.683341</td>\n",
       "      <td>-2.010387</td>\n",
       "      <td>-0.572934</td>\n",
       "      <td>-1.281958</td>\n",
       "      <td>0.589021</td>\n",
       "      <td>1.365911</td>\n",
       "      <td>0.389193</td>\n",
       "      <td>-1.749976</td>\n",
       "      <td>-1.462024</td>\n",
       "      <td>0.554708</td>\n",
       "      <td>1.263263</td>\n",
       "      <td>0.134707</td>\n",
       "      <td>1.180043</td>\n",
       "      <td>1.228996</td>\n",
       "      <td>0.893787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.683776</td>\n",
       "      <td>-1.870713</td>\n",
       "      <td>-0.588777</td>\n",
       "      <td>-1.358588</td>\n",
       "      <td>1.043728</td>\n",
       "      <td>1.376610</td>\n",
       "      <td>0.370392</td>\n",
       "      <td>-1.593005</td>\n",
       "      <td>-1.475003</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>1.258638</td>\n",
       "      <td>0.140184</td>\n",
       "      <td>1.177055</td>\n",
       "      <td>1.225355</td>\n",
       "      <td>0.914063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.711014</td>\n",
       "      <td>-1.806230</td>\n",
       "      <td>-0.612606</td>\n",
       "      <td>-1.511565</td>\n",
       "      <td>1.387010</td>\n",
       "      <td>1.387308</td>\n",
       "      <td>0.351592</td>\n",
       "      <td>-1.351489</td>\n",
       "      <td>-1.487981</td>\n",
       "      <td>0.461157</td>\n",
       "      <td>1.254013</td>\n",
       "      <td>0.145662</td>\n",
       "      <td>1.174067</td>\n",
       "      <td>1.221714</td>\n",
       "      <td>0.954337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.715019</td>\n",
       "      <td>-1.727496</td>\n",
       "      <td>-0.640956</td>\n",
       "      <td>-1.421708</td>\n",
       "      <td>1.815728</td>\n",
       "      <td>1.398006</td>\n",
       "      <td>0.332791</td>\n",
       "      <td>-1.198492</td>\n",
       "      <td>-1.500960</td>\n",
       "      <td>0.414381</td>\n",
       "      <td>1.249388</td>\n",
       "      <td>0.151139</td>\n",
       "      <td>1.171079</td>\n",
       "      <td>1.218073</td>\n",
       "      <td>0.995597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.730334</td>\n",
       "      <td>-1.391283</td>\n",
       "      <td>-0.663358</td>\n",
       "      <td>-1.593676</td>\n",
       "      <td>2.181106</td>\n",
       "      <td>1.408704</td>\n",
       "      <td>0.313991</td>\n",
       "      <td>-1.100894</td>\n",
       "      <td>-1.513939</td>\n",
       "      <td>0.367605</td>\n",
       "      <td>1.244762</td>\n",
       "      <td>0.156616</td>\n",
       "      <td>1.168091</td>\n",
       "      <td>1.214432</td>\n",
       "      <td>1.046376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Mato Grosso - Produção de Cimento (t)  \\\n",
       "0                                -1.582351   \n",
       "1                                -1.585655   \n",
       "2                                -1.579669   \n",
       "3                                -1.575671   \n",
       "4                                -1.549625   \n",
       "..                                     ...   \n",
       "187                               0.683341   \n",
       "188                               0.683776   \n",
       "189                               0.711014   \n",
       "190                               0.715019   \n",
       "191                               0.730334   \n",
       "\n",
       "      IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "0                                            2.723741   \n",
       "1                                            2.350880   \n",
       "2                                            2.123016   \n",
       "3                                            2.021477   \n",
       "4                                            1.887113   \n",
       "..                                                ...   \n",
       "187                                         -2.010387   \n",
       "188                                         -1.870713   \n",
       "189                                         -1.806230   \n",
       "190                                         -1.727496   \n",
       "191                                         -1.391283   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI  População  \\\n",
       "0                         4.398348        2.052494  3.890153  -2.042341   \n",
       "1                         4.222509        1.242280  3.551840  -2.014760   \n",
       "2                         4.026019        2.936580  3.391423  -1.987179   \n",
       "3                         3.811492        1.094024  3.135979  -1.959598   \n",
       "4                         3.567576        2.351810  2.955412  -1.932017   \n",
       "..                             ...             ...       ...        ...   \n",
       "187                      -0.572934       -1.281958  0.589021   1.365911   \n",
       "188                      -0.588777       -1.358588  1.043728   1.376610   \n",
       "189                      -0.612606       -1.511565  1.387010   1.387308   \n",
       "190                      -0.640956       -1.421708  1.815728   1.398006   \n",
       "191                      -0.663358       -1.593676  2.181106   1.408704   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  \\\n",
       "0                                 -2.389042   3.122582   \n",
       "1                                 -2.352139   2.970356   \n",
       "2                                 -2.315236   2.869895   \n",
       "3                                 -2.278333   2.773628   \n",
       "4                                 -2.241431   2.977624   \n",
       "..                                      ...        ...   \n",
       "187                                0.389193  -1.749976   \n",
       "188                                0.370392  -1.593005   \n",
       "189                                0.351592  -1.351489   \n",
       "190                                0.332791  -1.198492   \n",
       "191                                0.313991  -1.100894   \n",
       "\n",
       "     Mato Grosso - Desemprego  Mato Grosso - IDH  \\\n",
       "0                    1.216878          -2.486031   \n",
       "1                    1.204107          -2.439296   \n",
       "2                    1.191337          -2.392562   \n",
       "3                    1.178566          -2.345827   \n",
       "4                    1.165795          -2.299093   \n",
       "..                        ...                ...   \n",
       "187                 -1.462024           0.554708   \n",
       "188                 -1.475003           0.507932   \n",
       "189                 -1.487981           0.461157   \n",
       "190                 -1.500960           0.414381   \n",
       "191                 -1.513939           0.367605   \n",
       "\n",
       "     Mato Grosso - PIB - Estadual  Mato Grosso - PIB - Construção Civil  \\\n",
       "0                       -1.594683                             -2.079312   \n",
       "1                       -1.579788                             -2.049593   \n",
       "2                       -1.564893                             -2.019873   \n",
       "3                       -1.549998                             -1.990154   \n",
       "4                       -1.535104                             -1.960435   \n",
       "..                            ...                                   ...   \n",
       "187                      1.263263                              0.134707   \n",
       "188                      1.258638                              0.140184   \n",
       "189                      1.254013                              0.145662   \n",
       "190                      1.249388                              0.151139   \n",
       "191                      1.244762                              0.156616   \n",
       "\n",
       "     Mato Grosso - PIB - Per Capita  Mato Grosso - PIB - Preços de Mercado  \\\n",
       "0                         -1.524159                              -1.566766   \n",
       "1                         -1.511861                              -1.551683   \n",
       "2                         -1.499562                              -1.536601   \n",
       "3                         -1.487263                              -1.521519   \n",
       "4                         -1.474964                              -1.506437   \n",
       "..                              ...                                    ...   \n",
       "187                        1.180043                               1.228996   \n",
       "188                        1.177055                               1.225355   \n",
       "189                        1.174067                               1.221714   \n",
       "190                        1.171079                               1.218073   \n",
       "191                        1.168091                               1.214432   \n",
       "\n",
       "     Mato Grosso - value  \n",
       "0              -1.015691  \n",
       "1              -0.977876  \n",
       "2              -0.951860  \n",
       "3              -0.927186  \n",
       "4              -0.918874  \n",
       "..                   ...  \n",
       "187             0.893787  \n",
       "188             0.914063  \n",
       "189             0.954337  \n",
       "190             0.995597  \n",
       "191             1.046376  \n",
       "\n",
       "[192 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = data.iloc[:split_index + 1,1:-1]\n",
    "mean = np.mean(input_data, axis=0)\n",
    "stddev =  np.std(input_data, axis=0)\n",
    "input_data = ((input_data - mean) /stddev)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb83d26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      51.972\n",
       "1      40.729\n",
       "2      54.446\n",
       "3      51.788\n",
       "4      58.702\n",
       "        ...  \n",
       "235       NaN\n",
       "236       NaN\n",
       "237       NaN\n",
       "238       NaN\n",
       "239       NaN\n",
       "Name: Mato Grosso - Consumo de Cimento (t), Length: 240, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shift para prever futuro e não presente\n",
    "target_data = data[subject].shift(-12)\n",
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bdb2b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mato Grosso - Produção de Cimento (t)</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Mato Grosso - Desemprego</th>\n",
       "      <th>Mato Grosso - IDH</th>\n",
       "      <th>Mato Grosso - PIB - Estadual</th>\n",
       "      <th>Mato Grosso - PIB - Construção Civil</th>\n",
       "      <th>Mato Grosso - PIB - Per Capita</th>\n",
       "      <th>Mato Grosso - PIB - Preços de Mercado</th>\n",
       "      <th>Mato Grosso - value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.582351</td>\n",
       "      <td>2.723741</td>\n",
       "      <td>4.398348</td>\n",
       "      <td>2.052494</td>\n",
       "      <td>3.890153</td>\n",
       "      <td>-2.042341</td>\n",
       "      <td>-2.389042</td>\n",
       "      <td>3.122582</td>\n",
       "      <td>1.216878</td>\n",
       "      <td>-2.486031</td>\n",
       "      <td>-1.594683</td>\n",
       "      <td>-2.079312</td>\n",
       "      <td>-1.524159</td>\n",
       "      <td>-1.566766</td>\n",
       "      <td>-1.015691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.585655</td>\n",
       "      <td>2.350880</td>\n",
       "      <td>4.222509</td>\n",
       "      <td>1.242280</td>\n",
       "      <td>3.551840</td>\n",
       "      <td>-2.014760</td>\n",
       "      <td>-2.352139</td>\n",
       "      <td>2.970356</td>\n",
       "      <td>1.204107</td>\n",
       "      <td>-2.439296</td>\n",
       "      <td>-1.579788</td>\n",
       "      <td>-2.049593</td>\n",
       "      <td>-1.511861</td>\n",
       "      <td>-1.551683</td>\n",
       "      <td>-0.977876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.579669</td>\n",
       "      <td>2.123016</td>\n",
       "      <td>4.026019</td>\n",
       "      <td>2.936580</td>\n",
       "      <td>3.391423</td>\n",
       "      <td>-1.987179</td>\n",
       "      <td>-2.315236</td>\n",
       "      <td>2.869895</td>\n",
       "      <td>1.191337</td>\n",
       "      <td>-2.392562</td>\n",
       "      <td>-1.564893</td>\n",
       "      <td>-2.019873</td>\n",
       "      <td>-1.499562</td>\n",
       "      <td>-1.536601</td>\n",
       "      <td>-0.951860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.575671</td>\n",
       "      <td>2.021477</td>\n",
       "      <td>3.811492</td>\n",
       "      <td>1.094024</td>\n",
       "      <td>3.135979</td>\n",
       "      <td>-1.959598</td>\n",
       "      <td>-2.278333</td>\n",
       "      <td>2.773628</td>\n",
       "      <td>1.178566</td>\n",
       "      <td>-2.345827</td>\n",
       "      <td>-1.549998</td>\n",
       "      <td>-1.990154</td>\n",
       "      <td>-1.487263</td>\n",
       "      <td>-1.521519</td>\n",
       "      <td>-0.927186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.549625</td>\n",
       "      <td>1.887113</td>\n",
       "      <td>3.567576</td>\n",
       "      <td>2.351810</td>\n",
       "      <td>2.955412</td>\n",
       "      <td>-1.932017</td>\n",
       "      <td>-2.241431</td>\n",
       "      <td>2.977624</td>\n",
       "      <td>1.165795</td>\n",
       "      <td>-2.299093</td>\n",
       "      <td>-1.535104</td>\n",
       "      <td>-1.960435</td>\n",
       "      <td>-1.474964</td>\n",
       "      <td>-1.506437</td>\n",
       "      <td>-0.918874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.341278</td>\n",
       "      <td>-0.214006</td>\n",
       "      <td>-0.607704</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>-1.233012</td>\n",
       "      <td>1.031384</td>\n",
       "      <td>0.819304</td>\n",
       "      <td>-0.883659</td>\n",
       "      <td>-0.104237</td>\n",
       "      <td>1.450474</td>\n",
       "      <td>1.217726</td>\n",
       "      <td>0.274296</td>\n",
       "      <td>1.163104</td>\n",
       "      <td>1.185879</td>\n",
       "      <td>1.167738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.322813</td>\n",
       "      <td>-0.434717</td>\n",
       "      <td>-0.620523</td>\n",
       "      <td>0.222389</td>\n",
       "      <td>-1.299304</td>\n",
       "      <td>1.042716</td>\n",
       "      <td>0.808136</td>\n",
       "      <td>-0.950771</td>\n",
       "      <td>-0.191347</td>\n",
       "      <td>1.427979</td>\n",
       "      <td>1.221923</td>\n",
       "      <td>0.260732</td>\n",
       "      <td>1.162467</td>\n",
       "      <td>1.188073</td>\n",
       "      <td>1.148285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.303209</td>\n",
       "      <td>-0.524091</td>\n",
       "      <td>-0.631530</td>\n",
       "      <td>0.191929</td>\n",
       "      <td>-1.248662</td>\n",
       "      <td>1.054049</td>\n",
       "      <td>0.796969</td>\n",
       "      <td>-1.028465</td>\n",
       "      <td>-0.278458</td>\n",
       "      <td>1.405484</td>\n",
       "      <td>1.226120</td>\n",
       "      <td>0.247169</td>\n",
       "      <td>1.161831</td>\n",
       "      <td>1.190267</td>\n",
       "      <td>1.127808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.275768</td>\n",
       "      <td>-0.614500</td>\n",
       "      <td>-0.640320</td>\n",
       "      <td>0.385687</td>\n",
       "      <td>-1.068274</td>\n",
       "      <td>1.065381</td>\n",
       "      <td>0.785801</td>\n",
       "      <td>-1.103668</td>\n",
       "      <td>-0.365569</td>\n",
       "      <td>1.382988</td>\n",
       "      <td>1.230317</td>\n",
       "      <td>0.233605</td>\n",
       "      <td>1.161195</td>\n",
       "      <td>1.192461</td>\n",
       "      <td>1.108543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.305350</td>\n",
       "      <td>-0.552198</td>\n",
       "      <td>-0.649685</td>\n",
       "      <td>0.712055</td>\n",
       "      <td>-1.035336</td>\n",
       "      <td>1.076713</td>\n",
       "      <td>0.774634</td>\n",
       "      <td>-0.978419</td>\n",
       "      <td>-0.452680</td>\n",
       "      <td>1.360493</td>\n",
       "      <td>1.234514</td>\n",
       "      <td>0.220042</td>\n",
       "      <td>1.160559</td>\n",
       "      <td>1.194654</td>\n",
       "      <td>1.088390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Mato Grosso - Produção de Cimento (t)  \\\n",
       "0                                -1.582351   \n",
       "1                                -1.585655   \n",
       "2                                -1.579669   \n",
       "3                                -1.575671   \n",
       "4                                -1.549625   \n",
       "..                                     ...   \n",
       "157                               0.341278   \n",
       "158                               0.322813   \n",
       "159                               0.303209   \n",
       "160                               0.275768   \n",
       "161                               0.305350   \n",
       "\n",
       "      IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "0                                            2.723741   \n",
       "1                                            2.350880   \n",
       "2                                            2.123016   \n",
       "3                                            2.021477   \n",
       "4                                            1.887113   \n",
       "..                                                ...   \n",
       "157                                         -0.214006   \n",
       "158                                         -0.434717   \n",
       "159                                         -0.524091   \n",
       "160                                         -0.614500   \n",
       "161                                         -0.552198   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI  População  \\\n",
       "0                         4.398348        2.052494  3.890153  -2.042341   \n",
       "1                         4.222509        1.242280  3.551840  -2.014760   \n",
       "2                         4.026019        2.936580  3.391423  -1.987179   \n",
       "3                         3.811492        1.094024  3.135979  -1.959598   \n",
       "4                         3.567576        2.351810  2.955412  -1.932017   \n",
       "..                             ...             ...       ...        ...   \n",
       "157                      -0.607704        0.059280 -1.233012   1.031384   \n",
       "158                      -0.620523        0.222389 -1.299304   1.042716   \n",
       "159                      -0.631530        0.191929 -1.248662   1.054049   \n",
       "160                      -0.640320        0.385687 -1.068274   1.065381   \n",
       "161                      -0.649685        0.712055 -1.035336   1.076713   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  \\\n",
       "0                                 -2.389042   3.122582   \n",
       "1                                 -2.352139   2.970356   \n",
       "2                                 -2.315236   2.869895   \n",
       "3                                 -2.278333   2.773628   \n",
       "4                                 -2.241431   2.977624   \n",
       "..                                      ...        ...   \n",
       "157                                0.819304  -0.883659   \n",
       "158                                0.808136  -0.950771   \n",
       "159                                0.796969  -1.028465   \n",
       "160                                0.785801  -1.103668   \n",
       "161                                0.774634  -0.978419   \n",
       "\n",
       "     Mato Grosso - Desemprego  Mato Grosso - IDH  \\\n",
       "0                    1.216878          -2.486031   \n",
       "1                    1.204107          -2.439296   \n",
       "2                    1.191337          -2.392562   \n",
       "3                    1.178566          -2.345827   \n",
       "4                    1.165795          -2.299093   \n",
       "..                        ...                ...   \n",
       "157                 -0.104237           1.450474   \n",
       "158                 -0.191347           1.427979   \n",
       "159                 -0.278458           1.405484   \n",
       "160                 -0.365569           1.382988   \n",
       "161                 -0.452680           1.360493   \n",
       "\n",
       "     Mato Grosso - PIB - Estadual  Mato Grosso - PIB - Construção Civil  \\\n",
       "0                       -1.594683                             -2.079312   \n",
       "1                       -1.579788                             -2.049593   \n",
       "2                       -1.564893                             -2.019873   \n",
       "3                       -1.549998                             -1.990154   \n",
       "4                       -1.535104                             -1.960435   \n",
       "..                            ...                                   ...   \n",
       "157                      1.217726                              0.274296   \n",
       "158                      1.221923                              0.260732   \n",
       "159                      1.226120                              0.247169   \n",
       "160                      1.230317                              0.233605   \n",
       "161                      1.234514                              0.220042   \n",
       "\n",
       "     Mato Grosso - PIB - Per Capita  Mato Grosso - PIB - Preços de Mercado  \\\n",
       "0                         -1.524159                              -1.566766   \n",
       "1                         -1.511861                              -1.551683   \n",
       "2                         -1.499562                              -1.536601   \n",
       "3                         -1.487263                              -1.521519   \n",
       "4                         -1.474964                              -1.506437   \n",
       "..                              ...                                    ...   \n",
       "157                        1.163104                               1.185879   \n",
       "158                        1.162467                               1.188073   \n",
       "159                        1.161831                               1.190267   \n",
       "160                        1.161195                               1.192461   \n",
       "161                        1.160559                               1.194654   \n",
       "\n",
       "     Mato Grosso - value  \n",
       "0              -1.015691  \n",
       "1              -0.977876  \n",
       "2              -0.951860  \n",
       "3              -0.927186  \n",
       "4              -0.918874  \n",
       "..                   ...  \n",
       "157             1.167738  \n",
       "158             1.148285  \n",
       "159             1.127808  \n",
       "160             1.108543  \n",
       "161             1.088390  \n",
       "\n",
       "[162 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input para treinamento\n",
    "train_input = input_data.iloc[:train_split]\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9e1f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       51.972\n",
       "1       40.729\n",
       "2       54.446\n",
       "3       51.788\n",
       "4       58.702\n",
       "        ...   \n",
       "157     65.293\n",
       "158     90.338\n",
       "159     83.188\n",
       "160    107.230\n",
       "161    105.397\n",
       "Name: Mato Grosso - Consumo de Cimento (t), Length: 162, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alvo para treinamento\n",
    "train_target = target_data.iloc[:train_split]\n",
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b094360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mato Grosso - Produção de Cimento (t)</th>\n",
       "      <th>IPCA - Variação mensal durante o Plano Real (%)</th>\n",
       "      <th>NFSP - Porcentagem do PIB (%)</th>\n",
       "      <th>Taxa Selic (%)</th>\n",
       "      <th>IGP-DI</th>\n",
       "      <th>População</th>\n",
       "      <th>Estoque liquido de capital fixo - (R$)</th>\n",
       "      <th>INCC (%)</th>\n",
       "      <th>Mato Grosso - Desemprego</th>\n",
       "      <th>Mato Grosso - IDH</th>\n",
       "      <th>Mato Grosso - PIB - Estadual</th>\n",
       "      <th>Mato Grosso - PIB - Construção Civil</th>\n",
       "      <th>Mato Grosso - PIB - Per Capita</th>\n",
       "      <th>Mato Grosso - PIB - Preços de Mercado</th>\n",
       "      <th>Mato Grosso - value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.327220</td>\n",
       "      <td>-0.601510</td>\n",
       "      <td>-0.653307</td>\n",
       "      <td>0.157879</td>\n",
       "      <td>-1.255326</td>\n",
       "      <td>1.088045</td>\n",
       "      <td>0.763466</td>\n",
       "      <td>-1.213929</td>\n",
       "      <td>-0.539790</td>\n",
       "      <td>1.337998</td>\n",
       "      <td>1.238711</td>\n",
       "      <td>0.206478</td>\n",
       "      <td>1.159923</td>\n",
       "      <td>1.196848</td>\n",
       "      <td>1.067331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.378712</td>\n",
       "      <td>-0.786068</td>\n",
       "      <td>-0.653814</td>\n",
       "      <td>0.548855</td>\n",
       "      <td>-1.118679</td>\n",
       "      <td>1.099377</td>\n",
       "      <td>0.752299</td>\n",
       "      <td>-1.292173</td>\n",
       "      <td>-0.626901</td>\n",
       "      <td>1.315502</td>\n",
       "      <td>1.242908</td>\n",
       "      <td>0.192915</td>\n",
       "      <td>1.159287</td>\n",
       "      <td>1.199042</td>\n",
       "      <td>1.046219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.415005</td>\n",
       "      <td>-0.830387</td>\n",
       "      <td>-0.645656</td>\n",
       "      <td>0.385685</td>\n",
       "      <td>-0.936336</td>\n",
       "      <td>1.110709</td>\n",
       "      <td>0.741131</td>\n",
       "      <td>-1.324219</td>\n",
       "      <td>-0.714012</td>\n",
       "      <td>1.293007</td>\n",
       "      <td>1.247105</td>\n",
       "      <td>0.179352</td>\n",
       "      <td>1.158651</td>\n",
       "      <td>1.201236</td>\n",
       "      <td>1.024290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.428855</td>\n",
       "      <td>-0.801089</td>\n",
       "      <td>-0.634274</td>\n",
       "      <td>0.420965</td>\n",
       "      <td>-0.931790</td>\n",
       "      <td>1.122042</td>\n",
       "      <td>0.729964</td>\n",
       "      <td>-1.344446</td>\n",
       "      <td>-0.801123</td>\n",
       "      <td>1.270512</td>\n",
       "      <td>1.251302</td>\n",
       "      <td>0.165788</td>\n",
       "      <td>1.158014</td>\n",
       "      <td>1.203430</td>\n",
       "      <td>1.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.436119</td>\n",
       "      <td>-0.959917</td>\n",
       "      <td>-0.624035</td>\n",
       "      <td>0.548870</td>\n",
       "      <td>-1.168522</td>\n",
       "      <td>1.133374</td>\n",
       "      <td>0.718796</td>\n",
       "      <td>-1.381638</td>\n",
       "      <td>-0.888234</td>\n",
       "      <td>1.248017</td>\n",
       "      <td>1.255499</td>\n",
       "      <td>0.152225</td>\n",
       "      <td>1.157378</td>\n",
       "      <td>1.205623</td>\n",
       "      <td>1.007780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.457915</td>\n",
       "      <td>-1.022309</td>\n",
       "      <td>-0.619791</td>\n",
       "      <td>0.222499</td>\n",
       "      <td>-1.285611</td>\n",
       "      <td>1.144706</td>\n",
       "      <td>0.707629</td>\n",
       "      <td>-1.411208</td>\n",
       "      <td>-0.975344</td>\n",
       "      <td>1.225521</td>\n",
       "      <td>1.259696</td>\n",
       "      <td>0.138661</td>\n",
       "      <td>1.156742</td>\n",
       "      <td>1.207817</td>\n",
       "      <td>0.999210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.462907</td>\n",
       "      <td>-1.074401</td>\n",
       "      <td>-0.620112</td>\n",
       "      <td>-0.614754</td>\n",
       "      <td>-1.446316</td>\n",
       "      <td>1.156038</td>\n",
       "      <td>0.696461</td>\n",
       "      <td>-1.412953</td>\n",
       "      <td>-1.062455</td>\n",
       "      <td>1.203026</td>\n",
       "      <td>1.263893</td>\n",
       "      <td>0.125098</td>\n",
       "      <td>1.156106</td>\n",
       "      <td>1.210011</td>\n",
       "      <td>0.992238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.467673</td>\n",
       "      <td>-1.119597</td>\n",
       "      <td>-0.615973</td>\n",
       "      <td>-0.478229</td>\n",
       "      <td>-1.357832</td>\n",
       "      <td>1.167287</td>\n",
       "      <td>0.681823</td>\n",
       "      <td>-1.491464</td>\n",
       "      <td>-1.088182</td>\n",
       "      <td>1.176285</td>\n",
       "      <td>1.266539</td>\n",
       "      <td>0.122704</td>\n",
       "      <td>1.159844</td>\n",
       "      <td>1.213717</td>\n",
       "      <td>0.985213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.491605</td>\n",
       "      <td>-1.078648</td>\n",
       "      <td>-0.609777</td>\n",
       "      <td>-0.393157</td>\n",
       "      <td>-1.235682</td>\n",
       "      <td>1.178536</td>\n",
       "      <td>0.667184</td>\n",
       "      <td>-1.573805</td>\n",
       "      <td>-1.113908</td>\n",
       "      <td>1.149545</td>\n",
       "      <td>1.269184</td>\n",
       "      <td>0.120309</td>\n",
       "      <td>1.163582</td>\n",
       "      <td>1.217423</td>\n",
       "      <td>0.978151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.482102</td>\n",
       "      <td>-1.055426</td>\n",
       "      <td>-0.602141</td>\n",
       "      <td>-0.557552</td>\n",
       "      <td>-1.065340</td>\n",
       "      <td>1.189784</td>\n",
       "      <td>0.652545</td>\n",
       "      <td>-1.564950</td>\n",
       "      <td>-1.139635</td>\n",
       "      <td>1.122804</td>\n",
       "      <td>1.271830</td>\n",
       "      <td>0.117915</td>\n",
       "      <td>1.167320</td>\n",
       "      <td>1.221129</td>\n",
       "      <td>0.975477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.498153</td>\n",
       "      <td>-1.101053</td>\n",
       "      <td>-0.593094</td>\n",
       "      <td>-1.033381</td>\n",
       "      <td>-0.826250</td>\n",
       "      <td>1.201033</td>\n",
       "      <td>0.637906</td>\n",
       "      <td>-1.581584</td>\n",
       "      <td>-1.165362</td>\n",
       "      <td>1.096063</td>\n",
       "      <td>1.274475</td>\n",
       "      <td>0.115521</td>\n",
       "      <td>1.171058</td>\n",
       "      <td>1.224836</td>\n",
       "      <td>0.968727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.509778</td>\n",
       "      <td>-1.211370</td>\n",
       "      <td>-0.583407</td>\n",
       "      <td>0.310838</td>\n",
       "      <td>-0.657967</td>\n",
       "      <td>1.212282</td>\n",
       "      <td>0.623268</td>\n",
       "      <td>-1.565976</td>\n",
       "      <td>-1.191088</td>\n",
       "      <td>1.069322</td>\n",
       "      <td>1.277121</td>\n",
       "      <td>0.113127</td>\n",
       "      <td>1.174796</td>\n",
       "      <td>1.228542</td>\n",
       "      <td>0.962088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.526197</td>\n",
       "      <td>-1.157198</td>\n",
       "      <td>-0.572795</td>\n",
       "      <td>-1.062966</td>\n",
       "      <td>-0.512365</td>\n",
       "      <td>1.223531</td>\n",
       "      <td>0.608629</td>\n",
       "      <td>-1.648556</td>\n",
       "      <td>-1.216815</td>\n",
       "      <td>1.042582</td>\n",
       "      <td>1.279767</td>\n",
       "      <td>0.110732</td>\n",
       "      <td>1.178533</td>\n",
       "      <td>1.232248</td>\n",
       "      <td>0.955583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.536514</td>\n",
       "      <td>-1.223444</td>\n",
       "      <td>-0.559889</td>\n",
       "      <td>-1.347961</td>\n",
       "      <td>-0.546480</td>\n",
       "      <td>1.234780</td>\n",
       "      <td>0.593990</td>\n",
       "      <td>-1.650049</td>\n",
       "      <td>-1.242541</td>\n",
       "      <td>1.015841</td>\n",
       "      <td>1.282412</td>\n",
       "      <td>0.108338</td>\n",
       "      <td>1.182271</td>\n",
       "      <td>1.235954</td>\n",
       "      <td>0.947063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.549557</td>\n",
       "      <td>-1.311519</td>\n",
       "      <td>-0.555452</td>\n",
       "      <td>-0.609120</td>\n",
       "      <td>-0.504812</td>\n",
       "      <td>1.246029</td>\n",
       "      <td>0.579351</td>\n",
       "      <td>-1.653957</td>\n",
       "      <td>-1.268268</td>\n",
       "      <td>0.989100</td>\n",
       "      <td>1.285058</td>\n",
       "      <td>0.105944</td>\n",
       "      <td>1.186009</td>\n",
       "      <td>1.239660</td>\n",
       "      <td>0.938634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.562806</td>\n",
       "      <td>-1.362602</td>\n",
       "      <td>-0.546328</td>\n",
       "      <td>-0.215312</td>\n",
       "      <td>-0.515805</td>\n",
       "      <td>1.257277</td>\n",
       "      <td>0.564713</td>\n",
       "      <td>-1.652572</td>\n",
       "      <td>-1.293995</td>\n",
       "      <td>0.962359</td>\n",
       "      <td>1.287703</td>\n",
       "      <td>0.103549</td>\n",
       "      <td>1.189747</td>\n",
       "      <td>1.243366</td>\n",
       "      <td>0.930316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.570351</td>\n",
       "      <td>-1.380125</td>\n",
       "      <td>-0.529973</td>\n",
       "      <td>0.222485</td>\n",
       "      <td>-0.391158</td>\n",
       "      <td>1.268526</td>\n",
       "      <td>0.550074</td>\n",
       "      <td>-1.715349</td>\n",
       "      <td>-1.319721</td>\n",
       "      <td>0.935618</td>\n",
       "      <td>1.290349</td>\n",
       "      <td>0.101155</td>\n",
       "      <td>1.193485</td>\n",
       "      <td>1.247072</td>\n",
       "      <td>0.922043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.558789</td>\n",
       "      <td>-1.219296</td>\n",
       "      <td>-0.518885</td>\n",
       "      <td>-1.266605</td>\n",
       "      <td>-0.230121</td>\n",
       "      <td>1.279775</td>\n",
       "      <td>0.535435</td>\n",
       "      <td>-1.750917</td>\n",
       "      <td>-1.345448</td>\n",
       "      <td>0.908878</td>\n",
       "      <td>1.292994</td>\n",
       "      <td>0.098761</td>\n",
       "      <td>1.197223</td>\n",
       "      <td>1.250778</td>\n",
       "      <td>0.915958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.539565</td>\n",
       "      <td>-1.300284</td>\n",
       "      <td>-0.513088</td>\n",
       "      <td>-1.588002</td>\n",
       "      <td>-0.318758</td>\n",
       "      <td>1.291024</td>\n",
       "      <td>0.520796</td>\n",
       "      <td>-1.718448</td>\n",
       "      <td>-1.371174</td>\n",
       "      <td>0.882137</td>\n",
       "      <td>1.295640</td>\n",
       "      <td>0.096366</td>\n",
       "      <td>1.200961</td>\n",
       "      <td>1.254485</td>\n",
       "      <td>0.910086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.545281</td>\n",
       "      <td>-1.336476</td>\n",
       "      <td>-0.513451</td>\n",
       "      <td>-1.226154</td>\n",
       "      <td>-0.396427</td>\n",
       "      <td>1.301722</td>\n",
       "      <td>0.501996</td>\n",
       "      <td>-1.733426</td>\n",
       "      <td>-1.384153</td>\n",
       "      <td>0.835361</td>\n",
       "      <td>1.291015</td>\n",
       "      <td>0.101844</td>\n",
       "      <td>1.197973</td>\n",
       "      <td>1.250843</td>\n",
       "      <td>0.904451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.567873</td>\n",
       "      <td>-1.415774</td>\n",
       "      <td>-0.516743</td>\n",
       "      <td>-1.358090</td>\n",
       "      <td>-0.140581</td>\n",
       "      <td>1.312420</td>\n",
       "      <td>0.483195</td>\n",
       "      <td>-1.729362</td>\n",
       "      <td>-1.397131</td>\n",
       "      <td>0.788586</td>\n",
       "      <td>1.286389</td>\n",
       "      <td>0.107321</td>\n",
       "      <td>1.194984</td>\n",
       "      <td>1.247202</td>\n",
       "      <td>0.899078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.567029</td>\n",
       "      <td>-1.526021</td>\n",
       "      <td>-0.521796</td>\n",
       "      <td>-1.395002</td>\n",
       "      <td>-0.198662</td>\n",
       "      <td>1.323118</td>\n",
       "      <td>0.464395</td>\n",
       "      <td>-1.748544</td>\n",
       "      <td>-1.410110</td>\n",
       "      <td>0.741810</td>\n",
       "      <td>1.281764</td>\n",
       "      <td>0.112798</td>\n",
       "      <td>1.191996</td>\n",
       "      <td>1.243561</td>\n",
       "      <td>0.893992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.597307</td>\n",
       "      <td>-1.681806</td>\n",
       "      <td>-0.529878</td>\n",
       "      <td>-1.430606</td>\n",
       "      <td>-0.154798</td>\n",
       "      <td>1.333817</td>\n",
       "      <td>0.445594</td>\n",
       "      <td>-1.778060</td>\n",
       "      <td>-1.423089</td>\n",
       "      <td>0.695035</td>\n",
       "      <td>1.277139</td>\n",
       "      <td>0.118275</td>\n",
       "      <td>1.189008</td>\n",
       "      <td>1.239920</td>\n",
       "      <td>0.889221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.639091</td>\n",
       "      <td>-1.735167</td>\n",
       "      <td>-0.540010</td>\n",
       "      <td>-1.511562</td>\n",
       "      <td>-0.142566</td>\n",
       "      <td>1.344515</td>\n",
       "      <td>0.426794</td>\n",
       "      <td>-1.773710</td>\n",
       "      <td>-1.436067</td>\n",
       "      <td>0.648259</td>\n",
       "      <td>1.272514</td>\n",
       "      <td>0.123753</td>\n",
       "      <td>1.186020</td>\n",
       "      <td>1.236279</td>\n",
       "      <td>0.884745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.647552</td>\n",
       "      <td>-1.962315</td>\n",
       "      <td>-0.555482</td>\n",
       "      <td>-1.433584</td>\n",
       "      <td>0.013469</td>\n",
       "      <td>1.355213</td>\n",
       "      <td>0.407993</td>\n",
       "      <td>-1.757007</td>\n",
       "      <td>-1.449046</td>\n",
       "      <td>0.601483</td>\n",
       "      <td>1.267888</td>\n",
       "      <td>0.129230</td>\n",
       "      <td>1.183032</td>\n",
       "      <td>1.232638</td>\n",
       "      <td>0.880589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.683341</td>\n",
       "      <td>-2.010387</td>\n",
       "      <td>-0.572934</td>\n",
       "      <td>-1.281958</td>\n",
       "      <td>0.589021</td>\n",
       "      <td>1.365911</td>\n",
       "      <td>0.389193</td>\n",
       "      <td>-1.749976</td>\n",
       "      <td>-1.462024</td>\n",
       "      <td>0.554708</td>\n",
       "      <td>1.263263</td>\n",
       "      <td>0.134707</td>\n",
       "      <td>1.180043</td>\n",
       "      <td>1.228996</td>\n",
       "      <td>0.893787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.683776</td>\n",
       "      <td>-1.870713</td>\n",
       "      <td>-0.588777</td>\n",
       "      <td>-1.358588</td>\n",
       "      <td>1.043728</td>\n",
       "      <td>1.376610</td>\n",
       "      <td>0.370392</td>\n",
       "      <td>-1.593005</td>\n",
       "      <td>-1.475003</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>1.258638</td>\n",
       "      <td>0.140184</td>\n",
       "      <td>1.177055</td>\n",
       "      <td>1.225355</td>\n",
       "      <td>0.914063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.711014</td>\n",
       "      <td>-1.806230</td>\n",
       "      <td>-0.612606</td>\n",
       "      <td>-1.511565</td>\n",
       "      <td>1.387010</td>\n",
       "      <td>1.387308</td>\n",
       "      <td>0.351592</td>\n",
       "      <td>-1.351489</td>\n",
       "      <td>-1.487981</td>\n",
       "      <td>0.461157</td>\n",
       "      <td>1.254013</td>\n",
       "      <td>0.145662</td>\n",
       "      <td>1.174067</td>\n",
       "      <td>1.221714</td>\n",
       "      <td>0.954337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.715019</td>\n",
       "      <td>-1.727496</td>\n",
       "      <td>-0.640956</td>\n",
       "      <td>-1.421708</td>\n",
       "      <td>1.815728</td>\n",
       "      <td>1.398006</td>\n",
       "      <td>0.332791</td>\n",
       "      <td>-1.198492</td>\n",
       "      <td>-1.500960</td>\n",
       "      <td>0.414381</td>\n",
       "      <td>1.249388</td>\n",
       "      <td>0.151139</td>\n",
       "      <td>1.171079</td>\n",
       "      <td>1.218073</td>\n",
       "      <td>0.995597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.730334</td>\n",
       "      <td>-1.391283</td>\n",
       "      <td>-0.663358</td>\n",
       "      <td>-1.593676</td>\n",
       "      <td>2.181106</td>\n",
       "      <td>1.408704</td>\n",
       "      <td>0.313991</td>\n",
       "      <td>-1.100894</td>\n",
       "      <td>-1.513939</td>\n",
       "      <td>0.367605</td>\n",
       "      <td>1.244762</td>\n",
       "      <td>0.156616</td>\n",
       "      <td>1.168091</td>\n",
       "      <td>1.214432</td>\n",
       "      <td>1.046376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Mato Grosso - Produção de Cimento (t)  \\\n",
       "162                               0.327220   \n",
       "163                               0.378712   \n",
       "164                               0.415005   \n",
       "165                               0.428855   \n",
       "166                               0.436119   \n",
       "167                               0.457915   \n",
       "168                               0.462907   \n",
       "169                               0.467673   \n",
       "170                               0.491605   \n",
       "171                               0.482102   \n",
       "172                               0.498153   \n",
       "173                               0.509778   \n",
       "174                               0.526197   \n",
       "175                               0.536514   \n",
       "176                               0.549557   \n",
       "177                               0.562806   \n",
       "178                               0.570351   \n",
       "179                               0.558789   \n",
       "180                               0.539565   \n",
       "181                               0.545281   \n",
       "182                               0.567873   \n",
       "183                               0.567029   \n",
       "184                               0.597307   \n",
       "185                               0.639091   \n",
       "186                               0.647552   \n",
       "187                               0.683341   \n",
       "188                               0.683776   \n",
       "189                               0.711014   \n",
       "190                               0.715019   \n",
       "191                               0.730334   \n",
       "\n",
       "      IPCA - Variação mensal durante o Plano Real (%)  \\\n",
       "162                                         -0.601510   \n",
       "163                                         -0.786068   \n",
       "164                                         -0.830387   \n",
       "165                                         -0.801089   \n",
       "166                                         -0.959917   \n",
       "167                                         -1.022309   \n",
       "168                                         -1.074401   \n",
       "169                                         -1.119597   \n",
       "170                                         -1.078648   \n",
       "171                                         -1.055426   \n",
       "172                                         -1.101053   \n",
       "173                                         -1.211370   \n",
       "174                                         -1.157198   \n",
       "175                                         -1.223444   \n",
       "176                                         -1.311519   \n",
       "177                                         -1.362602   \n",
       "178                                         -1.380125   \n",
       "179                                         -1.219296   \n",
       "180                                         -1.300284   \n",
       "181                                         -1.336476   \n",
       "182                                         -1.415774   \n",
       "183                                         -1.526021   \n",
       "184                                         -1.681806   \n",
       "185                                         -1.735167   \n",
       "186                                         -1.962315   \n",
       "187                                         -2.010387   \n",
       "188                                         -1.870713   \n",
       "189                                         -1.806230   \n",
       "190                                         -1.727496   \n",
       "191                                         -1.391283   \n",
       "\n",
       "     NFSP - Porcentagem do PIB (%)  Taxa Selic (%)    IGP-DI  População  \\\n",
       "162                      -0.653307        0.157879 -1.255326   1.088045   \n",
       "163                      -0.653814        0.548855 -1.118679   1.099377   \n",
       "164                      -0.645656        0.385685 -0.936336   1.110709   \n",
       "165                      -0.634274        0.420965 -0.931790   1.122042   \n",
       "166                      -0.624035        0.548870 -1.168522   1.133374   \n",
       "167                      -0.619791        0.222499 -1.285611   1.144706   \n",
       "168                      -0.620112       -0.614754 -1.446316   1.156038   \n",
       "169                      -0.615973       -0.478229 -1.357832   1.167287   \n",
       "170                      -0.609777       -0.393157 -1.235682   1.178536   \n",
       "171                      -0.602141       -0.557552 -1.065340   1.189784   \n",
       "172                      -0.593094       -1.033381 -0.826250   1.201033   \n",
       "173                      -0.583407        0.310838 -0.657967   1.212282   \n",
       "174                      -0.572795       -1.062966 -0.512365   1.223531   \n",
       "175                      -0.559889       -1.347961 -0.546480   1.234780   \n",
       "176                      -0.555452       -0.609120 -0.504812   1.246029   \n",
       "177                      -0.546328       -0.215312 -0.515805   1.257277   \n",
       "178                      -0.529973        0.222485 -0.391158   1.268526   \n",
       "179                      -0.518885       -1.266605 -0.230121   1.279775   \n",
       "180                      -0.513088       -1.588002 -0.318758   1.291024   \n",
       "181                      -0.513451       -1.226154 -0.396427   1.301722   \n",
       "182                      -0.516743       -1.358090 -0.140581   1.312420   \n",
       "183                      -0.521796       -1.395002 -0.198662   1.323118   \n",
       "184                      -0.529878       -1.430606 -0.154798   1.333817   \n",
       "185                      -0.540010       -1.511562 -0.142566   1.344515   \n",
       "186                      -0.555482       -1.433584  0.013469   1.355213   \n",
       "187                      -0.572934       -1.281958  0.589021   1.365911   \n",
       "188                      -0.588777       -1.358588  1.043728   1.376610   \n",
       "189                      -0.612606       -1.511565  1.387010   1.387308   \n",
       "190                      -0.640956       -1.421708  1.815728   1.398006   \n",
       "191                      -0.663358       -1.593676  2.181106   1.408704   \n",
       "\n",
       "     Estoque liquido de capital fixo - (R$)   INCC (%)  \\\n",
       "162                                0.763466  -1.213929   \n",
       "163                                0.752299  -1.292173   \n",
       "164                                0.741131  -1.324219   \n",
       "165                                0.729964  -1.344446   \n",
       "166                                0.718796  -1.381638   \n",
       "167                                0.707629  -1.411208   \n",
       "168                                0.696461  -1.412953   \n",
       "169                                0.681823  -1.491464   \n",
       "170                                0.667184  -1.573805   \n",
       "171                                0.652545  -1.564950   \n",
       "172                                0.637906  -1.581584   \n",
       "173                                0.623268  -1.565976   \n",
       "174                                0.608629  -1.648556   \n",
       "175                                0.593990  -1.650049   \n",
       "176                                0.579351  -1.653957   \n",
       "177                                0.564713  -1.652572   \n",
       "178                                0.550074  -1.715349   \n",
       "179                                0.535435  -1.750917   \n",
       "180                                0.520796  -1.718448   \n",
       "181                                0.501996  -1.733426   \n",
       "182                                0.483195  -1.729362   \n",
       "183                                0.464395  -1.748544   \n",
       "184                                0.445594  -1.778060   \n",
       "185                                0.426794  -1.773710   \n",
       "186                                0.407993  -1.757007   \n",
       "187                                0.389193  -1.749976   \n",
       "188                                0.370392  -1.593005   \n",
       "189                                0.351592  -1.351489   \n",
       "190                                0.332791  -1.198492   \n",
       "191                                0.313991  -1.100894   \n",
       "\n",
       "     Mato Grosso - Desemprego  Mato Grosso - IDH  \\\n",
       "162                 -0.539790           1.337998   \n",
       "163                 -0.626901           1.315502   \n",
       "164                 -0.714012           1.293007   \n",
       "165                 -0.801123           1.270512   \n",
       "166                 -0.888234           1.248017   \n",
       "167                 -0.975344           1.225521   \n",
       "168                 -1.062455           1.203026   \n",
       "169                 -1.088182           1.176285   \n",
       "170                 -1.113908           1.149545   \n",
       "171                 -1.139635           1.122804   \n",
       "172                 -1.165362           1.096063   \n",
       "173                 -1.191088           1.069322   \n",
       "174                 -1.216815           1.042582   \n",
       "175                 -1.242541           1.015841   \n",
       "176                 -1.268268           0.989100   \n",
       "177                 -1.293995           0.962359   \n",
       "178                 -1.319721           0.935618   \n",
       "179                 -1.345448           0.908878   \n",
       "180                 -1.371174           0.882137   \n",
       "181                 -1.384153           0.835361   \n",
       "182                 -1.397131           0.788586   \n",
       "183                 -1.410110           0.741810   \n",
       "184                 -1.423089           0.695035   \n",
       "185                 -1.436067           0.648259   \n",
       "186                 -1.449046           0.601483   \n",
       "187                 -1.462024           0.554708   \n",
       "188                 -1.475003           0.507932   \n",
       "189                 -1.487981           0.461157   \n",
       "190                 -1.500960           0.414381   \n",
       "191                 -1.513939           0.367605   \n",
       "\n",
       "     Mato Grosso - PIB - Estadual  Mato Grosso - PIB - Construção Civil  \\\n",
       "162                      1.238711                              0.206478   \n",
       "163                      1.242908                              0.192915   \n",
       "164                      1.247105                              0.179352   \n",
       "165                      1.251302                              0.165788   \n",
       "166                      1.255499                              0.152225   \n",
       "167                      1.259696                              0.138661   \n",
       "168                      1.263893                              0.125098   \n",
       "169                      1.266539                              0.122704   \n",
       "170                      1.269184                              0.120309   \n",
       "171                      1.271830                              0.117915   \n",
       "172                      1.274475                              0.115521   \n",
       "173                      1.277121                              0.113127   \n",
       "174                      1.279767                              0.110732   \n",
       "175                      1.282412                              0.108338   \n",
       "176                      1.285058                              0.105944   \n",
       "177                      1.287703                              0.103549   \n",
       "178                      1.290349                              0.101155   \n",
       "179                      1.292994                              0.098761   \n",
       "180                      1.295640                              0.096366   \n",
       "181                      1.291015                              0.101844   \n",
       "182                      1.286389                              0.107321   \n",
       "183                      1.281764                              0.112798   \n",
       "184                      1.277139                              0.118275   \n",
       "185                      1.272514                              0.123753   \n",
       "186                      1.267888                              0.129230   \n",
       "187                      1.263263                              0.134707   \n",
       "188                      1.258638                              0.140184   \n",
       "189                      1.254013                              0.145662   \n",
       "190                      1.249388                              0.151139   \n",
       "191                      1.244762                              0.156616   \n",
       "\n",
       "     Mato Grosso - PIB - Per Capita  Mato Grosso - PIB - Preços de Mercado  \\\n",
       "162                        1.159923                               1.196848   \n",
       "163                        1.159287                               1.199042   \n",
       "164                        1.158651                               1.201236   \n",
       "165                        1.158014                               1.203430   \n",
       "166                        1.157378                               1.205623   \n",
       "167                        1.156742                               1.207817   \n",
       "168                        1.156106                               1.210011   \n",
       "169                        1.159844                               1.213717   \n",
       "170                        1.163582                               1.217423   \n",
       "171                        1.167320                               1.221129   \n",
       "172                        1.171058                               1.224836   \n",
       "173                        1.174796                               1.228542   \n",
       "174                        1.178533                               1.232248   \n",
       "175                        1.182271                               1.235954   \n",
       "176                        1.186009                               1.239660   \n",
       "177                        1.189747                               1.243366   \n",
       "178                        1.193485                               1.247072   \n",
       "179                        1.197223                               1.250778   \n",
       "180                        1.200961                               1.254485   \n",
       "181                        1.197973                               1.250843   \n",
       "182                        1.194984                               1.247202   \n",
       "183                        1.191996                               1.243561   \n",
       "184                        1.189008                               1.239920   \n",
       "185                        1.186020                               1.236279   \n",
       "186                        1.183032                               1.232638   \n",
       "187                        1.180043                               1.228996   \n",
       "188                        1.177055                               1.225355   \n",
       "189                        1.174067                               1.221714   \n",
       "190                        1.171079                               1.218073   \n",
       "191                        1.168091                               1.214432   \n",
       "\n",
       "     Mato Grosso - value  \n",
       "162             1.067331  \n",
       "163             1.046219  \n",
       "164             1.024290  \n",
       "165             1.016200  \n",
       "166             1.007780  \n",
       "167             0.999210  \n",
       "168             0.992238  \n",
       "169             0.985213  \n",
       "170             0.978151  \n",
       "171             0.975477  \n",
       "172             0.968727  \n",
       "173             0.962088  \n",
       "174             0.955583  \n",
       "175             0.947063  \n",
       "176             0.938634  \n",
       "177             0.930316  \n",
       "178             0.922043  \n",
       "179             0.915958  \n",
       "180             0.910086  \n",
       "181             0.904451  \n",
       "182             0.899078  \n",
       "183             0.893992  \n",
       "184             0.889221  \n",
       "185             0.884745  \n",
       "186             0.880589  \n",
       "187             0.893787  \n",
       "188             0.914063  \n",
       "189             0.954337  \n",
       "190             0.995597  \n",
       "191             1.046376  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input de test (Ano 2021)\n",
    "test_input = input_data.iloc[train_split:]\n",
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5a92e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162    120.055\n",
       "163    125.769\n",
       "164    112.904\n",
       "165    123.201\n",
       "166    112.725\n",
       "167     90.828\n",
       "168     96.360\n",
       "169     86.444\n",
       "170     94.285\n",
       "171     98.986\n",
       "172     88.072\n",
       "173    141.010\n",
       "174    122.652\n",
       "175    142.145\n",
       "176    121.124\n",
       "177    130.503\n",
       "178    104.115\n",
       "179     90.690\n",
       "180    102.685\n",
       "181     96.144\n",
       "182    102.197\n",
       "183    106.712\n",
       "184    124.057\n",
       "185    124.625\n",
       "186    133.116\n",
       "187    144.310\n",
       "188    140.357\n",
       "189    152.769\n",
       "190    124.038\n",
       "191     95.054\n",
       "Name: Mato Grosso - Consumo de Cimento (t), dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alvo de test (Ano 2021)\n",
    "test_target = target_data.iloc[train_split:split_index + 1]\n",
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f831f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_splitter(df, div_factor):\n",
    "    split_factor = len(df)//div_factor\n",
    "    positions_to_drop = []\n",
    "    for i in range(split_factor):\n",
    "        pos = df.shape[0] - (i*div_factor + 1)\n",
    "        positions_to_drop.append(pos)\n",
    "    \n",
    "    return df.drop(positions_to_drop), df.iloc[positions_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3842bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede neural feed-forward com optmizador Estocástico\n",
    "def neural_network_model(train_input, train_target, want_verbose=1, seed=0):\n",
    "    if seed != 0:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "    # Aṕos 500 epochs sem grandes melhoras no val_loss, interrompe.\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                      patience=500, \n",
    "                                                      verbose=want_verbose, \n",
    "                                                      restore_best_weights=True,\n",
    "                                                      start_from_epoch=500)\n",
    "    # Método estocástico e learning rate=0.005\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.005)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(2048, activation='tanh', input_shape=(train_input.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1024, activation='tanh'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1024, activation='tanh'),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    train, train_val = validation_splitter(train_input, 7)\n",
    "    target,target_val = validation_splitter(train_target, 7)\n",
    "#     display(train)\n",
    "#     display(train_val)\n",
    "#     display(target)\n",
    "#     display(target_val)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')   \n",
    "    history = model.fit(train, \n",
    "                        target, \n",
    "                        epochs=10000,\n",
    "                        validation_data=(train_val, target_val),\n",
    "                        callbacks=[early_stopping], \n",
    "                        verbose=want_verbose)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67d5c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_good_seed(train_input, train_target, test_target, test_input):\n",
    "\n",
    "    random_seeds = [random.randint(0, 2**32 - 1) for _ in range(25)]\n",
    "    print(random_seeds)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    winner_seed = None\n",
    "    i = 0\n",
    "    for seed in random_seeds:\n",
    "        print(f\"\\n\\nStep: {i} ___________________________________________\")\n",
    "        i += 1\n",
    "\n",
    "        model, history = neural_network_model(train_input, train_target, want_verbose=0, seed=seed)\n",
    "        current_loss = min(history.history['val_loss'][500:])\n",
    "        print(f\"val_loss: {current_loss}\")\n",
    "\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            winner_seed = seed\n",
    "            print(f\"winner_seed: {winner_seed}\")\n",
    "            if winner_seed == 0.0:\n",
    "                return winner_seed\n",
    "\n",
    "    return winner_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1acb58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1400218057, 5067504, 702131231, 2461182336, 1249198093, 3671784685, 4156832109, 352242301, 478062164, 1370490601, 670041207, 3616518440, 3758750456, 1043582868, 1158710163, 521754126, 1191840380, 1825004737, 4043393830, 2790677958, 2917524552, 3159585003, 881316346, 310717172, 4165869612]\n",
      "\n",
      "\n",
      "Step: 0 ___________________________________________\n",
      "val_loss: 101.48622131347656\n",
      "winner_seed: 1400218057\n",
      "\n",
      "\n",
      "Step: 1 ___________________________________________\n",
      "val_loss: 89.9061508178711\n",
      "winner_seed: 5067504\n",
      "\n",
      "\n",
      "Step: 2 ___________________________________________\n",
      "val_loss: 97.91637420654297\n",
      "\n",
      "\n",
      "Step: 3 ___________________________________________\n",
      "val_loss: 68.97753143310547\n",
      "winner_seed: 2461182336\n",
      "\n",
      "\n",
      "Step: 4 ___________________________________________\n",
      "val_loss: 249.74295043945312\n",
      "\n",
      "\n",
      "Step: 5 ___________________________________________\n",
      "val_loss: 69.87838745117188\n",
      "\n",
      "\n",
      "Step: 6 ___________________________________________\n",
      "val_loss: 278.2451477050781\n",
      "\n",
      "\n",
      "Step: 7 ___________________________________________\n",
      "val_loss: 85.97993469238281\n",
      "\n",
      "\n",
      "Step: 8 ___________________________________________\n",
      "val_loss: 100.9579086303711\n",
      "\n",
      "\n",
      "Step: 9 ___________________________________________\n",
      "val_loss: 189.39552307128906\n",
      "\n",
      "\n",
      "Step: 10 ___________________________________________\n",
      "val_loss: 87.3271484375\n",
      "\n",
      "\n",
      "Step: 11 ___________________________________________\n",
      "val_loss: 97.83458709716797\n",
      "\n",
      "\n",
      "Step: 12 ___________________________________________\n",
      "val_loss: 94.25218963623047\n",
      "\n",
      "\n",
      "Step: 13 ___________________________________________\n",
      "val_loss: 89.66175079345703\n",
      "\n",
      "\n",
      "Step: 14 ___________________________________________\n",
      "val_loss: 77.13597106933594\n",
      "\n",
      "\n",
      "Step: 15 ___________________________________________\n",
      "val_loss: 81.25086212158203\n",
      "\n",
      "\n",
      "Step: 16 ___________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 17:02:44.506971: W tensorflow/core/data/root_dataset.cc:286] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 264.751953125\n",
      "\n",
      "\n",
      "Step: 17 ___________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 17:03:41.213740: W tensorflow/core/data/root_dataset.cc:286] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 90.18218231201172\n",
      "\n",
      "\n",
      "Step: 18 ___________________________________________\n",
      "val_loss: 81.07556915283203\n",
      "\n",
      "\n",
      "Step: 19 ___________________________________________\n",
      "val_loss: 63.881412506103516\n",
      "winner_seed: 2790677958\n",
      "\n",
      "\n",
      "Step: 20 ___________________________________________\n",
      "val_loss: 76.33917236328125\n",
      "\n",
      "\n",
      "Step: 21 ___________________________________________\n",
      "val_loss: 83.08082580566406\n",
      "\n",
      "\n",
      "Step: 22 ___________________________________________\n",
      "val_loss: 85.22663879394531\n",
      "\n",
      "\n",
      "Step: 23 ___________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 17:08:34.260957: W tensorflow/core/data/root_dataset.cc:286] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2023-10-14 17:08:34.314841: W tensorflow/core/data/root_dataset.cc:286] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 89.31342315673828\n",
      "\n",
      "\n",
      "Step: 24 ___________________________________________\n",
      "val_loss: 84.21521759033203\n",
      "\n",
      "\n",
      "final_seed: 2790677958\n"
     ]
    }
   ],
   "source": [
    "winner_seed = get_a_good_seed(train_input, train_target, test_target, test_input)\n",
    "print(f\"\\n\\nfinal_seed: {winner_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903643e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "5/5 [==============================] - 1s 26ms/step - loss: 7362.6143 - val_loss: 14538.1152\n",
      "Epoch 2/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8320.8037 - val_loss: 7749.1543\n",
      "Epoch 3/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6777.6675 - val_loss: 6114.8140\n",
      "Epoch 4/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 5791.9404 - val_loss: 4493.5400\n",
      "Epoch 5/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5228.0483 - val_loss: 3800.3176\n",
      "Epoch 6/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4191.5664 - val_loss: 1470.5496\n",
      "Epoch 7/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1905.9834 - val_loss: 1753.1826\n",
      "Epoch 8/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2477.3142 - val_loss: 371.7469\n",
      "Epoch 9/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 357.7549 - val_loss: 244.3091\n",
      "Epoch 10/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 296.6781 - val_loss: 307.7059\n",
      "Epoch 11/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 305.0265 - val_loss: 204.1266\n",
      "Epoch 12/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 322.5435 - val_loss: 289.3607\n",
      "Epoch 13/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 289.7017 - val_loss: 135.9720\n",
      "Epoch 14/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 228.0853 - val_loss: 168.3945\n",
      "Epoch 15/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 185.9398 - val_loss: 199.8750\n",
      "Epoch 16/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 176.3638 - val_loss: 147.9765\n",
      "Epoch 17/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 208.7354 - val_loss: 158.8649\n",
      "Epoch 18/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 185.6494 - val_loss: 249.0193\n",
      "Epoch 19/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 229.8432 - val_loss: 135.5954\n",
      "Epoch 20/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 233.5971 - val_loss: 269.8079\n",
      "Epoch 21/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 234.3464 - val_loss: 158.6354\n",
      "Epoch 22/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 189.8755 - val_loss: 188.5204\n",
      "Epoch 23/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 170.4411 - val_loss: 203.3566\n",
      "Epoch 24/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 264.6686 - val_loss: 123.6334\n",
      "Epoch 25/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 187.6708 - val_loss: 113.4973\n",
      "Epoch 26/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 204.7000 - val_loss: 151.5011\n",
      "Epoch 27/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 177.3341 - val_loss: 120.0368\n",
      "Epoch 28/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 173.6830 - val_loss: 168.3829\n",
      "Epoch 29/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 198.3935 - val_loss: 143.2054\n",
      "Epoch 30/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 164.1110 - val_loss: 106.2878\n",
      "Epoch 31/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.7871 - val_loss: 112.0508\n",
      "Epoch 32/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 185.5947 - val_loss: 176.0476\n",
      "Epoch 33/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.6476 - val_loss: 145.5697\n",
      "Epoch 34/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 236.5968 - val_loss: 234.7533\n",
      "Epoch 35/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 201.6791 - val_loss: 191.1378\n",
      "Epoch 36/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.0260 - val_loss: 185.1062\n",
      "Epoch 37/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 180.9498 - val_loss: 190.4655\n",
      "Epoch 38/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 169.1418 - val_loss: 128.8391\n",
      "Epoch 39/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 160.2619 - val_loss: 159.8882\n",
      "Epoch 40/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 155.6277 - val_loss: 119.6807\n",
      "Epoch 41/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 196.5314 - val_loss: 138.0896\n",
      "Epoch 42/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.8661 - val_loss: 152.1053\n",
      "Epoch 43/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 154.2338 - val_loss: 168.5565\n",
      "Epoch 44/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 173.4981 - val_loss: 142.6885\n",
      "Epoch 45/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 158.7225 - val_loss: 128.7926\n",
      "Epoch 46/10000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 132.3956 - val_loss: 146.8443\n",
      "Epoch 47/10000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 172.7224 - val_loss: 221.5574\n",
      "Epoch 48/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 198.2319 - val_loss: 144.0455\n",
      "Epoch 49/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 145.4881 - val_loss: 134.7618\n",
      "Epoch 50/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 167.3900 - val_loss: 108.8693\n",
      "Epoch 51/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.8213 - val_loss: 126.7261\n",
      "Epoch 52/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 150.0400 - val_loss: 123.2364\n",
      "Epoch 53/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 210.6598 - val_loss: 138.4763\n",
      "Epoch 54/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 175.2239 - val_loss: 117.9788\n",
      "Epoch 55/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 165.3155 - val_loss: 124.7112\n",
      "Epoch 56/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 193.1109 - val_loss: 162.0150\n",
      "Epoch 57/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 161.3617 - val_loss: 144.8636\n",
      "Epoch 58/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.7408 - val_loss: 115.1428\n",
      "Epoch 59/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 141.5475 - val_loss: 110.5463\n",
      "Epoch 60/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 162.2937 - val_loss: 123.4908\n",
      "Epoch 61/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.5613 - val_loss: 120.7357\n",
      "Epoch 62/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.4615 - val_loss: 221.7627\n",
      "Epoch 63/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 179.0487 - val_loss: 140.5491\n",
      "Epoch 64/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 142.2859 - val_loss: 126.7565\n",
      "Epoch 65/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.1141 - val_loss: 113.0073\n",
      "Epoch 66/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 233.7138 - val_loss: 144.9905\n",
      "Epoch 67/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 169.1526 - val_loss: 107.5842\n",
      "Epoch 68/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 142.6653 - val_loss: 138.3155\n",
      "Epoch 69/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 187.3623 - val_loss: 175.1911\n",
      "Epoch 70/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 203.1850 - val_loss: 142.6396\n",
      "Epoch 71/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 164.0719 - val_loss: 147.7245\n",
      "Epoch 72/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 197.2133 - val_loss: 175.1137\n",
      "Epoch 73/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.5241 - val_loss: 138.9127\n",
      "Epoch 74/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 155.4332 - val_loss: 129.2872\n",
      "Epoch 75/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 155.4120 - val_loss: 158.7088\n",
      "Epoch 76/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 174.7520 - val_loss: 119.6498\n",
      "Epoch 77/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 145.2810 - val_loss: 104.1848\n",
      "Epoch 78/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 153.0374 - val_loss: 123.7586\n",
      "Epoch 79/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 152.3326 - val_loss: 136.0126\n",
      "Epoch 80/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.9256 - val_loss: 98.7045\n",
      "Epoch 81/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 172.4497 - val_loss: 127.5226\n",
      "Epoch 82/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 143.4392 - val_loss: 131.5808\n",
      "Epoch 83/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.3283 - val_loss: 126.9979\n",
      "Epoch 84/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.8052 - val_loss: 122.7912\n",
      "Epoch 85/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.6247 - val_loss: 184.8777\n",
      "Epoch 86/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 250.0753 - val_loss: 136.6172\n",
      "Epoch 87/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 166.8985 - val_loss: 125.8850\n",
      "Epoch 88/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 142.6296 - val_loss: 154.2964\n",
      "Epoch 89/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 203.0190 - val_loss: 150.1810\n",
      "Epoch 90/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 165.4656 - val_loss: 119.0875\n",
      "Epoch 91/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 675.3005 - val_loss: 1228.8693\n",
      "Epoch 92/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 537.3038 - val_loss: 164.7832\n",
      "Epoch 93/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 167.2159 - val_loss: 131.0665\n",
      "Epoch 94/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.8624 - val_loss: 178.9355\n",
      "Epoch 95/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 173.2067 - val_loss: 155.3094\n",
      "Epoch 96/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 154.6501 - val_loss: 126.5201\n",
      "Epoch 97/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.2096 - val_loss: 154.6371\n",
      "Epoch 98/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 178.9154 - val_loss: 169.2451\n",
      "Epoch 99/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 186.6187 - val_loss: 126.8872\n",
      "Epoch 100/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.7394 - val_loss: 156.8215\n",
      "Epoch 101/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 208.9890 - val_loss: 127.5326\n",
      "Epoch 102/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.3761 - val_loss: 182.1545\n",
      "Epoch 103/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 169.1326 - val_loss: 147.7571\n",
      "Epoch 104/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.9451 - val_loss: 110.1406\n",
      "Epoch 105/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.5793 - val_loss: 123.2827\n",
      "Epoch 106/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.0216 - val_loss: 136.3295\n",
      "Epoch 107/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 159.8035 - val_loss: 141.1344\n",
      "Epoch 108/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 158.9855 - val_loss: 88.7177\n",
      "Epoch 109/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 160.9659 - val_loss: 164.0260\n",
      "Epoch 110/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 130.5672 - val_loss: 147.9666\n",
      "Epoch 111/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.6302 - val_loss: 156.3320\n",
      "Epoch 112/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 124.9505 - val_loss: 142.8328\n",
      "Epoch 113/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 145.8999 - val_loss: 133.4851\n",
      "Epoch 114/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.4068 - val_loss: 169.0733\n",
      "Epoch 115/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 137.5262 - val_loss: 150.6613\n",
      "Epoch 116/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.1088 - val_loss: 156.6757\n",
      "Epoch 117/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 112.0134 - val_loss: 140.6065\n",
      "Epoch 118/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.1414 - val_loss: 175.1115\n",
      "Epoch 119/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 100.9528 - val_loss: 161.2731\n",
      "Epoch 120/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 129.9246 - val_loss: 114.4141\n",
      "Epoch 121/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 121.9900 - val_loss: 198.1341\n",
      "Epoch 122/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.4927 - val_loss: 93.7541\n",
      "Epoch 123/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.4824 - val_loss: 183.9327\n",
      "Epoch 124/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.8398 - val_loss: 214.0065\n",
      "Epoch 125/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 122.6902 - val_loss: 116.1830\n",
      "Epoch 126/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.4407 - val_loss: 240.1180\n",
      "Epoch 127/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 151.0550 - val_loss: 136.7421\n",
      "Epoch 128/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.1992 - val_loss: 190.5200\n",
      "Epoch 129/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.9174 - val_loss: 109.8355\n",
      "Epoch 130/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 206.8837 - val_loss: 105.8612\n",
      "Epoch 131/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.2452 - val_loss: 127.8476\n",
      "Epoch 132/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.0555 - val_loss: 122.2906\n",
      "Epoch 133/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 113.8708 - val_loss: 139.5034\n",
      "Epoch 134/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.8498 - val_loss: 158.1333\n",
      "Epoch 135/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 143.5433 - val_loss: 195.4565\n",
      "Epoch 136/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 179.0452 - val_loss: 178.3521\n",
      "Epoch 137/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 173.8816 - val_loss: 185.3322\n",
      "Epoch 138/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 184.3167 - val_loss: 166.3903\n",
      "Epoch 139/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 248.2116 - val_loss: 128.4874\n",
      "Epoch 140/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 179.0696 - val_loss: 121.7291\n",
      "Epoch 141/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 165.7728 - val_loss: 118.5509\n",
      "Epoch 142/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 142.6159 - val_loss: 162.0429\n",
      "Epoch 143/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 145.6615 - val_loss: 139.2366\n",
      "Epoch 144/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 137.4305 - val_loss: 144.9878\n",
      "Epoch 145/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.5263 - val_loss: 119.8686\n",
      "Epoch 146/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 139.0023 - val_loss: 158.5543\n",
      "Epoch 147/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 173.7597 - val_loss: 147.6342\n",
      "Epoch 148/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.3500 - val_loss: 152.6357\n",
      "Epoch 149/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 147.7397 - val_loss: 171.1647\n",
      "Epoch 150/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 138.4050 - val_loss: 131.2116\n",
      "Epoch 151/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.7240 - val_loss: 126.7936\n",
      "Epoch 152/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 154.6652 - val_loss: 106.2208\n",
      "Epoch 153/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.4129 - val_loss: 122.6200\n",
      "Epoch 154/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 155.6290 - val_loss: 103.0784\n",
      "Epoch 155/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 143.8060 - val_loss: 189.1681\n",
      "Epoch 156/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 152.2815 - val_loss: 107.6379\n",
      "Epoch 157/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 133.6372 - val_loss: 109.6189\n",
      "Epoch 158/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.8777 - val_loss: 117.7261\n",
      "Epoch 159/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.4240 - val_loss: 102.2123\n",
      "Epoch 160/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 144.7990 - val_loss: 107.2571\n",
      "Epoch 161/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 164.2989 - val_loss: 175.0839\n",
      "Epoch 162/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 166.4443 - val_loss: 205.9170\n",
      "Epoch 163/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 236.9656 - val_loss: 105.8753\n",
      "Epoch 164/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 168.9181 - val_loss: 171.9391\n",
      "Epoch 165/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 179.7061 - val_loss: 158.7379\n",
      "Epoch 166/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 149.8112 - val_loss: 230.9358\n",
      "Epoch 167/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 215.1424 - val_loss: 162.0027\n",
      "Epoch 168/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 301.1174 - val_loss: 188.2579\n",
      "Epoch 169/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 235.8150 - val_loss: 188.9805\n",
      "Epoch 170/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 211.2043 - val_loss: 196.7885\n",
      "Epoch 171/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 197.4052 - val_loss: 157.5101\n",
      "Epoch 172/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 190.1120 - val_loss: 150.6243\n",
      "Epoch 173/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 168.3455 - val_loss: 142.1037\n",
      "Epoch 174/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 170.5689 - val_loss: 130.5708\n",
      "Epoch 175/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 201.2780 - val_loss: 124.9675\n",
      "Epoch 176/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 171.0094 - val_loss: 117.4595\n",
      "Epoch 177/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 170.1494 - val_loss: 139.4953\n",
      "Epoch 178/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.5267 - val_loss: 148.3389\n",
      "Epoch 179/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 198.4552 - val_loss: 150.8018\n",
      "Epoch 180/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.3744 - val_loss: 156.4202\n",
      "Epoch 181/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.3078 - val_loss: 158.7101\n",
      "Epoch 182/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 135.8173 - val_loss: 144.2081\n",
      "Epoch 183/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2320.6758 - val_loss: 866.9114\n",
      "Epoch 184/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 488.5642 - val_loss: 150.7724\n",
      "Epoch 185/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 205.1529 - val_loss: 198.5200\n",
      "Epoch 186/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 188.5146 - val_loss: 149.3313\n",
      "Epoch 187/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 176.0366 - val_loss: 159.3264\n",
      "Epoch 188/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 137.7372 - val_loss: 167.6718\n",
      "Epoch 189/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.8275 - val_loss: 196.6223\n",
      "Epoch 190/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.7716 - val_loss: 169.4517\n",
      "Epoch 191/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.7691 - val_loss: 153.1012\n",
      "Epoch 192/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.6043 - val_loss: 211.3680\n",
      "Epoch 193/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 174.2251 - val_loss: 328.4248\n",
      "Epoch 194/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 259.9232 - val_loss: 174.7740\n",
      "Epoch 195/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 200.9238 - val_loss: 138.0912\n",
      "Epoch 196/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 189.0809 - val_loss: 134.7505\n",
      "Epoch 197/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 161.0415 - val_loss: 128.8494\n",
      "Epoch 198/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 152.0010 - val_loss: 130.7679\n",
      "Epoch 199/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 263.1631 - val_loss: 141.6222\n",
      "Epoch 200/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 195.2236 - val_loss: 147.5251\n",
      "Epoch 201/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 190.4249 - val_loss: 146.2203\n",
      "Epoch 202/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 198.0869 - val_loss: 206.6596\n",
      "Epoch 203/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 235.5147 - val_loss: 160.6578\n",
      "Epoch 204/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 164.7094 - val_loss: 137.8367\n",
      "Epoch 205/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 146.6319 - val_loss: 140.3169\n",
      "Epoch 206/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 144.5529 - val_loss: 137.8173\n",
      "Epoch 207/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 173.3323 - val_loss: 155.8497\n",
      "Epoch 208/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 184.2244 - val_loss: 144.4920\n",
      "Epoch 209/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 196.7565 - val_loss: 165.3719\n",
      "Epoch 210/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 175.6952 - val_loss: 161.2353\n",
      "Epoch 211/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 180.7515 - val_loss: 172.7739\n",
      "Epoch 212/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 184.9041 - val_loss: 270.3713\n",
      "Epoch 213/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 230.5439 - val_loss: 186.9973\n",
      "Epoch 214/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 207.0059 - val_loss: 150.2047\n",
      "Epoch 215/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 189.2003 - val_loss: 159.3374\n",
      "Epoch 216/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 194.0215 - val_loss: 144.8434\n",
      "Epoch 217/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 174.2899 - val_loss: 152.8799\n",
      "Epoch 218/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 192.0953 - val_loss: 160.2696\n",
      "Epoch 219/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 199.0580 - val_loss: 157.4580\n",
      "Epoch 220/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 213.5515 - val_loss: 167.9305\n",
      "Epoch 221/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 213.4532 - val_loss: 150.5796\n",
      "Epoch 222/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 161.7702 - val_loss: 149.3703\n",
      "Epoch 223/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 166.3082 - val_loss: 162.7855\n",
      "Epoch 224/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 172.0916 - val_loss: 124.2184\n",
      "Epoch 225/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 155.4675 - val_loss: 147.6702\n",
      "Epoch 226/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 154.2860 - val_loss: 145.1043\n",
      "Epoch 227/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 167.8702 - val_loss: 117.9200\n",
      "Epoch 228/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 170.2891 - val_loss: 117.9861\n",
      "Epoch 229/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 151.7460 - val_loss: 108.0442\n",
      "Epoch 230/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 165.1084 - val_loss: 120.2770\n",
      "Epoch 231/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 179.4520 - val_loss: 141.7369\n",
      "Epoch 232/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 172.8427 - val_loss: 116.0886\n",
      "Epoch 233/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 163.7510 - val_loss: 150.1519\n",
      "Epoch 234/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 225.4344 - val_loss: 138.0745\n",
      "Epoch 235/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 189.5333 - val_loss: 187.5339\n",
      "Epoch 236/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 209.5379 - val_loss: 144.6026\n",
      "Epoch 237/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 200.6584 - val_loss: 130.0162\n",
      "Epoch 238/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 186.4392 - val_loss: 148.5745\n",
      "Epoch 239/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 224.3144 - val_loss: 237.3400\n",
      "Epoch 240/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 218.7155 - val_loss: 172.5709\n",
      "Epoch 241/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 217.3755 - val_loss: 190.3451\n",
      "Epoch 242/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 185.5774 - val_loss: 143.6536\n",
      "Epoch 243/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 180.8171 - val_loss: 135.4354\n",
      "Epoch 244/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 180.6282 - val_loss: 134.5282\n",
      "Epoch 245/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 195.3542 - val_loss: 166.7733\n",
      "Epoch 246/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 187.9038 - val_loss: 160.2830\n",
      "Epoch 247/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 200.3104 - val_loss: 134.3660\n",
      "Epoch 248/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 207.2417 - val_loss: 141.2317\n",
      "Epoch 249/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 184.5510 - val_loss: 143.4025\n",
      "Epoch 250/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 182.0789 - val_loss: 132.4359\n",
      "Epoch 251/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 189.9647 - val_loss: 125.9290\n",
      "Epoch 252/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 186.5086 - val_loss: 125.5288\n",
      "Epoch 253/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 185.2404 - val_loss: 117.7540\n",
      "Epoch 254/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 179.7563 - val_loss: 160.5392\n",
      "Epoch 255/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.5864 - val_loss: 127.5197\n",
      "Epoch 256/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 180.1867 - val_loss: 128.6445\n",
      "Epoch 257/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 155.5218 - val_loss: 133.6421\n",
      "Epoch 258/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 141.2393 - val_loss: 100.1140\n",
      "Epoch 259/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 305.3120 - val_loss: 270.8822\n",
      "Epoch 260/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 172.6301 - val_loss: 145.9205\n",
      "Epoch 261/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.1745 - val_loss: 149.9550\n",
      "Epoch 262/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.5989 - val_loss: 190.2144\n",
      "Epoch 263/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.0998 - val_loss: 105.0281\n",
      "Epoch 264/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.6680 - val_loss: 121.9946\n",
      "Epoch 265/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.8860 - val_loss: 118.6246\n",
      "Epoch 266/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.7072 - val_loss: 137.4203\n",
      "Epoch 267/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 122.7285 - val_loss: 171.2206\n",
      "Epoch 268/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 113.5107 - val_loss: 177.9891\n",
      "Epoch 269/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 113.7578 - val_loss: 186.3655\n",
      "Epoch 270/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.1050 - val_loss: 191.7501\n",
      "Epoch 271/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.2291 - val_loss: 215.0496\n",
      "Epoch 272/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.0677 - val_loss: 229.7723\n",
      "Epoch 273/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.8735 - val_loss: 110.1726\n",
      "Epoch 274/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.7260 - val_loss: 178.0074\n",
      "Epoch 275/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 139.2754 - val_loss: 116.5143\n",
      "Epoch 276/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.7712 - val_loss: 232.1873\n",
      "Epoch 277/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 204.4728 - val_loss: 165.5341\n",
      "Epoch 278/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.7696 - val_loss: 220.8376\n",
      "Epoch 279/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 163.8245 - val_loss: 148.7060\n",
      "Epoch 280/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 178.6137 - val_loss: 146.6213\n",
      "Epoch 281/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.9328 - val_loss: 203.9180\n",
      "Epoch 282/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 351.2409 - val_loss: 121.2465\n",
      "Epoch 283/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 250.2974 - val_loss: 122.3188\n",
      "Epoch 284/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1069.6309 - val_loss: 335.0229\n",
      "Epoch 285/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 366.7393 - val_loss: 200.1618\n",
      "Epoch 286/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 285.7188 - val_loss: 120.4089\n",
      "Epoch 287/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 237.3613 - val_loss: 229.5539\n",
      "Epoch 288/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 242.8300 - val_loss: 176.3528\n",
      "Epoch 289/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 153.4230 - val_loss: 153.3669\n",
      "Epoch 290/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.9545 - val_loss: 154.4162\n",
      "Epoch 291/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.4581 - val_loss: 145.7399\n",
      "Epoch 292/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.6939 - val_loss: 148.4547\n",
      "Epoch 293/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 119.7049 - val_loss: 121.7237\n",
      "Epoch 294/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.1506 - val_loss: 231.2427\n",
      "Epoch 295/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 264.4926 - val_loss: 154.1357\n",
      "Epoch 296/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 170.7651 - val_loss: 170.4235\n",
      "Epoch 297/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 160.2037 - val_loss: 150.4005\n",
      "Epoch 298/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 134.1999 - val_loss: 134.1833\n",
      "Epoch 299/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 147.8514 - val_loss: 137.5146\n",
      "Epoch 300/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.8850 - val_loss: 139.9637\n",
      "Epoch 301/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.5984 - val_loss: 152.7941\n",
      "Epoch 302/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.2481 - val_loss: 145.4097\n",
      "Epoch 303/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.2768 - val_loss: 134.9658\n",
      "Epoch 304/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 168.3691 - val_loss: 126.7184\n",
      "Epoch 305/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 206.9418 - val_loss: 127.2459\n",
      "Epoch 306/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 175.7511 - val_loss: 95.8182\n",
      "Epoch 307/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 154.8513 - val_loss: 131.3363\n",
      "Epoch 308/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 146.8458 - val_loss: 116.9094\n",
      "Epoch 309/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.6244 - val_loss: 125.8644\n",
      "Epoch 310/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.5060 - val_loss: 129.2492\n",
      "Epoch 311/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 173.6163 - val_loss: 139.3152\n",
      "Epoch 312/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.7493 - val_loss: 181.9312\n",
      "Epoch 313/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 161.4910 - val_loss: 205.9286\n",
      "Epoch 314/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.2291 - val_loss: 170.8841\n",
      "Epoch 315/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 137.6402 - val_loss: 117.3898\n",
      "Epoch 316/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 136.7343 - val_loss: 187.1544\n",
      "Epoch 317/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.5453 - val_loss: 128.1528\n",
      "Epoch 318/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.7673 - val_loss: 125.8750\n",
      "Epoch 319/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.2880 - val_loss: 178.6842\n",
      "Epoch 320/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.3224 - val_loss: 110.2299\n",
      "Epoch 321/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.1650 - val_loss: 136.7408\n",
      "Epoch 322/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 126.6089 - val_loss: 126.5348\n",
      "Epoch 323/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 155.8631 - val_loss: 122.2122\n",
      "Epoch 324/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.7944 - val_loss: 127.3009\n",
      "Epoch 325/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 127.4186 - val_loss: 159.5619\n",
      "Epoch 326/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.3603 - val_loss: 140.5542\n",
      "Epoch 327/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 156.2220 - val_loss: 140.1318\n",
      "Epoch 328/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.1860 - val_loss: 152.5334\n",
      "Epoch 329/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 117.9519 - val_loss: 162.6742\n",
      "Epoch 330/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 202.1583 - val_loss: 391.7807\n",
      "Epoch 331/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 636.1822 - val_loss: 425.0575\n",
      "Epoch 332/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 312.2630 - val_loss: 153.9516\n",
      "Epoch 333/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 208.0109 - val_loss: 145.8596\n",
      "Epoch 334/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 162.9634 - val_loss: 180.2731\n",
      "Epoch 335/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.4316 - val_loss: 214.2945\n",
      "Epoch 336/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.0613 - val_loss: 198.9916\n",
      "Epoch 337/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 109.0743 - val_loss: 169.2374\n",
      "Epoch 338/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.4946 - val_loss: 186.6378\n",
      "Epoch 339/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 116.0873 - val_loss: 159.4967\n",
      "Epoch 340/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.8012 - val_loss: 182.5299\n",
      "Epoch 341/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.9346 - val_loss: 157.1708\n",
      "Epoch 342/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 140.3185 - val_loss: 296.5022\n",
      "Epoch 343/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 158.7318 - val_loss: 168.7138\n",
      "Epoch 344/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 130.3433 - val_loss: 272.9027\n",
      "Epoch 345/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.2725 - val_loss: 161.7351\n",
      "Epoch 346/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 162.5486 - val_loss: 114.7051\n",
      "Epoch 347/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.3983 - val_loss: 120.3695\n",
      "Epoch 348/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.2980 - val_loss: 185.2376\n",
      "Epoch 349/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 147.2264 - val_loss: 161.8924\n",
      "Epoch 350/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 169.3911 - val_loss: 157.6723\n",
      "Epoch 351/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 187.3364 - val_loss: 201.6922\n",
      "Epoch 352/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 161.2070 - val_loss: 134.7458\n",
      "Epoch 353/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.7998 - val_loss: 155.8195\n",
      "Epoch 354/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 136.4583 - val_loss: 134.4129\n",
      "Epoch 355/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.4693 - val_loss: 168.5111\n",
      "Epoch 356/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.6397 - val_loss: 161.6349\n",
      "Epoch 357/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.1772 - val_loss: 206.2108\n",
      "Epoch 358/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 167.4394 - val_loss: 170.0234\n",
      "Epoch 359/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.2337 - val_loss: 229.9284\n",
      "Epoch 360/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.0688 - val_loss: 195.7428\n",
      "Epoch 361/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.4037 - val_loss: 191.7483\n",
      "Epoch 362/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 122.5597 - val_loss: 184.3130\n",
      "Epoch 363/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.7632 - val_loss: 235.9250\n",
      "Epoch 364/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.5950 - val_loss: 197.8431\n",
      "Epoch 365/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.6134 - val_loss: 215.7130\n",
      "Epoch 366/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.1572 - val_loss: 108.3363\n",
      "Epoch 367/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.3572 - val_loss: 227.4700\n",
      "Epoch 368/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 120.9748 - val_loss: 178.5191\n",
      "Epoch 369/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 103.1442 - val_loss: 132.4090\n",
      "Epoch 370/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.6330 - val_loss: 138.8505\n",
      "Epoch 371/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.2128 - val_loss: 193.4615\n",
      "Epoch 372/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.0825 - val_loss: 186.1400\n",
      "Epoch 373/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.6284 - val_loss: 238.4825\n",
      "Epoch 374/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.8650 - val_loss: 248.3879\n",
      "Epoch 375/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.0038 - val_loss: 202.2402\n",
      "Epoch 376/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.1075 - val_loss: 201.1570\n",
      "Epoch 377/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 104.9257 - val_loss: 184.1692\n",
      "Epoch 378/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 122.4845 - val_loss: 201.4378\n",
      "Epoch 379/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.3774 - val_loss: 218.3629\n",
      "Epoch 380/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.0677 - val_loss: 149.9998\n",
      "Epoch 381/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.4328 - val_loss: 249.2776\n",
      "Epoch 382/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 113.6427 - val_loss: 197.3521\n",
      "Epoch 383/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.3143 - val_loss: 95.0341\n",
      "Epoch 384/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 154.7058 - val_loss: 814.4199\n",
      "Epoch 385/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 709.0992 - val_loss: 138.3000\n",
      "Epoch 386/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 165.0229 - val_loss: 224.9145\n",
      "Epoch 387/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 181.2372 - val_loss: 134.8411\n",
      "Epoch 388/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 139.0502 - val_loss: 127.8977\n",
      "Epoch 389/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.8229 - val_loss: 191.9234\n",
      "Epoch 390/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 119.1410 - val_loss: 160.7093\n",
      "Epoch 391/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 175.1006 - val_loss: 111.9857\n",
      "Epoch 392/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 130.9339 - val_loss: 243.3156\n",
      "Epoch 393/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.1069 - val_loss: 180.7159\n",
      "Epoch 394/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 141.6260 - val_loss: 131.0028\n",
      "Epoch 395/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.7251 - val_loss: 175.8356\n",
      "Epoch 396/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.8776 - val_loss: 172.5795\n",
      "Epoch 397/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.7499 - val_loss: 162.1223\n",
      "Epoch 398/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.9157 - val_loss: 244.7162\n",
      "Epoch 399/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 165.3457 - val_loss: 182.4753\n",
      "Epoch 400/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.2110 - val_loss: 178.2797\n",
      "Epoch 401/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 194.2415 - val_loss: 186.3535\n",
      "Epoch 402/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 170.5954 - val_loss: 155.0527\n",
      "Epoch 403/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.4063 - val_loss: 144.9970\n",
      "Epoch 404/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 142.9250 - val_loss: 169.8743\n",
      "Epoch 405/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.2833 - val_loss: 185.2751\n",
      "Epoch 406/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 133.9719 - val_loss: 161.1655\n",
      "Epoch 407/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 120.2206 - val_loss: 170.3383\n",
      "Epoch 408/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.2364 - val_loss: 158.6347\n",
      "Epoch 409/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.9989 - val_loss: 127.0836\n",
      "Epoch 410/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.2078 - val_loss: 183.8090\n",
      "Epoch 411/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.6999 - val_loss: 149.0660\n",
      "Epoch 412/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 109.4259 - val_loss: 157.2388\n",
      "Epoch 413/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 106.9606 - val_loss: 188.8583\n",
      "Epoch 414/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.2829 - val_loss: 168.9662\n",
      "Epoch 415/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.8290 - val_loss: 156.4122\n",
      "Epoch 416/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 116.6235 - val_loss: 209.7589\n",
      "Epoch 417/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 185.0549 - val_loss: 181.7813\n",
      "Epoch 418/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 155.5135 - val_loss: 179.0196\n",
      "Epoch 419/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 144.6098 - val_loss: 147.9492\n",
      "Epoch 420/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.1609 - val_loss: 133.6300\n",
      "Epoch 421/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.9856 - val_loss: 145.4480\n",
      "Epoch 422/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 152.1555 - val_loss: 104.4908\n",
      "Epoch 423/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.5979 - val_loss: 140.8379\n",
      "Epoch 424/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.0287 - val_loss: 132.8700\n",
      "Epoch 425/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.6549 - val_loss: 123.9286\n",
      "Epoch 426/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 133.0021 - val_loss: 110.8027\n",
      "Epoch 427/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 139.8050 - val_loss: 129.8077\n",
      "Epoch 428/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.1304 - val_loss: 151.8591\n",
      "Epoch 429/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 157.5615 - val_loss: 126.4603\n",
      "Epoch 430/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.2082 - val_loss: 135.5719\n",
      "Epoch 431/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 143.0709 - val_loss: 177.6226\n",
      "Epoch 432/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.3413 - val_loss: 131.2325\n",
      "Epoch 433/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.3819 - val_loss: 145.5609\n",
      "Epoch 434/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.0236 - val_loss: 159.6251\n",
      "Epoch 435/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.4977 - val_loss: 138.5996\n",
      "Epoch 436/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.7090 - val_loss: 182.4775\n",
      "Epoch 437/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.3835 - val_loss: 153.3578\n",
      "Epoch 438/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.1465 - val_loss: 223.3549\n",
      "Epoch 439/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.0435 - val_loss: 168.5148\n",
      "Epoch 440/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.2060 - val_loss: 201.1560\n",
      "Epoch 441/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 172.9856 - val_loss: 116.9848\n",
      "Epoch 442/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 137.6562 - val_loss: 128.0516\n",
      "Epoch 443/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 135.0281 - val_loss: 155.1237\n",
      "Epoch 444/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.8291 - val_loss: 208.8134\n",
      "Epoch 445/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.9793 - val_loss: 178.5348\n",
      "Epoch 446/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.9072 - val_loss: 209.2063\n",
      "Epoch 447/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.0970 - val_loss: 132.8748\n",
      "Epoch 448/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 130.2082 - val_loss: 183.7314\n",
      "Epoch 449/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 113.2476 - val_loss: 256.6056\n",
      "Epoch 450/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 140.6188 - val_loss: 155.3999\n",
      "Epoch 451/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.6702 - val_loss: 168.6858\n",
      "Epoch 452/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.0083 - val_loss: 148.8776\n",
      "Epoch 453/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.9710 - val_loss: 198.8637\n",
      "Epoch 454/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.8147 - val_loss: 186.9415\n",
      "Epoch 455/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.6343 - val_loss: 205.7249\n",
      "Epoch 456/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.8598 - val_loss: 201.6609\n",
      "Epoch 457/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.1990 - val_loss: 184.8017\n",
      "Epoch 458/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.6876 - val_loss: 172.9594\n",
      "Epoch 459/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 116.8038 - val_loss: 213.4056\n",
      "Epoch 460/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.8813 - val_loss: 220.2379\n",
      "Epoch 461/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.2802 - val_loss: 217.2821\n",
      "Epoch 462/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.8032 - val_loss: 211.4374\n",
      "Epoch 463/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.4131 - val_loss: 207.4986\n",
      "Epoch 464/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.6182 - val_loss: 202.1724\n",
      "Epoch 465/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.9257 - val_loss: 175.4581\n",
      "Epoch 466/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.3775 - val_loss: 136.5737\n",
      "Epoch 467/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.8278 - val_loss: 235.2720\n",
      "Epoch 468/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.0464 - val_loss: 225.0485\n",
      "Epoch 469/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 101.0238 - val_loss: 160.7332\n",
      "Epoch 470/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 107.3609 - val_loss: 221.8173\n",
      "Epoch 471/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 100.5199 - val_loss: 225.1681\n",
      "Epoch 472/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 96.4825 - val_loss: 222.2679\n",
      "Epoch 473/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.1369 - val_loss: 265.0319\n",
      "Epoch 474/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 175.0187 - val_loss: 122.3790\n",
      "Epoch 475/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 162.9146 - val_loss: 198.2104\n",
      "Epoch 476/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.9220 - val_loss: 204.8439\n",
      "Epoch 477/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 112.2819 - val_loss: 160.0705\n",
      "Epoch 478/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.9649 - val_loss: 154.2434\n",
      "Epoch 479/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 127.0046 - val_loss: 160.4927\n",
      "Epoch 480/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.0286 - val_loss: 143.3340\n",
      "Epoch 481/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 154.1834 - val_loss: 140.9194\n",
      "Epoch 482/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 158.7589 - val_loss: 150.3408\n",
      "Epoch 483/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 174.5104 - val_loss: 114.8163\n",
      "Epoch 484/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 164.9756 - val_loss: 136.6577\n",
      "Epoch 485/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.3342 - val_loss: 119.8614\n",
      "Epoch 486/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 133.6323 - val_loss: 126.2599\n",
      "Epoch 487/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.4745 - val_loss: 139.0986\n",
      "Epoch 488/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.5382 - val_loss: 138.2979\n",
      "Epoch 489/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.6752 - val_loss: 129.9276\n",
      "Epoch 490/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.8423 - val_loss: 150.3258\n",
      "Epoch 491/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 155.5539 - val_loss: 139.1959\n",
      "Epoch 492/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 161.7018 - val_loss: 126.1140\n",
      "Epoch 493/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 147.5414 - val_loss: 155.8876\n",
      "Epoch 494/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 153.2055 - val_loss: 134.6320\n",
      "Epoch 495/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 162.8351 - val_loss: 151.3992\n",
      "Epoch 496/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 172.6171 - val_loss: 137.4469\n",
      "Epoch 497/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 144.6202 - val_loss: 173.3106\n",
      "Epoch 498/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.3646 - val_loss: 138.9408\n",
      "Epoch 499/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 116.2243 - val_loss: 160.2810\n",
      "Epoch 500/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.7983 - val_loss: 146.1905\n",
      "Epoch 501/10000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 108.7985 - val_loss: 153.6136\n",
      "Epoch 502/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.4452 - val_loss: 172.8962\n",
      "Epoch 503/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.5576 - val_loss: 235.4767\n",
      "Epoch 504/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.0153 - val_loss: 225.0647\n",
      "Epoch 505/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 112.7198 - val_loss: 191.9480\n",
      "Epoch 506/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 124.4849 - val_loss: 178.0642\n",
      "Epoch 507/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.1067 - val_loss: 169.6676\n",
      "Epoch 508/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.9755 - val_loss: 157.7375\n",
      "Epoch 509/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.3089 - val_loss: 133.9795\n",
      "Epoch 510/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.0159 - val_loss: 202.4643\n",
      "Epoch 511/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 111.1596 - val_loss: 197.1084\n",
      "Epoch 512/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 109.5064 - val_loss: 151.5236\n",
      "Epoch 513/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.1600 - val_loss: 181.4456\n",
      "Epoch 514/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.8177 - val_loss: 177.9207\n",
      "Epoch 515/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 106.0344 - val_loss: 126.3411\n",
      "Epoch 516/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.2959 - val_loss: 152.1856\n",
      "Epoch 517/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.2910 - val_loss: 131.7422\n",
      "Epoch 518/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 117.9081 - val_loss: 117.1828\n",
      "Epoch 519/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 116.5505 - val_loss: 95.2840\n",
      "Epoch 520/10000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 118.3824 - val_loss: 91.8452\n",
      "Epoch 521/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.9392 - val_loss: 150.1003\n",
      "Epoch 522/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.0996 - val_loss: 208.1327\n",
      "Epoch 523/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.7514 - val_loss: 191.3163\n",
      "Epoch 524/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.3773 - val_loss: 141.3831\n",
      "Epoch 525/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.9032 - val_loss: 160.8088\n",
      "Epoch 526/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.1233 - val_loss: 135.9879\n",
      "Epoch 527/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.4186 - val_loss: 92.8799\n",
      "Epoch 528/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.5597 - val_loss: 161.9281\n",
      "Epoch 529/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.6216 - val_loss: 117.6119\n",
      "Epoch 530/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.4523 - val_loss: 205.3148\n",
      "Epoch 531/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 112.1320 - val_loss: 164.3394\n",
      "Epoch 532/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 119.8164 - val_loss: 212.8519\n",
      "Epoch 533/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.1568 - val_loss: 160.6159\n",
      "Epoch 534/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 106.5595 - val_loss: 160.4709\n",
      "Epoch 535/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.0695 - val_loss: 206.6183\n",
      "Epoch 536/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.0459 - val_loss: 120.8643\n",
      "Epoch 537/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.6101 - val_loss: 143.8728\n",
      "Epoch 538/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.4831 - val_loss: 145.0705\n",
      "Epoch 539/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 112.8014 - val_loss: 142.3742\n",
      "Epoch 540/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.5146 - val_loss: 142.5234\n",
      "Epoch 541/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.4978 - val_loss: 163.1084\n",
      "Epoch 542/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.1300 - val_loss: 148.1175\n",
      "Epoch 543/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.1331 - val_loss: 147.3055\n",
      "Epoch 544/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.0171 - val_loss: 151.3815\n",
      "Epoch 545/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.7054 - val_loss: 120.0686\n",
      "Epoch 546/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.8885 - val_loss: 115.6905\n",
      "Epoch 547/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 100.4076 - val_loss: 140.9748\n",
      "Epoch 548/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 96.5429 - val_loss: 179.1364\n",
      "Epoch 549/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 93.1258 - val_loss: 199.2187\n",
      "Epoch 550/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.2987 - val_loss: 208.5320\n",
      "Epoch 551/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.9386 - val_loss: 183.4868\n",
      "Epoch 552/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 90.4787 - val_loss: 135.0897\n",
      "Epoch 553/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 92.4167 - val_loss: 138.6149\n",
      "Epoch 554/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 84.7501 - val_loss: 162.2660\n",
      "Epoch 555/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.7887 - val_loss: 184.4529\n",
      "Epoch 556/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 85.6426 - val_loss: 181.5954\n",
      "Epoch 557/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 87.0907 - val_loss: 187.4831\n",
      "Epoch 558/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.7759 - val_loss: 194.1461\n",
      "Epoch 559/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.1540 - val_loss: 264.2067\n",
      "Epoch 560/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 116.1167 - val_loss: 265.8765\n",
      "Epoch 561/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 119.8689 - val_loss: 209.7493\n",
      "Epoch 562/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 106.3788 - val_loss: 118.4985\n",
      "Epoch 563/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 180.9304 - val_loss: 108.6272\n",
      "Epoch 564/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.9961 - val_loss: 257.8299\n",
      "Epoch 565/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.0963 - val_loss: 161.5063\n",
      "Epoch 566/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 118.9620 - val_loss: 125.2298\n",
      "Epoch 567/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.6966 - val_loss: 210.1572\n",
      "Epoch 568/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 142.1165 - val_loss: 137.3628\n",
      "Epoch 569/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.9245 - val_loss: 151.6233\n",
      "Epoch 570/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.6080 - val_loss: 134.5679\n",
      "Epoch 571/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.4445 - val_loss: 118.0183\n",
      "Epoch 572/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.8135 - val_loss: 122.4604\n",
      "Epoch 573/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.9855 - val_loss: 140.3464\n",
      "Epoch 574/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.5623 - val_loss: 147.5338\n",
      "Epoch 575/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 100.3953 - val_loss: 144.4409\n",
      "Epoch 576/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.7735 - val_loss: 139.9437\n",
      "Epoch 577/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 116.4643 - val_loss: 158.5862\n",
      "Epoch 578/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.8829 - val_loss: 143.6515\n",
      "Epoch 579/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 111.9333 - val_loss: 192.6199\n",
      "Epoch 580/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.5348 - val_loss: 162.9763\n",
      "Epoch 581/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.9214 - val_loss: 186.8636\n",
      "Epoch 582/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.8991 - val_loss: 170.9848\n",
      "Epoch 583/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.6837 - val_loss: 117.4869\n",
      "Epoch 584/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.8661 - val_loss: 113.4255\n",
      "Epoch 585/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 144.8066 - val_loss: 113.6840\n",
      "Epoch 586/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.4523 - val_loss: 120.9510\n",
      "Epoch 587/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.3919 - val_loss: 227.4149\n",
      "Epoch 588/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 161.8618 - val_loss: 117.0191\n",
      "Epoch 589/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 116.6711 - val_loss: 113.4044\n",
      "Epoch 590/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.2931 - val_loss: 149.1418\n",
      "Epoch 591/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.6547 - val_loss: 383.4786\n",
      "Epoch 592/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 337.8403 - val_loss: 205.9078\n",
      "Epoch 593/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 168.9871 - val_loss: 143.6188\n",
      "Epoch 594/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.7609 - val_loss: 149.3457\n",
      "Epoch 595/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.1546 - val_loss: 126.2318\n",
      "Epoch 596/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.7125 - val_loss: 117.4516\n",
      "Epoch 597/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.7871 - val_loss: 127.6490\n",
      "Epoch 598/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 130.6795 - val_loss: 137.5894\n",
      "Epoch 599/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.8737 - val_loss: 157.0955\n",
      "Epoch 600/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.1646 - val_loss: 173.2785\n",
      "Epoch 601/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.1351 - val_loss: 133.3260\n",
      "Epoch 602/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.0285 - val_loss: 168.6119\n",
      "Epoch 603/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.1348 - val_loss: 154.9642\n",
      "Epoch 604/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.7558 - val_loss: 232.0238\n",
      "Epoch 605/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 153.8983 - val_loss: 220.8913\n",
      "Epoch 606/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 162.4976 - val_loss: 198.0446\n",
      "Epoch 607/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 140.9800 - val_loss: 208.2649\n",
      "Epoch 608/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.4346 - val_loss: 198.9431\n",
      "Epoch 609/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.2603 - val_loss: 156.0605\n",
      "Epoch 610/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.0304 - val_loss: 156.6524\n",
      "Epoch 611/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.0882 - val_loss: 307.3185\n",
      "Epoch 612/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 179.4356 - val_loss: 150.8699\n",
      "Epoch 613/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 140.6630 - val_loss: 113.0069\n",
      "Epoch 614/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 140.9096 - val_loss: 113.0637\n",
      "Epoch 615/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.5618 - val_loss: 201.9370\n",
      "Epoch 616/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 180.1169 - val_loss: 157.9242\n",
      "Epoch 617/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 156.7935 - val_loss: 113.2143\n",
      "Epoch 618/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.0409 - val_loss: 143.1895\n",
      "Epoch 619/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.8602 - val_loss: 151.7420\n",
      "Epoch 620/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.4475 - val_loss: 147.9994\n",
      "Epoch 621/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.5248 - val_loss: 175.2495\n",
      "Epoch 622/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.2121 - val_loss: 165.1719\n",
      "Epoch 623/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 117.4454 - val_loss: 152.8672\n",
      "Epoch 624/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.1171 - val_loss: 178.7001\n",
      "Epoch 625/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 125.3707 - val_loss: 120.9669\n",
      "Epoch 626/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.1870 - val_loss: 162.5875\n",
      "Epoch 627/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 107.4711 - val_loss: 145.9633\n",
      "Epoch 628/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.8071 - val_loss: 156.5777\n",
      "Epoch 629/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 100.8389 - val_loss: 180.3808\n",
      "Epoch 630/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 107.7554 - val_loss: 176.0263\n",
      "Epoch 631/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.0091 - val_loss: 455.6332\n",
      "Epoch 632/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 384.4100 - val_loss: 209.5762\n",
      "Epoch 633/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.3659 - val_loss: 114.0058\n",
      "Epoch 634/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 195.7135 - val_loss: 97.7250\n",
      "Epoch 635/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 161.2777 - val_loss: 173.2583\n",
      "Epoch 636/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 160.6011 - val_loss: 115.8018\n",
      "Epoch 637/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 153.9788 - val_loss: 134.1733\n",
      "Epoch 638/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 148.1894 - val_loss: 121.9678\n",
      "Epoch 639/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 134.0032 - val_loss: 105.4130\n",
      "Epoch 640/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.5949 - val_loss: 89.0182\n",
      "Epoch 641/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.8858 - val_loss: 252.3884\n",
      "Epoch 642/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 167.2333 - val_loss: 142.5940\n",
      "Epoch 643/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 195.9072 - val_loss: 149.0173\n",
      "Epoch 644/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 157.5739 - val_loss: 117.3298\n",
      "Epoch 645/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 134.1093 - val_loss: 134.4250\n",
      "Epoch 646/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.2480 - val_loss: 100.4790\n",
      "Epoch 647/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.4114 - val_loss: 123.9818\n",
      "Epoch 648/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 157.2878 - val_loss: 116.3635\n",
      "Epoch 649/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 150.4917 - val_loss: 141.5940\n",
      "Epoch 650/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.7246 - val_loss: 162.3713\n",
      "Epoch 651/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.0994 - val_loss: 161.5501\n",
      "Epoch 652/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.3559 - val_loss: 154.1970\n",
      "Epoch 653/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 155.8623 - val_loss: 163.7052\n",
      "Epoch 654/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.0474 - val_loss: 135.0835\n",
      "Epoch 655/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.8849 - val_loss: 162.8750\n",
      "Epoch 656/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.1553 - val_loss: 124.5828\n",
      "Epoch 657/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.6875 - val_loss: 116.7346\n",
      "Epoch 658/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.6836 - val_loss: 130.4239\n",
      "Epoch 659/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.4087 - val_loss: 149.9619\n",
      "Epoch 660/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.7546 - val_loss: 167.4530\n",
      "Epoch 661/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.2020 - val_loss: 118.4131\n",
      "Epoch 662/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.5900 - val_loss: 101.0395\n",
      "Epoch 663/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 206.7989 - val_loss: 186.3746\n",
      "Epoch 664/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 264.8086 - val_loss: 109.1601\n",
      "Epoch 665/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.1132 - val_loss: 97.0250\n",
      "Epoch 666/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 192.9587 - val_loss: 106.6917\n",
      "Epoch 667/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 165.8190 - val_loss: 143.5930\n",
      "Epoch 668/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 139.7698 - val_loss: 106.6804\n",
      "Epoch 669/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 149.2899 - val_loss: 97.1938\n",
      "Epoch 670/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 156.4915 - val_loss: 100.2593\n",
      "Epoch 671/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.7041 - val_loss: 106.6415\n",
      "Epoch 672/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 177.4971 - val_loss: 141.0907\n",
      "Epoch 673/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.1031 - val_loss: 148.8089\n",
      "Epoch 674/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 139.1492 - val_loss: 145.9996\n",
      "Epoch 675/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.2324 - val_loss: 165.8390\n",
      "Epoch 676/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.2656 - val_loss: 172.6953\n",
      "Epoch 677/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 147.1258 - val_loss: 143.2897\n",
      "Epoch 678/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.4280 - val_loss: 121.9636\n",
      "Epoch 679/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 134.0987 - val_loss: 111.0650\n",
      "Epoch 680/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 112.7473 - val_loss: 126.3494\n",
      "Epoch 681/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.8107 - val_loss: 132.1819\n",
      "Epoch 682/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.6074 - val_loss: 116.8074\n",
      "Epoch 683/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.2319 - val_loss: 110.1049\n",
      "Epoch 684/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 136.6414 - val_loss: 125.0514\n",
      "Epoch 685/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 133.1443 - val_loss: 127.4916\n",
      "Epoch 686/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.6496 - val_loss: 159.8083\n",
      "Epoch 687/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.3934 - val_loss: 130.7894\n",
      "Epoch 688/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.9398 - val_loss: 137.6817\n",
      "Epoch 689/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.3700 - val_loss: 152.8329\n",
      "Epoch 690/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.1758 - val_loss: 174.4518\n",
      "Epoch 691/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 194.8399 - val_loss: 210.9549\n",
      "Epoch 692/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 237.6337 - val_loss: 275.8967\n",
      "Epoch 693/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 313.6890 - val_loss: 189.3141\n",
      "Epoch 694/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 255.4875 - val_loss: 212.2559\n",
      "Epoch 695/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 240.7499 - val_loss: 183.5652\n",
      "Epoch 696/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 253.9761 - val_loss: 189.1412\n",
      "Epoch 697/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 227.9989 - val_loss: 173.9512\n",
      "Epoch 698/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 209.3073 - val_loss: 189.7475\n",
      "Epoch 699/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 191.3340 - val_loss: 165.2543\n",
      "Epoch 700/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 157.6938 - val_loss: 179.0808\n",
      "Epoch 701/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 164.4401 - val_loss: 191.7519\n",
      "Epoch 702/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 153.2641 - val_loss: 164.2901\n",
      "Epoch 703/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 140.9897 - val_loss: 148.8729\n",
      "Epoch 704/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.1603 - val_loss: 148.6335\n",
      "Epoch 705/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.9638 - val_loss: 194.2022\n",
      "Epoch 706/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 211.9640 - val_loss: 140.6123\n",
      "Epoch 707/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 203.0275 - val_loss: 179.2903\n",
      "Epoch 708/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 311.8444 - val_loss: 254.8231\n",
      "Epoch 709/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 276.1686 - val_loss: 217.1401\n",
      "Epoch 710/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 295.5851 - val_loss: 211.1773\n",
      "Epoch 711/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 224.7148 - val_loss: 166.2036\n",
      "Epoch 712/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.2692 - val_loss: 125.3423\n",
      "Epoch 713/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1018.6794 - val_loss: 452.9418\n",
      "Epoch 714/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 369.1862 - val_loss: 225.3806\n",
      "Epoch 715/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 216.9070 - val_loss: 148.3220\n",
      "Epoch 716/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 232.8020 - val_loss: 590.7792\n",
      "Epoch 717/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 515.0270 - val_loss: 175.1044\n",
      "Epoch 718/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 218.9239 - val_loss: 173.7828\n",
      "Epoch 719/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 182.7216 - val_loss: 226.9346\n",
      "Epoch 720/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 232.1567 - val_loss: 160.5391\n",
      "Epoch 721/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 223.1270 - val_loss: 217.5540\n",
      "Epoch 722/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 219.5250 - val_loss: 113.7259\n",
      "Epoch 723/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 149.7479 - val_loss: 114.2264\n",
      "Epoch 724/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.2170 - val_loss: 182.0794\n",
      "Epoch 725/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 161.6642 - val_loss: 119.5580\n",
      "Epoch 726/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 166.7005 - val_loss: 141.5618\n",
      "Epoch 727/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 156.8322 - val_loss: 186.4840\n",
      "Epoch 728/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 170.5181 - val_loss: 118.8977\n",
      "Epoch 729/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 171.5523 - val_loss: 120.0895\n",
      "Epoch 730/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 152.8020 - val_loss: 127.3342\n",
      "Epoch 731/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.1788 - val_loss: 123.1009\n",
      "Epoch 732/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 150.1768 - val_loss: 129.6343\n",
      "Epoch 733/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 141.5126 - val_loss: 100.9443\n",
      "Epoch 734/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 143.6779 - val_loss: 144.1605\n",
      "Epoch 735/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 168.9339 - val_loss: 172.9902\n",
      "Epoch 736/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 166.0434 - val_loss: 141.5009\n",
      "Epoch 737/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 157.8286 - val_loss: 165.3145\n",
      "Epoch 738/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 157.0823 - val_loss: 137.4203\n",
      "Epoch 739/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.0958 - val_loss: 131.4969\n",
      "Epoch 740/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 138.8895 - val_loss: 145.4786\n",
      "Epoch 741/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 139.2134 - val_loss: 123.1477\n",
      "Epoch 742/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.1316 - val_loss: 129.6945\n",
      "Epoch 743/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.5690 - val_loss: 144.7605\n",
      "Epoch 744/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 137.6575 - val_loss: 143.6245\n",
      "Epoch 745/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.1269 - val_loss: 185.8400\n",
      "Epoch 746/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 133.6801 - val_loss: 129.4588\n",
      "Epoch 747/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.3220 - val_loss: 110.6869\n",
      "Epoch 748/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 181.5381 - val_loss: 126.3392\n",
      "Epoch 749/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.0400 - val_loss: 180.2281\n",
      "Epoch 750/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.2325 - val_loss: 122.4664\n",
      "Epoch 751/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 120.7341 - val_loss: 135.3745\n",
      "Epoch 752/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.1360 - val_loss: 117.7797\n",
      "Epoch 753/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.2314 - val_loss: 139.5819\n",
      "Epoch 754/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.2005 - val_loss: 149.8873\n",
      "Epoch 755/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.6487 - val_loss: 167.2610\n",
      "Epoch 756/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.1991 - val_loss: 205.8446\n",
      "Epoch 757/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 174.8234 - val_loss: 148.7666\n",
      "Epoch 758/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 156.2457 - val_loss: 146.5367\n",
      "Epoch 759/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.2291 - val_loss: 151.9593\n",
      "Epoch 760/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 111.2232 - val_loss: 154.0614\n",
      "Epoch 761/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.7647 - val_loss: 128.2555\n",
      "Epoch 762/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.5726 - val_loss: 76.6620\n",
      "Epoch 763/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 180.0504 - val_loss: 220.3341\n",
      "Epoch 764/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 206.1628 - val_loss: 136.9586\n",
      "Epoch 765/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 172.8360 - val_loss: 100.6705\n",
      "Epoch 766/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 162.9289 - val_loss: 153.7195\n",
      "Epoch 767/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 189.2991 - val_loss: 217.2754\n",
      "Epoch 768/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 213.6565 - val_loss: 268.6644\n",
      "Epoch 769/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 212.0264 - val_loss: 143.3631\n",
      "Epoch 770/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 188.5967 - val_loss: 154.4260\n",
      "Epoch 771/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 196.2445 - val_loss: 138.2127\n",
      "Epoch 772/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 181.1009 - val_loss: 154.0482\n",
      "Epoch 773/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 171.9894 - val_loss: 191.6971\n",
      "Epoch 774/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 200.1260 - val_loss: 164.5780\n",
      "Epoch 775/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 162.4911 - val_loss: 85.8046\n",
      "Epoch 776/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 175.6690 - val_loss: 139.8745\n",
      "Epoch 777/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 183.0592 - val_loss: 124.4212\n",
      "Epoch 778/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 156.5988 - val_loss: 175.4397\n",
      "Epoch 779/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.6071 - val_loss: 130.3712\n",
      "Epoch 780/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.9921 - val_loss: 129.9712\n",
      "Epoch 781/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 130.6363 - val_loss: 108.3454\n",
      "Epoch 782/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 205.5001 - val_loss: 121.4508\n",
      "Epoch 783/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 187.3863 - val_loss: 109.1071\n",
      "Epoch 784/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 208.7340 - val_loss: 126.2406\n",
      "Epoch 785/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 174.6889 - val_loss: 99.6308\n",
      "Epoch 786/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 160.0523 - val_loss: 129.0505\n",
      "Epoch 787/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 171.2710 - val_loss: 119.2459\n",
      "Epoch 788/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 179.4974 - val_loss: 138.1460\n",
      "Epoch 789/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 177.8366 - val_loss: 298.7466\n",
      "Epoch 790/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 294.9134 - val_loss: 123.7468\n",
      "Epoch 791/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 189.1831 - val_loss: 267.5047\n",
      "Epoch 792/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 293.4612 - val_loss: 176.9382\n",
      "Epoch 793/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 204.7947 - val_loss: 154.5868\n",
      "Epoch 794/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 184.8869 - val_loss: 123.8938\n",
      "Epoch 795/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 153.2749 - val_loss: 130.1271\n",
      "Epoch 796/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 178.9064 - val_loss: 126.8041\n",
      "Epoch 797/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 188.5197 - val_loss: 108.8763\n",
      "Epoch 798/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 154.9682 - val_loss: 137.4536\n",
      "Epoch 799/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 146.7192 - val_loss: 147.0800\n",
      "Epoch 800/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 167.6263 - val_loss: 129.0109\n",
      "Epoch 801/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 156.6558 - val_loss: 138.5873\n",
      "Epoch 802/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.4162 - val_loss: 195.7162\n",
      "Epoch 803/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 161.4639 - val_loss: 192.3676\n",
      "Epoch 804/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 196.4166 - val_loss: 152.6614\n",
      "Epoch 805/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 171.7986 - val_loss: 171.2381\n",
      "Epoch 806/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 148.5331 - val_loss: 105.7188\n",
      "Epoch 807/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 146.4356 - val_loss: 207.9911\n",
      "Epoch 808/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 199.0208 - val_loss: 153.0541\n",
      "Epoch 809/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 174.0125 - val_loss: 207.3312\n",
      "Epoch 810/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 169.6074 - val_loss: 145.0058\n",
      "Epoch 811/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 155.4214 - val_loss: 159.4892\n",
      "Epoch 812/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 165.8713 - val_loss: 160.8037\n",
      "Epoch 813/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 169.7636 - val_loss: 145.1750\n",
      "Epoch 814/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 155.1416 - val_loss: 198.7378\n",
      "Epoch 815/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 179.6915 - val_loss: 228.1509\n",
      "Epoch 816/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 159.3274 - val_loss: 144.3750\n",
      "Epoch 817/10000\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 136.8436 - val_loss: 206.4873\n",
      "Epoch 818/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 156.7133 - val_loss: 149.6373\n",
      "Epoch 819/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 147.3935 - val_loss: 113.2177\n",
      "Epoch 820/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 163.8742 - val_loss: 147.9446\n",
      "Epoch 821/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.5967 - val_loss: 117.5777\n",
      "Epoch 822/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.8636 - val_loss: 128.3011\n",
      "Epoch 823/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.5706 - val_loss: 128.5213\n",
      "Epoch 824/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 145.3648 - val_loss: 129.0233\n",
      "Epoch 825/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 116.0257 - val_loss: 113.8801\n",
      "Epoch 826/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.3374 - val_loss: 162.4274\n",
      "Epoch 827/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 145.9420 - val_loss: 114.0419\n",
      "Epoch 828/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 144.5507 - val_loss: 136.3869\n",
      "Epoch 829/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 138.0538 - val_loss: 136.4551\n",
      "Epoch 830/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 186.4510 - val_loss: 202.4257\n",
      "Epoch 831/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 174.8598 - val_loss: 133.9428\n",
      "Epoch 832/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 145.3166 - val_loss: 196.4047\n",
      "Epoch 833/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 190.3704 - val_loss: 130.2801\n",
      "Epoch 834/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 170.7551 - val_loss: 133.2792\n",
      "Epoch 835/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.7741 - val_loss: 125.1338\n",
      "Epoch 836/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 156.4243 - val_loss: 118.1998\n",
      "Epoch 837/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 159.1206 - val_loss: 125.8480\n",
      "Epoch 838/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.4426 - val_loss: 128.9885\n",
      "Epoch 839/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 136.5349 - val_loss: 142.9199\n",
      "Epoch 840/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.6736 - val_loss: 140.0548\n",
      "Epoch 841/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 138.9658 - val_loss: 118.0889\n",
      "Epoch 842/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.3608 - val_loss: 112.7743\n",
      "Epoch 843/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 154.6127 - val_loss: 108.3878\n",
      "Epoch 844/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.5115 - val_loss: 138.6617\n",
      "Epoch 845/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.7332 - val_loss: 97.4719\n",
      "Epoch 846/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.3114 - val_loss: 404.3539\n",
      "Epoch 847/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 287.1660 - val_loss: 99.4510\n",
      "Epoch 848/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.1191 - val_loss: 91.9127\n",
      "Epoch 849/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 151.0564 - val_loss: 81.6143\n",
      "Epoch 850/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 139.7745 - val_loss: 113.7193\n",
      "Epoch 851/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.2409 - val_loss: 110.5139\n",
      "Epoch 852/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 126.5533 - val_loss: 127.5721\n",
      "Epoch 853/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 119.1442 - val_loss: 140.0728\n",
      "Epoch 854/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 116.6662 - val_loss: 179.7509\n",
      "Epoch 855/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 149.0342 - val_loss: 126.0182\n",
      "Epoch 856/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.2060 - val_loss: 99.8101\n",
      "Epoch 857/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.8815 - val_loss: 193.4451\n",
      "Epoch 858/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 135.7341 - val_loss: 88.4079\n",
      "Epoch 859/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 221.7986 - val_loss: 211.7974\n",
      "Epoch 860/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 178.7613 - val_loss: 183.8840\n",
      "Epoch 861/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.8470 - val_loss: 145.5476\n",
      "Epoch 862/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 161.4189 - val_loss: 123.9133\n",
      "Epoch 863/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 164.0369 - val_loss: 111.9485\n",
      "Epoch 864/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 139.8631 - val_loss: 123.5195\n",
      "Epoch 865/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.4894 - val_loss: 156.9877\n",
      "Epoch 866/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 153.9628 - val_loss: 141.2869\n",
      "Epoch 867/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.0337 - val_loss: 95.3350\n",
      "Epoch 868/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.4156 - val_loss: 161.7962\n",
      "Epoch 869/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 103.9574 - val_loss: 135.5117\n",
      "Epoch 870/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.7644 - val_loss: 181.6630\n",
      "Epoch 871/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.9321 - val_loss: 193.8025\n",
      "Epoch 872/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.4225 - val_loss: 150.2625\n",
      "Epoch 873/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 113.7223 - val_loss: 173.3522\n",
      "Epoch 874/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.8579 - val_loss: 251.7865\n",
      "Epoch 875/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 127.8073 - val_loss: 170.0525\n",
      "Epoch 876/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.6853 - val_loss: 177.2711\n",
      "Epoch 877/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.3271 - val_loss: 175.6733\n",
      "Epoch 878/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.3060 - val_loss: 142.1184\n",
      "Epoch 879/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 94.1185 - val_loss: 175.7801\n",
      "Epoch 880/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 148.0725 - val_loss: 171.4955\n",
      "Epoch 881/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.3316 - val_loss: 144.4782\n",
      "Epoch 882/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.7356 - val_loss: 200.4083\n",
      "Epoch 883/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.8171 - val_loss: 152.5064\n",
      "Epoch 884/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.4939 - val_loss: 151.8421\n",
      "Epoch 885/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.0663 - val_loss: 177.3271\n",
      "Epoch 886/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.0501 - val_loss: 109.7684\n",
      "Epoch 887/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 120.2418 - val_loss: 143.2653\n",
      "Epoch 888/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.3097 - val_loss: 126.5721\n",
      "Epoch 889/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 106.4741 - val_loss: 133.0825\n",
      "Epoch 890/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.2818 - val_loss: 166.8552\n",
      "Epoch 891/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.7450 - val_loss: 110.4520\n",
      "Epoch 892/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.0766 - val_loss: 133.4957\n",
      "Epoch 893/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.3470 - val_loss: 112.0772\n",
      "Epoch 894/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 116.0355 - val_loss: 152.3270\n",
      "Epoch 895/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.5592 - val_loss: 154.1046\n",
      "Epoch 896/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 119.9348 - val_loss: 127.4733\n",
      "Epoch 897/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 109.4923 - val_loss: 137.1584\n",
      "Epoch 898/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.4904 - val_loss: 118.6095\n",
      "Epoch 899/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.3909 - val_loss: 107.2716\n",
      "Epoch 900/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.3191 - val_loss: 105.7260\n",
      "Epoch 901/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 183.5120 - val_loss: 188.4726\n",
      "Epoch 902/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 203.8041 - val_loss: 214.7483\n",
      "Epoch 903/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 177.2733 - val_loss: 176.4070\n",
      "Epoch 904/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 188.7992 - val_loss: 189.6335\n",
      "Epoch 905/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.6360 - val_loss: 108.7916\n",
      "Epoch 906/10000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 129.7460 - val_loss: 155.3951\n",
      "Epoch 907/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.9013 - val_loss: 109.2433\n",
      "Epoch 908/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 125.2099 - val_loss: 156.7463\n",
      "Epoch 909/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.7077 - val_loss: 126.5484\n",
      "Epoch 910/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.0351 - val_loss: 112.9023\n",
      "Epoch 911/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.8393 - val_loss: 131.6061\n",
      "Epoch 912/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.2647 - val_loss: 150.3251\n",
      "Epoch 913/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 147.4530 - val_loss: 125.8719\n",
      "Epoch 914/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.7074 - val_loss: 132.9939\n",
      "Epoch 915/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.2561 - val_loss: 134.8826\n",
      "Epoch 916/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.0103 - val_loss: 129.9523\n",
      "Epoch 917/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.1986 - val_loss: 110.2390\n",
      "Epoch 918/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.7648 - val_loss: 127.0824\n",
      "Epoch 919/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.7998 - val_loss: 140.1345\n",
      "Epoch 920/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.3201 - val_loss: 104.7683\n",
      "Epoch 921/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.7468 - val_loss: 117.3158\n",
      "Epoch 922/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.0420 - val_loss: 114.0358\n",
      "Epoch 923/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 141.7230 - val_loss: 160.0787\n",
      "Epoch 924/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 122.5520 - val_loss: 118.4582\n",
      "Epoch 925/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.7441 - val_loss: 127.0899\n",
      "Epoch 926/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.5889 - val_loss: 153.0995\n",
      "Epoch 927/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.6496 - val_loss: 154.8985\n",
      "Epoch 928/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 94.4588 - val_loss: 135.2548\n",
      "Epoch 929/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.9583 - val_loss: 251.9177\n",
      "Epoch 930/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 157.4254 - val_loss: 160.1076\n",
      "Epoch 931/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.1715 - val_loss: 124.4870\n",
      "Epoch 932/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.3524 - val_loss: 268.0025\n",
      "Epoch 933/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 164.1786 - val_loss: 206.1337\n",
      "Epoch 934/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 133.2642 - val_loss: 178.2327\n",
      "Epoch 935/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 99.3734 - val_loss: 234.8997\n",
      "Epoch 936/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.6374 - val_loss: 216.9319\n",
      "Epoch 937/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 107.6593 - val_loss: 190.7178\n",
      "Epoch 938/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 96.3937 - val_loss: 192.0420\n",
      "Epoch 939/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 115.1868 - val_loss: 215.4492\n",
      "Epoch 940/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.2830 - val_loss: 194.3687\n",
      "Epoch 941/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.5534 - val_loss: 171.4805\n",
      "Epoch 942/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.7824 - val_loss: 233.5591\n",
      "Epoch 943/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.0694 - val_loss: 211.7163\n",
      "Epoch 944/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.4588 - val_loss: 208.4164\n",
      "Epoch 945/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 116.4210 - val_loss: 223.1996\n",
      "Epoch 946/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.8050 - val_loss: 147.0398\n",
      "Epoch 947/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 113.3133 - val_loss: 192.0428\n",
      "Epoch 948/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.1551 - val_loss: 153.5218\n",
      "Epoch 949/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.5741 - val_loss: 201.2884\n",
      "Epoch 950/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.8865 - val_loss: 159.8622\n",
      "Epoch 951/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.0800 - val_loss: 143.8857\n",
      "Epoch 952/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.9456 - val_loss: 125.2883\n",
      "Epoch 953/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 159.2184 - val_loss: 202.0712\n",
      "Epoch 954/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 181.9877 - val_loss: 198.6135\n",
      "Epoch 955/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 142.2726 - val_loss: 145.9569\n",
      "Epoch 956/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.4303 - val_loss: 170.2986\n",
      "Epoch 957/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 161.9855 - val_loss: 125.4296\n",
      "Epoch 958/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.7152 - val_loss: 136.7153\n",
      "Epoch 959/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 113.5384 - val_loss: 164.0885\n",
      "Epoch 960/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.4302 - val_loss: 157.6145\n",
      "Epoch 961/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.1139 - val_loss: 164.0613\n",
      "Epoch 962/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.3416 - val_loss: 162.1101\n",
      "Epoch 963/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.7719 - val_loss: 210.8769\n",
      "Epoch 964/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 150.0016 - val_loss: 209.6493\n",
      "Epoch 965/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 148.4514 - val_loss: 146.3282\n",
      "Epoch 966/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 176.1824 - val_loss: 119.3551\n",
      "Epoch 967/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.1028 - val_loss: 149.9684\n",
      "Epoch 968/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 158.0231 - val_loss: 129.9359\n",
      "Epoch 969/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.3729 - val_loss: 147.8395\n",
      "Epoch 970/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 144.1689 - val_loss: 129.2055\n",
      "Epoch 971/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 144.0307 - val_loss: 137.9272\n",
      "Epoch 972/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 186.5612 - val_loss: 201.9655\n",
      "Epoch 973/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.0221 - val_loss: 149.0851\n",
      "Epoch 974/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.3031 - val_loss: 127.5366\n",
      "Epoch 975/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.8743 - val_loss: 116.3600\n",
      "Epoch 976/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 156.5284 - val_loss: 100.9898\n",
      "Epoch 977/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.0310 - val_loss: 106.4861\n",
      "Epoch 978/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 114.8982 - val_loss: 76.5774\n",
      "Epoch 979/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 146.5117 - val_loss: 115.8097\n",
      "Epoch 980/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.0966 - val_loss: 99.9648\n",
      "Epoch 981/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 160.3430 - val_loss: 63.8814\n",
      "Epoch 982/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 140.6488 - val_loss: 64.9012\n",
      "Epoch 983/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 139.4181 - val_loss: 108.6574\n",
      "Epoch 984/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 156.4538 - val_loss: 104.5300\n",
      "Epoch 985/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.2589 - val_loss: 129.7817\n",
      "Epoch 986/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.9080 - val_loss: 108.7341\n",
      "Epoch 987/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.6382 - val_loss: 115.3491\n",
      "Epoch 988/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.6872 - val_loss: 136.5287\n",
      "Epoch 989/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.0096 - val_loss: 146.8332\n",
      "Epoch 990/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.6021 - val_loss: 97.4817\n",
      "Epoch 991/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.8535 - val_loss: 173.3444\n",
      "Epoch 992/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.4377 - val_loss: 163.0936\n",
      "Epoch 993/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.2921 - val_loss: 138.0040\n",
      "Epoch 994/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.9227 - val_loss: 159.7190\n",
      "Epoch 995/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.6277 - val_loss: 162.6521\n",
      "Epoch 996/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.0215 - val_loss: 147.8447\n",
      "Epoch 997/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.9473 - val_loss: 154.1777\n",
      "Epoch 998/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.4653 - val_loss: 149.2009\n",
      "Epoch 999/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 116.9381 - val_loss: 119.8064\n",
      "Epoch 1000/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.7803 - val_loss: 130.9675\n",
      "Epoch 1001/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.0664 - val_loss: 157.7453\n",
      "Epoch 1002/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.3425 - val_loss: 166.9634\n",
      "Epoch 1003/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.3496 - val_loss: 154.6658\n",
      "Epoch 1004/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.0735 - val_loss: 156.1042\n",
      "Epoch 1005/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.3083 - val_loss: 192.5351\n",
      "Epoch 1006/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.3934 - val_loss: 155.2240\n",
      "Epoch 1007/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 102.8255 - val_loss: 142.3577\n",
      "Epoch 1008/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.0445 - val_loss: 165.3863\n",
      "Epoch 1009/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.1336 - val_loss: 155.3729\n",
      "Epoch 1010/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.8068 - val_loss: 172.9683\n",
      "Epoch 1011/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.3533 - val_loss: 185.1225\n",
      "Epoch 1012/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.2582 - val_loss: 166.8931\n",
      "Epoch 1013/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.9016 - val_loss: 179.6663\n",
      "Epoch 1014/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.6186 - val_loss: 224.4556\n",
      "Epoch 1015/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 116.0482 - val_loss: 189.1709\n",
      "Epoch 1016/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.1009 - val_loss: 195.6078\n",
      "Epoch 1017/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.9181 - val_loss: 187.0659\n",
      "Epoch 1018/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.7809 - val_loss: 197.4778\n",
      "Epoch 1019/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 109.2328 - val_loss: 183.3354\n",
      "Epoch 1020/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.2011 - val_loss: 157.3975\n",
      "Epoch 1021/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.9734 - val_loss: 238.0311\n",
      "Epoch 1022/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.2051 - val_loss: 209.4218\n",
      "Epoch 1023/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.4961 - val_loss: 206.7674\n",
      "Epoch 1024/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.5479 - val_loss: 175.2620\n",
      "Epoch 1025/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.9776 - val_loss: 206.1313\n",
      "Epoch 1026/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 109.0310 - val_loss: 160.7508\n",
      "Epoch 1027/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.4236 - val_loss: 197.3137\n",
      "Epoch 1028/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 87.4919 - val_loss: 206.6707\n",
      "Epoch 1029/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 91.2740 - val_loss: 208.9063\n",
      "Epoch 1030/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 90.0407 - val_loss: 211.1749\n",
      "Epoch 1031/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.7859 - val_loss: 206.6855\n",
      "Epoch 1032/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 109.5349 - val_loss: 184.6429\n",
      "Epoch 1033/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 134.3884 - val_loss: 143.4963\n",
      "Epoch 1034/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 119.7690 - val_loss: 190.7797\n",
      "Epoch 1035/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 139.4385 - val_loss: 103.1872\n",
      "Epoch 1036/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.3233 - val_loss: 139.7725\n",
      "Epoch 1037/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.4845 - val_loss: 138.0142\n",
      "Epoch 1038/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.0394 - val_loss: 114.7022\n",
      "Epoch 1039/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.3043 - val_loss: 128.7775\n",
      "Epoch 1040/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 106.7671 - val_loss: 125.3744\n",
      "Epoch 1041/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 99.7942 - val_loss: 131.2370\n",
      "Epoch 1042/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.8644 - val_loss: 126.1443\n",
      "Epoch 1043/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.2566 - val_loss: 170.1565\n",
      "Epoch 1044/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.2773 - val_loss: 130.1288\n",
      "Epoch 1045/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.5306 - val_loss: 123.7076\n",
      "Epoch 1046/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.4080 - val_loss: 125.9572\n",
      "Epoch 1047/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.4409 - val_loss: 164.4160\n",
      "Epoch 1048/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.1069 - val_loss: 177.4145\n",
      "Epoch 1049/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 103.5280 - val_loss: 176.5075\n",
      "Epoch 1050/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.6500 - val_loss: 138.8973\n",
      "Epoch 1051/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.9352 - val_loss: 156.0016\n",
      "Epoch 1052/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.5181 - val_loss: 166.4975\n",
      "Epoch 1053/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 103.3079 - val_loss: 178.1432\n",
      "Epoch 1054/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 94.6267 - val_loss: 188.0323\n",
      "Epoch 1055/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.9350 - val_loss: 126.9910\n",
      "Epoch 1056/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.4188 - val_loss: 146.8276\n",
      "Epoch 1057/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 111.9547 - val_loss: 169.1062\n",
      "Epoch 1058/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.5048 - val_loss: 159.4805\n",
      "Epoch 1059/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.3380 - val_loss: 183.1187\n",
      "Epoch 1060/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 107.3401 - val_loss: 210.2035\n",
      "Epoch 1061/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 109.7379 - val_loss: 127.4369\n",
      "Epoch 1062/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 102.2481 - val_loss: 214.4509\n",
      "Epoch 1063/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 154.0285 - val_loss: 178.0784\n",
      "Epoch 1064/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 155.7935 - val_loss: 120.5518\n",
      "Epoch 1065/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 150.7692 - val_loss: 218.3966\n",
      "Epoch 1066/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 159.2888 - val_loss: 146.7181\n",
      "Epoch 1067/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 152.2626 - val_loss: 145.4251\n",
      "Epoch 1068/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 145.5675 - val_loss: 123.5804\n",
      "Epoch 1069/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.9046 - val_loss: 167.3283\n",
      "Epoch 1070/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.8432 - val_loss: 140.8394\n",
      "Epoch 1071/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 150.9639 - val_loss: 136.2630\n",
      "Epoch 1072/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.7389 - val_loss: 106.3486\n",
      "Epoch 1073/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.7226 - val_loss: 108.2560\n",
      "Epoch 1074/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.9719 - val_loss: 117.3706\n",
      "Epoch 1075/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 145.1242 - val_loss: 107.0199\n",
      "Epoch 1076/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.8335 - val_loss: 127.8123\n",
      "Epoch 1077/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.5809 - val_loss: 118.4575\n",
      "Epoch 1078/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.0187 - val_loss: 123.2257\n",
      "Epoch 1079/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 188.2691 - val_loss: 126.7035\n",
      "Epoch 1080/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.3825 - val_loss: 101.8431\n",
      "Epoch 1081/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 117.3941 - val_loss: 121.0879\n",
      "Epoch 1082/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 128.5915 - val_loss: 107.7346\n",
      "Epoch 1083/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.6612 - val_loss: 107.5110\n",
      "Epoch 1084/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.7352 - val_loss: 95.7547\n",
      "Epoch 1085/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.8958 - val_loss: 111.9049\n",
      "Epoch 1086/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 146.5410 - val_loss: 90.1894\n",
      "Epoch 1087/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 145.4662 - val_loss: 148.2407\n",
      "Epoch 1088/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.9985 - val_loss: 107.6304\n",
      "Epoch 1089/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.9499 - val_loss: 101.1556\n",
      "Epoch 1090/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.8451 - val_loss: 93.6548\n",
      "Epoch 1091/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.3835 - val_loss: 120.6165\n",
      "Epoch 1092/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 132.8453 - val_loss: 98.9533\n",
      "Epoch 1093/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.1007 - val_loss: 129.0210\n",
      "Epoch 1094/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 201.3119 - val_loss: 169.0666\n",
      "Epoch 1095/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 184.1682 - val_loss: 112.7765\n",
      "Epoch 1096/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.1415 - val_loss: 92.5321\n",
      "Epoch 1097/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.1447 - val_loss: 89.5527\n",
      "Epoch 1098/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.0930 - val_loss: 82.5428\n",
      "Epoch 1099/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 144.2722 - val_loss: 166.8867\n",
      "Epoch 1100/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 142.9542 - val_loss: 141.8858\n",
      "Epoch 1101/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 135.8305 - val_loss: 132.8930\n",
      "Epoch 1102/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 123.3566 - val_loss: 131.4501\n",
      "Epoch 1103/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 123.5447 - val_loss: 135.0736\n",
      "Epoch 1104/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 109.5355 - val_loss: 141.8842\n",
      "Epoch 1105/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.1333 - val_loss: 126.8507\n",
      "Epoch 1106/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 116.3101 - val_loss: 133.2669\n",
      "Epoch 1107/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 145.4339 - val_loss: 131.1311\n",
      "Epoch 1108/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.2142 - val_loss: 141.1678\n",
      "Epoch 1109/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.9302 - val_loss: 143.7967\n",
      "Epoch 1110/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.5324 - val_loss: 132.3347\n",
      "Epoch 1111/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.6374 - val_loss: 125.8686\n",
      "Epoch 1112/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.6384 - val_loss: 130.7689\n",
      "Epoch 1113/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 137.8593 - val_loss: 156.8115\n",
      "Epoch 1114/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 126.3476 - val_loss: 135.0278\n",
      "Epoch 1115/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 137.6061 - val_loss: 162.8874\n",
      "Epoch 1116/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.5527 - val_loss: 155.8305\n",
      "Epoch 1117/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.0381 - val_loss: 180.9780\n",
      "Epoch 1118/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.9985 - val_loss: 171.3988\n",
      "Epoch 1119/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 156.3835 - val_loss: 134.9027\n",
      "Epoch 1120/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.6651 - val_loss: 145.0634\n",
      "Epoch 1121/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.9838 - val_loss: 145.8987\n",
      "Epoch 1122/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.1727 - val_loss: 147.0934\n",
      "Epoch 1123/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 139.0496 - val_loss: 95.4741\n",
      "Epoch 1124/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.1300 - val_loss: 166.2538\n",
      "Epoch 1125/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.6645 - val_loss: 156.4098\n",
      "Epoch 1126/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.6319 - val_loss: 162.3673\n",
      "Epoch 1127/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.5069 - val_loss: 162.0006\n",
      "Epoch 1128/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.8760 - val_loss: 95.8132\n",
      "Epoch 1129/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 168.6354 - val_loss: 149.1560\n",
      "Epoch 1130/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 135.2587 - val_loss: 119.7512\n",
      "Epoch 1131/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.1670 - val_loss: 116.5163\n",
      "Epoch 1132/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.4432 - val_loss: 146.9102\n",
      "Epoch 1133/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 172.8953 - val_loss: 155.1359\n",
      "Epoch 1134/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 161.2742 - val_loss: 140.5280\n",
      "Epoch 1135/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 155.2281 - val_loss: 134.7335\n",
      "Epoch 1136/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 151.2893 - val_loss: 159.5461\n",
      "Epoch 1137/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 152.7774 - val_loss: 131.1873\n",
      "Epoch 1138/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 166.4616 - val_loss: 152.2819\n",
      "Epoch 1139/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.2959 - val_loss: 192.4116\n",
      "Epoch 1140/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 140.9387 - val_loss: 158.3487\n",
      "Epoch 1141/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.1592 - val_loss: 135.8486\n",
      "Epoch 1142/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.1536 - val_loss: 126.1533\n",
      "Epoch 1143/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 149.8505 - val_loss: 146.2566\n",
      "Epoch 1144/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.8740 - val_loss: 141.4017\n",
      "Epoch 1145/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 143.1501 - val_loss: 145.2750\n",
      "Epoch 1146/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 133.6291 - val_loss: 169.4583\n",
      "Epoch 1147/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 135.7617 - val_loss: 141.1234\n",
      "Epoch 1148/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 132.2192 - val_loss: 132.1713\n",
      "Epoch 1149/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.1488 - val_loss: 130.0939\n",
      "Epoch 1150/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 130.1549 - val_loss: 144.0919\n",
      "Epoch 1151/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 127.5862 - val_loss: 142.6550\n",
      "Epoch 1152/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.1610 - val_loss: 152.9681\n",
      "Epoch 1153/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.8641 - val_loss: 168.6755\n",
      "Epoch 1154/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.1810 - val_loss: 157.0496\n",
      "Epoch 1155/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.6562 - val_loss: 194.9456\n",
      "Epoch 1156/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 133.9735 - val_loss: 169.2274\n",
      "Epoch 1157/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.8535 - val_loss: 158.5569\n",
      "Epoch 1158/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 117.7701 - val_loss: 158.2877\n",
      "Epoch 1159/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.3225 - val_loss: 167.2883\n",
      "Epoch 1160/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 109.8424 - val_loss: 188.8836\n",
      "Epoch 1161/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 131.4577 - val_loss: 150.0913\n",
      "Epoch 1162/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.1572 - val_loss: 148.6817\n",
      "Epoch 1163/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.3175 - val_loss: 147.6582\n",
      "Epoch 1164/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.3917 - val_loss: 149.0783\n",
      "Epoch 1165/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 227.7101 - val_loss: 194.6915\n",
      "Epoch 1166/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 281.0677 - val_loss: 131.4417\n",
      "Epoch 1167/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 216.5764 - val_loss: 102.7567\n",
      "Epoch 1168/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 201.8925 - val_loss: 93.6402\n",
      "Epoch 1169/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 165.1177 - val_loss: 113.4639\n",
      "Epoch 1170/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.5469 - val_loss: 174.2698\n",
      "Epoch 1171/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.2059 - val_loss: 145.1059\n",
      "Epoch 1172/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.8856 - val_loss: 164.6861\n",
      "Epoch 1173/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 126.3762 - val_loss: 172.0092\n",
      "Epoch 1174/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 100.6522 - val_loss: 161.5015\n",
      "Epoch 1175/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.0910 - val_loss: 185.8884\n",
      "Epoch 1176/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 86.2393 - val_loss: 197.4422\n",
      "Epoch 1177/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 87.3692 - val_loss: 300.8357\n",
      "Epoch 1178/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 178.9920 - val_loss: 166.7044\n",
      "Epoch 1179/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.1639 - val_loss: 171.5579\n",
      "Epoch 1180/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.1690 - val_loss: 152.0536\n",
      "Epoch 1181/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.1945 - val_loss: 153.5253\n",
      "Epoch 1182/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.3756 - val_loss: 145.5637\n",
      "Epoch 1183/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 87.0088 - val_loss: 173.3962\n",
      "Epoch 1184/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.5671 - val_loss: 189.4109\n",
      "Epoch 1185/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 125.5173 - val_loss: 162.3125\n",
      "Epoch 1186/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.9515 - val_loss: 240.6379\n",
      "Epoch 1187/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 149.2252 - val_loss: 153.7812\n",
      "Epoch 1188/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.0373 - val_loss: 156.3796\n",
      "Epoch 1189/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 117.4937 - val_loss: 168.1550\n",
      "Epoch 1190/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 186.3011 - val_loss: 104.7911\n",
      "Epoch 1191/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 144.7274 - val_loss: 110.3844\n",
      "Epoch 1192/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 160.6832 - val_loss: 124.3001\n",
      "Epoch 1193/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 138.1069 - val_loss: 121.8394\n",
      "Epoch 1194/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.5685 - val_loss: 129.1914\n",
      "Epoch 1195/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 130.9909 - val_loss: 122.4983\n",
      "Epoch 1196/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.2626 - val_loss: 140.7521\n",
      "Epoch 1197/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 153.1793 - val_loss: 148.4760\n",
      "Epoch 1198/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.2376 - val_loss: 135.1707\n",
      "Epoch 1199/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.9015 - val_loss: 125.0987\n",
      "Epoch 1200/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.5512 - val_loss: 151.5459\n",
      "Epoch 1201/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.5576 - val_loss: 147.1873\n",
      "Epoch 1202/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 174.8554 - val_loss: 148.1004\n",
      "Epoch 1203/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 178.7426 - val_loss: 150.0691\n",
      "Epoch 1204/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 154.2221 - val_loss: 166.1709\n",
      "Epoch 1205/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 155.6274 - val_loss: 117.6916\n",
      "Epoch 1206/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.5837 - val_loss: 142.6518\n",
      "Epoch 1207/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 146.7153 - val_loss: 139.5072\n",
      "Epoch 1208/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.4075 - val_loss: 190.8046\n",
      "Epoch 1209/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.1183 - val_loss: 137.6820\n",
      "Epoch 1210/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 120.8686 - val_loss: 201.1647\n",
      "Epoch 1211/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.3177 - val_loss: 220.4628\n",
      "Epoch 1212/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.4534 - val_loss: 194.9767\n",
      "Epoch 1213/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 111.2899 - val_loss: 167.6019\n",
      "Epoch 1214/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.8154 - val_loss: 105.2293\n",
      "Epoch 1215/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 229.8599 - val_loss: 107.9588\n",
      "Epoch 1216/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.7512 - val_loss: 144.7970\n",
      "Epoch 1217/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 131.0758 - val_loss: 154.3288\n",
      "Epoch 1218/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.0829 - val_loss: 141.2508\n",
      "Epoch 1219/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.5423 - val_loss: 118.0308\n",
      "Epoch 1220/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.1044 - val_loss: 132.3279\n",
      "Epoch 1221/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 133.8822 - val_loss: 116.2401\n",
      "Epoch 1222/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.2452 - val_loss: 117.0074\n",
      "Epoch 1223/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.7988 - val_loss: 153.6152\n",
      "Epoch 1224/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.3453 - val_loss: 149.3585\n",
      "Epoch 1225/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.0163 - val_loss: 145.6111\n",
      "Epoch 1226/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 130.1900 - val_loss: 170.3976\n",
      "Epoch 1227/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.9072 - val_loss: 186.9119\n",
      "Epoch 1228/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.6413 - val_loss: 208.1931\n",
      "Epoch 1229/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.5448 - val_loss: 156.7398\n",
      "Epoch 1230/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.9440 - val_loss: 166.0006\n",
      "Epoch 1231/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 141.7629 - val_loss: 133.5686\n",
      "Epoch 1232/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.9886 - val_loss: 155.3669\n",
      "Epoch 1233/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.2128 - val_loss: 158.3394\n",
      "Epoch 1234/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 134.2376 - val_loss: 132.5905\n",
      "Epoch 1235/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.3958 - val_loss: 150.8453\n",
      "Epoch 1236/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 102.0750 - val_loss: 168.9680\n",
      "Epoch 1237/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 95.4324 - val_loss: 164.4106\n",
      "Epoch 1238/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.8816 - val_loss: 149.0199\n",
      "Epoch 1239/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 94.7989 - val_loss: 156.4857\n",
      "Epoch 1240/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.7844 - val_loss: 141.1257\n",
      "Epoch 1241/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 187.5535 - val_loss: 131.4293\n",
      "Epoch 1242/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 171.6497 - val_loss: 142.1544\n",
      "Epoch 1243/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 189.7284 - val_loss: 111.6628\n",
      "Epoch 1244/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.0592 - val_loss: 162.0258\n",
      "Epoch 1245/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 132.0631 - val_loss: 156.2585\n",
      "Epoch 1246/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 106.8713 - val_loss: 164.4575\n",
      "Epoch 1247/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.9377 - val_loss: 150.4941\n",
      "Epoch 1248/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.3123 - val_loss: 207.4978\n",
      "Epoch 1249/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 109.2280 - val_loss: 182.1020\n",
      "Epoch 1250/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.3758 - val_loss: 172.4947\n",
      "Epoch 1251/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.1507 - val_loss: 164.0880\n",
      "Epoch 1252/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.0513 - val_loss: 149.9314\n",
      "Epoch 1253/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.9142 - val_loss: 184.3367\n",
      "Epoch 1254/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.6642 - val_loss: 176.5805\n",
      "Epoch 1255/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.9474 - val_loss: 162.2104\n",
      "Epoch 1256/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 103.6934 - val_loss: 156.1279\n",
      "Epoch 1257/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.6138 - val_loss: 192.7160\n",
      "Epoch 1258/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.8180 - val_loss: 187.0251\n",
      "Epoch 1259/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.4666 - val_loss: 184.1819\n",
      "Epoch 1260/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 128.4786 - val_loss: 171.1381\n",
      "Epoch 1261/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 120.0984 - val_loss: 161.5079\n",
      "Epoch 1262/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 113.0287 - val_loss: 165.0031\n",
      "Epoch 1263/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.8312 - val_loss: 153.8493\n",
      "Epoch 1264/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 102.2553 - val_loss: 161.1545\n",
      "Epoch 1265/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.8629 - val_loss: 148.6398\n",
      "Epoch 1266/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.8555 - val_loss: 159.1760\n",
      "Epoch 1267/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 96.9444 - val_loss: 156.0104\n",
      "Epoch 1268/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.3377 - val_loss: 153.2722\n",
      "Epoch 1269/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 129.6329 - val_loss: 142.3368\n",
      "Epoch 1270/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.5348 - val_loss: 163.5510\n",
      "Epoch 1271/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.7462 - val_loss: 156.8230\n",
      "Epoch 1272/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 121.0495 - val_loss: 158.4926\n",
      "Epoch 1273/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 124.9174 - val_loss: 174.6633\n",
      "Epoch 1274/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 117.7330 - val_loss: 129.8001\n",
      "Epoch 1275/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.4405 - val_loss: 122.0238\n",
      "Epoch 1276/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 119.8759 - val_loss: 152.0380\n",
      "Epoch 1277/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 116.4332 - val_loss: 150.4773\n",
      "Epoch 1278/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.7969 - val_loss: 119.3765\n",
      "Epoch 1279/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 109.9117 - val_loss: 166.1198\n",
      "Epoch 1280/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.6401 - val_loss: 152.3812\n",
      "Epoch 1281/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.6883 - val_loss: 154.9697\n",
      "Epoch 1282/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.3194 - val_loss: 122.4719\n",
      "Epoch 1283/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.2698 - val_loss: 113.0530\n",
      "Epoch 1284/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 130.4358 - val_loss: 127.8711\n",
      "Epoch 1285/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.9326 - val_loss: 134.4562\n",
      "Epoch 1286/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.8949 - val_loss: 170.6806\n",
      "Epoch 1287/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 120.8999 - val_loss: 200.0329\n",
      "Epoch 1288/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.5597 - val_loss: 192.6452\n",
      "Epoch 1289/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 102.7709 - val_loss: 177.2456\n",
      "Epoch 1290/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 93.6103 - val_loss: 208.9743\n",
      "Epoch 1291/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.5182 - val_loss: 195.5360\n",
      "Epoch 1292/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 96.1239 - val_loss: 186.4794\n",
      "Epoch 1293/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.3808 - val_loss: 190.5119\n",
      "Epoch 1294/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 105.8919 - val_loss: 183.8148\n",
      "Epoch 1295/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.2721 - val_loss: 201.7058\n",
      "Epoch 1296/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.2343 - val_loss: 181.5877\n",
      "Epoch 1297/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.7944 - val_loss: 173.8881\n",
      "Epoch 1298/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 96.0625 - val_loss: 171.1079\n",
      "Epoch 1299/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 93.9969 - val_loss: 169.1874\n",
      "Epoch 1300/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 92.7205 - val_loss: 188.2359\n",
      "Epoch 1301/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.9794 - val_loss: 173.4541\n",
      "Epoch 1302/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.3544 - val_loss: 178.0148\n",
      "Epoch 1303/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.9997 - val_loss: 233.8359\n",
      "Epoch 1304/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.2864 - val_loss: 172.9633\n",
      "Epoch 1305/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 107.7378 - val_loss: 136.1937\n",
      "Epoch 1306/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 109.3228 - val_loss: 161.0080\n",
      "Epoch 1307/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.5371 - val_loss: 137.2210\n",
      "Epoch 1308/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.8150 - val_loss: 124.2911\n",
      "Epoch 1309/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.3001 - val_loss: 110.9359\n",
      "Epoch 1310/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.7198 - val_loss: 152.8287\n",
      "Epoch 1311/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.9835 - val_loss: 140.7125\n",
      "Epoch 1312/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 103.0657 - val_loss: 212.5469\n",
      "Epoch 1313/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.6306 - val_loss: 246.9938\n",
      "Epoch 1314/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 118.5932 - val_loss: 194.6604\n",
      "Epoch 1315/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 100.0526 - val_loss: 179.2554\n",
      "Epoch 1316/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 132.1791 - val_loss: 123.7121\n",
      "Epoch 1317/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.8431 - val_loss: 163.7762\n",
      "Epoch 1318/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.4583 - val_loss: 181.2933\n",
      "Epoch 1319/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.9162 - val_loss: 202.2448\n",
      "Epoch 1320/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.3224 - val_loss: 197.5253\n",
      "Epoch 1321/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 103.1292 - val_loss: 125.2228\n",
      "Epoch 1322/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 101.4497 - val_loss: 135.0291\n",
      "Epoch 1323/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 99.1349 - val_loss: 139.5403\n",
      "Epoch 1324/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 95.8223 - val_loss: 135.0615\n",
      "Epoch 1325/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 92.6512 - val_loss: 193.5615\n",
      "Epoch 1326/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 93.2088 - val_loss: 196.8298\n",
      "Epoch 1327/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 99.3985 - val_loss: 196.2277\n",
      "Epoch 1328/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 94.9095 - val_loss: 229.8656\n",
      "Epoch 1329/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 100.3920 - val_loss: 181.7859\n",
      "Epoch 1330/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.6436 - val_loss: 190.5266\n",
      "Epoch 1331/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.5919 - val_loss: 200.8170\n",
      "Epoch 1332/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 94.5581 - val_loss: 190.4924\n",
      "Epoch 1333/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 93.6020 - val_loss: 189.6623\n",
      "Epoch 1334/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.9755 - val_loss: 173.3819\n",
      "Epoch 1335/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.9356 - val_loss: 174.2552\n",
      "Epoch 1336/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 99.2891 - val_loss: 205.8345\n",
      "Epoch 1337/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.2285 - val_loss: 178.4945\n",
      "Epoch 1338/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 96.1540 - val_loss: 185.3098\n",
      "Epoch 1339/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 94.5214 - val_loss: 188.7963\n",
      "Epoch 1340/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 92.1413 - val_loss: 202.3009\n",
      "Epoch 1341/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.5765 - val_loss: 223.2499\n",
      "Epoch 1342/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 92.3372 - val_loss: 203.2011\n",
      "Epoch 1343/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 93.3814 - val_loss: 229.0277\n",
      "Epoch 1344/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 101.0951 - val_loss: 212.1471\n",
      "Epoch 1345/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 92.3365 - val_loss: 202.6690\n",
      "Epoch 1346/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.4346 - val_loss: 196.7873\n",
      "Epoch 1347/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 93.8505 - val_loss: 186.4540\n",
      "Epoch 1348/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.1645 - val_loss: 191.4592\n",
      "Epoch 1349/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 92.9928 - val_loss: 195.0517\n",
      "Epoch 1350/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 91.5236 - val_loss: 193.0565\n",
      "Epoch 1351/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 94.4260 - val_loss: 194.3845\n",
      "Epoch 1352/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 92.6475 - val_loss: 194.7737\n",
      "Epoch 1353/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 89.8559 - val_loss: 210.1999\n",
      "Epoch 1354/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 115.0510 - val_loss: 188.1177\n",
      "Epoch 1355/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.0287 - val_loss: 165.6539\n",
      "Epoch 1356/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.8822 - val_loss: 169.7973\n",
      "Epoch 1357/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 103.5969 - val_loss: 168.7566\n",
      "Epoch 1358/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.8779 - val_loss: 161.1795\n",
      "Epoch 1359/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.0811 - val_loss: 169.9270\n",
      "Epoch 1360/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.4183 - val_loss: 176.9259\n",
      "Epoch 1361/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 100.2941 - val_loss: 170.7511\n",
      "Epoch 1362/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.6653 - val_loss: 173.4805\n",
      "Epoch 1363/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 102.9366 - val_loss: 200.0030\n",
      "Epoch 1364/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 100.5067 - val_loss: 178.3135\n",
      "Epoch 1365/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 96.1178 - val_loss: 255.4447\n",
      "Epoch 1366/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.2204 - val_loss: 179.0803\n",
      "Epoch 1367/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.1234 - val_loss: 178.8918\n",
      "Epoch 1368/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 99.5076 - val_loss: 203.6880\n",
      "Epoch 1369/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 100.8737 - val_loss: 195.0395\n",
      "Epoch 1370/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.0312 - val_loss: 177.0799\n",
      "Epoch 1371/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 122.1630 - val_loss: 156.6517\n",
      "Epoch 1372/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 110.0049 - val_loss: 170.0161\n",
      "Epoch 1373/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 143.6969 - val_loss: 132.2198\n",
      "Epoch 1374/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.3017 - val_loss: 166.3467\n",
      "Epoch 1375/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 116.8433 - val_loss: 138.8748\n",
      "Epoch 1376/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 125.1504 - val_loss: 106.3731\n",
      "Epoch 1377/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 112.7495 - val_loss: 169.3938\n",
      "Epoch 1378/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.4875 - val_loss: 165.3081\n",
      "Epoch 1379/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.5129 - val_loss: 164.8906\n",
      "Epoch 1380/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.2437 - val_loss: 164.3225\n",
      "Epoch 1381/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.7051 - val_loss: 173.9715\n",
      "Epoch 1382/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 95.4025 - val_loss: 176.5827\n",
      "Epoch 1383/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 96.1103 - val_loss: 176.2880\n",
      "Epoch 1384/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.1118 - val_loss: 116.3609\n",
      "Epoch 1385/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.5225 - val_loss: 113.4053\n",
      "Epoch 1386/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 102.7414 - val_loss: 124.9816\n",
      "Epoch 1387/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.1423 - val_loss: 198.4565\n",
      "Epoch 1388/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.0560 - val_loss: 162.3351\n",
      "Epoch 1389/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.0618 - val_loss: 169.2957\n",
      "Epoch 1390/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.9483 - val_loss: 158.2408\n",
      "Epoch 1391/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 113.8460 - val_loss: 162.3727\n",
      "Epoch 1392/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.7764 - val_loss: 160.8362\n",
      "Epoch 1393/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.4285 - val_loss: 149.3878\n",
      "Epoch 1394/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.7746 - val_loss: 146.7565\n",
      "Epoch 1395/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 114.7934 - val_loss: 123.3213\n",
      "Epoch 1396/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 107.6541 - val_loss: 142.0422\n",
      "Epoch 1397/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.5915 - val_loss: 124.0541\n",
      "Epoch 1398/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 102.5340 - val_loss: 148.2626\n",
      "Epoch 1399/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 104.3574 - val_loss: 163.1384\n",
      "Epoch 1400/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 105.9132 - val_loss: 176.9984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1401/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 100.9696 - val_loss: 182.2289\n",
      "Epoch 1402/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.8270 - val_loss: 277.0882\n",
      "Epoch 1403/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 129.8963 - val_loss: 185.9775\n",
      "Epoch 1404/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.8701 - val_loss: 138.0337\n",
      "Epoch 1405/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.2255 - val_loss: 112.7759\n",
      "Epoch 1406/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 118.0454 - val_loss: 111.4722\n",
      "Epoch 1407/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.5180 - val_loss: 134.9939\n",
      "Epoch 1408/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.6529 - val_loss: 145.2676\n",
      "Epoch 1409/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 123.1478 - val_loss: 153.2271\n",
      "Epoch 1410/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 153.2168 - val_loss: 143.4256\n",
      "Epoch 1411/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 149.7477 - val_loss: 165.1232\n",
      "Epoch 1412/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 189.6320 - val_loss: 135.6628\n",
      "Epoch 1413/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 172.4971 - val_loss: 172.0047\n",
      "Epoch 1414/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 164.2961 - val_loss: 148.6878\n",
      "Epoch 1415/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 159.1944 - val_loss: 219.6556\n",
      "Epoch 1416/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 148.8410 - val_loss: 180.9205\n",
      "Epoch 1417/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 148.8938 - val_loss: 154.8646\n",
      "Epoch 1418/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 198.1178 - val_loss: 173.8500\n",
      "Epoch 1419/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 140.2129 - val_loss: 126.0760\n",
      "Epoch 1420/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 125.5692 - val_loss: 182.3888\n",
      "Epoch 1421/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.8350 - val_loss: 173.7389\n",
      "Epoch 1422/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.3904 - val_loss: 185.8904\n",
      "Epoch 1423/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.4774 - val_loss: 130.9428\n",
      "Epoch 1424/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.0930 - val_loss: 156.1904\n",
      "Epoch 1425/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.4732 - val_loss: 129.4404\n",
      "Epoch 1426/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 95.3610 - val_loss: 133.1548\n",
      "Epoch 1427/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.4983 - val_loss: 129.4709\n",
      "Epoch 1428/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 97.3102 - val_loss: 119.9874\n",
      "Epoch 1429/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 93.3581 - val_loss: 133.5867\n",
      "Epoch 1430/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 104.4680 - val_loss: 140.1160\n",
      "Epoch 1431/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.7097 - val_loss: 134.9574\n",
      "Epoch 1432/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.9348 - val_loss: 140.5605\n",
      "Epoch 1433/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.1191 - val_loss: 122.9862\n",
      "Epoch 1434/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.4426 - val_loss: 134.7348\n",
      "Epoch 1435/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 121.6135 - val_loss: 176.2678\n",
      "Epoch 1436/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 127.9968 - val_loss: 174.6230\n",
      "Epoch 1437/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.6159 - val_loss: 180.6357\n",
      "Epoch 1438/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 112.6087 - val_loss: 131.0208\n",
      "Epoch 1439/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 97.4986 - val_loss: 144.3221\n",
      "Epoch 1440/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.7194 - val_loss: 145.3083\n",
      "Epoch 1441/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 104.2515 - val_loss: 143.6843\n",
      "Epoch 1442/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.2674 - val_loss: 147.1367\n",
      "Epoch 1443/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 102.6021 - val_loss: 175.0358\n",
      "Epoch 1444/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.2616 - val_loss: 186.5291\n",
      "Epoch 1445/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.3639 - val_loss: 211.5234\n",
      "Epoch 1446/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 107.6068 - val_loss: 209.5166\n",
      "Epoch 1447/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 101.3836 - val_loss: 198.0187\n",
      "Epoch 1448/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 99.1704 - val_loss: 185.0143\n",
      "Epoch 1449/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 108.5837 - val_loss: 169.7608\n",
      "Epoch 1450/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 106.7036 - val_loss: 120.9009\n",
      "Epoch 1451/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.0896 - val_loss: 238.6892\n",
      "Epoch 1452/10000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 221.8789 - val_loss: 195.7984\n",
      "Epoch 1453/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 213.2733 - val_loss: 113.5671\n",
      "Epoch 1454/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 171.7429 - val_loss: 113.3896\n",
      "Epoch 1455/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 145.7655 - val_loss: 111.5991\n",
      "Epoch 1456/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 152.6895 - val_loss: 90.6733\n",
      "Epoch 1457/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 170.8019 - val_loss: 70.2144\n",
      "Epoch 1458/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 157.2267 - val_loss: 120.5949\n",
      "Epoch 1459/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 161.3978 - val_loss: 124.6649\n",
      "Epoch 1460/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 154.2272 - val_loss: 138.6427\n",
      "Epoch 1461/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 157.8560 - val_loss: 185.1412\n",
      "Epoch 1462/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 159.0148 - val_loss: 174.1450\n",
      "Epoch 1463/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.9513 - val_loss: 149.5535\n",
      "Epoch 1464/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 111.6415 - val_loss: 177.2232\n",
      "Epoch 1465/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 117.9650 - val_loss: 170.3448\n",
      "Epoch 1466/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 119.2986 - val_loss: 174.5238\n",
      "Epoch 1467/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 119.1149 - val_loss: 218.5377\n",
      "Epoch 1468/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 110.3874 - val_loss: 264.7736\n",
      "Epoch 1469/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 136.1001 - val_loss: 159.9757\n",
      "Epoch 1470/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 124.9045 - val_loss: 199.9615\n",
      "Epoch 1471/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 108.7706 - val_loss: 168.8444\n",
      "Epoch 1472/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 103.8233 - val_loss: 152.3582\n",
      "Epoch 1473/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 100.0483 - val_loss: 222.2855\n",
      "Epoch 1474/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 134.6395 - val_loss: 176.3686\n",
      "Epoch 1475/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 114.9481 - val_loss: 185.5422\n",
      "Epoch 1476/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 96.9742 - val_loss: 172.8236\n",
      "Epoch 1477/10000\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 98.4425 - val_loss: 165.5529\n",
      "Epoch 1478/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 107.5062 - val_loss: 198.8039\n",
      "Epoch 1479/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 115.0577 - val_loss: 185.2336\n",
      "Epoch 1480/10000\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 98.6944 - val_loss: 198.4732\n",
      "Epoch 1481/10000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 108.2098Restoring model weights from the end of the best epoch: 981.\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 97.3759 - val_loss: 182.8431\n",
      "Epoch 1481: early stopping\n"
     ]
    }
   ],
   "source": [
    "trained_model, history = neural_network_model(train_input, \n",
    "                                              train_target, \n",
    "                                              want_verbose=1, \n",
    "                                              seed=winner_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "867771c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_mape_calculator(model, test_input, test_target, start_target):\n",
    "    \n",
    "    errors = []\n",
    "    error_percent = []\n",
    "    results_data = []\n",
    "\n",
    "    for i in range(len(test_target)):\n",
    "        prediction = model.predict(test_input.iloc[i:i+1])\n",
    "        target = test_target[start_target + i]\n",
    "        error = np.abs(prediction - target)\n",
    "        errors.append(error)\n",
    "        error_percent.append(error/target)\n",
    "        results_data.append([f\"Month-{i + 1}\", \n",
    "                             prediction[0][0], \n",
    "                             target, \n",
    "                             error[0][0]])\n",
    "\n",
    "    df_results = pd.DataFrame(results_data, columns=[\"Month\", \"Prediction\", \"Target\", \"Error\"])\n",
    "\n",
    "    mae = np.mean(errors)\n",
    "    mape = np.mean(error_percent) \n",
    "\n",
    "    return df_results, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09914e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>Month-1</td>\n",
       "      <td>Month-2</td>\n",
       "      <td>Month-3</td>\n",
       "      <td>Month-4</td>\n",
       "      <td>Month-5</td>\n",
       "      <td>Month-6</td>\n",
       "      <td>Month-7</td>\n",
       "      <td>Month-8</td>\n",
       "      <td>Month-9</td>\n",
       "      <td>Month-10</td>\n",
       "      <td>Month-11</td>\n",
       "      <td>Month-12</td>\n",
       "      <td>Month-13</td>\n",
       "      <td>Month-14</td>\n",
       "      <td>Month-15</td>\n",
       "      <td>Month-16</td>\n",
       "      <td>Month-17</td>\n",
       "      <td>Month-18</td>\n",
       "      <td>Month-19</td>\n",
       "      <td>Month-20</td>\n",
       "      <td>Month-21</td>\n",
       "      <td>Month-22</td>\n",
       "      <td>Month-23</td>\n",
       "      <td>Month-24</td>\n",
       "      <td>Month-25</td>\n",
       "      <td>Month-26</td>\n",
       "      <td>Month-27</td>\n",
       "      <td>Month-28</td>\n",
       "      <td>Month-29</td>\n",
       "      <td>Month-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction</th>\n",
       "      <td>92.779861</td>\n",
       "      <td>92.764565</td>\n",
       "      <td>92.719574</td>\n",
       "      <td>92.55719</td>\n",
       "      <td>80.586494</td>\n",
       "      <td>78.824364</td>\n",
       "      <td>79.213287</td>\n",
       "      <td>78.585823</td>\n",
       "      <td>80.060463</td>\n",
       "      <td>81.054634</td>\n",
       "      <td>83.144043</td>\n",
       "      <td>91.911934</td>\n",
       "      <td>82.901215</td>\n",
       "      <td>81.475563</td>\n",
       "      <td>92.801308</td>\n",
       "      <td>91.332153</td>\n",
       "      <td>96.144127</td>\n",
       "      <td>81.36525</td>\n",
       "      <td>79.375511</td>\n",
       "      <td>92.53215</td>\n",
       "      <td>92.043335</td>\n",
       "      <td>91.589127</td>\n",
       "      <td>97.542534</td>\n",
       "      <td>97.686256</td>\n",
       "      <td>97.724495</td>\n",
       "      <td>97.724503</td>\n",
       "      <td>97.72448</td>\n",
       "      <td>97.724472</td>\n",
       "      <td>97.724358</td>\n",
       "      <td>89.772186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>120.055</td>\n",
       "      <td>125.769</td>\n",
       "      <td>112.904</td>\n",
       "      <td>123.201</td>\n",
       "      <td>112.725</td>\n",
       "      <td>90.828</td>\n",
       "      <td>96.36</td>\n",
       "      <td>86.444</td>\n",
       "      <td>94.285</td>\n",
       "      <td>98.986</td>\n",
       "      <td>88.072</td>\n",
       "      <td>141.01</td>\n",
       "      <td>122.652</td>\n",
       "      <td>142.145</td>\n",
       "      <td>121.124</td>\n",
       "      <td>130.503</td>\n",
       "      <td>104.115</td>\n",
       "      <td>90.69</td>\n",
       "      <td>102.685</td>\n",
       "      <td>96.144</td>\n",
       "      <td>102.197</td>\n",
       "      <td>106.712</td>\n",
       "      <td>124.057</td>\n",
       "      <td>124.625</td>\n",
       "      <td>133.116</td>\n",
       "      <td>144.31</td>\n",
       "      <td>140.357</td>\n",
       "      <td>152.769</td>\n",
       "      <td>124.038</td>\n",
       "      <td>95.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error</th>\n",
       "      <td>27.275139</td>\n",
       "      <td>33.004433</td>\n",
       "      <td>20.184425</td>\n",
       "      <td>30.643806</td>\n",
       "      <td>32.138504</td>\n",
       "      <td>12.003639</td>\n",
       "      <td>17.146713</td>\n",
       "      <td>7.858177</td>\n",
       "      <td>14.224541</td>\n",
       "      <td>17.931366</td>\n",
       "      <td>4.927956</td>\n",
       "      <td>49.098061</td>\n",
       "      <td>39.750786</td>\n",
       "      <td>60.669441</td>\n",
       "      <td>28.322693</td>\n",
       "      <td>39.170853</td>\n",
       "      <td>7.970871</td>\n",
       "      <td>9.324753</td>\n",
       "      <td>23.309486</td>\n",
       "      <td>3.611847</td>\n",
       "      <td>10.153664</td>\n",
       "      <td>15.122871</td>\n",
       "      <td>26.514465</td>\n",
       "      <td>26.938744</td>\n",
       "      <td>35.391502</td>\n",
       "      <td>46.585495</td>\n",
       "      <td>42.632515</td>\n",
       "      <td>55.044525</td>\n",
       "      <td>26.313644</td>\n",
       "      <td>5.281815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0          1          2          3          4          5   \\\n",
       "Month         Month-1    Month-2    Month-3    Month-4    Month-5    Month-6   \n",
       "Prediction  92.779861  92.764565  92.719574   92.55719  80.586494  78.824364   \n",
       "Target        120.055    125.769    112.904    123.201    112.725     90.828   \n",
       "Error       27.275139  33.004433  20.184425  30.643806  32.138504  12.003639   \n",
       "\n",
       "                   6          7          8          9          10         11  \\\n",
       "Month         Month-7    Month-8    Month-9   Month-10   Month-11   Month-12   \n",
       "Prediction  79.213287  78.585823  80.060463  81.054634  83.144043  91.911934   \n",
       "Target          96.36     86.444     94.285     98.986     88.072     141.01   \n",
       "Error       17.146713   7.858177  14.224541  17.931366   4.927956  49.098061   \n",
       "\n",
       "                   12         13         14         15         16        17  \\\n",
       "Month        Month-13   Month-14   Month-15   Month-16   Month-17  Month-18   \n",
       "Prediction  82.901215  81.475563  92.801308  91.332153  96.144127  81.36525   \n",
       "Target        122.652    142.145    121.124    130.503    104.115     90.69   \n",
       "Error       39.750786  60.669441  28.322693  39.170853   7.970871  9.324753   \n",
       "\n",
       "                   18        19         20         21         22         23  \\\n",
       "Month        Month-19  Month-20   Month-21   Month-22   Month-23   Month-24   \n",
       "Prediction  79.375511  92.53215  92.043335  91.589127  97.542534  97.686256   \n",
       "Target        102.685    96.144    102.197    106.712    124.057    124.625   \n",
       "Error       23.309486  3.611847  10.153664  15.122871  26.514465  26.938744   \n",
       "\n",
       "                   24         25         26         27         28         29  \n",
       "Month        Month-25   Month-26   Month-27   Month-28   Month-29   Month-30  \n",
       "Prediction  97.724495  97.724503   97.72448  97.724472  97.724358  89.772186  \n",
       "Target        133.116     144.31    140.357    152.769    124.038     95.054  \n",
       "Error       35.391502  46.585495  42.632515  55.044525  26.313644   5.281815  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results, mae, mape = mae_mape_calculator(trained_model, \n",
    "                                            test_input, \n",
    "                                            test_target, \n",
    "                                            train_split)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16807ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.618225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2086544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(mae)\n",
    "display(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "447b6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_mae_mape_calculator(model, test_input, test_target, start_target):\n",
    "    \n",
    "    errors = []\n",
    "    error_percent = []\n",
    "    \n",
    "    target_sum = 0\n",
    "    prediction_sum = 0\n",
    "    \n",
    "    for i in range(len(test_target)):\n",
    "        if i % 12 == 0 and i != 0:\n",
    "            error = np.abs(target_sum - prediction_sum)\n",
    "            errors.append(error)\n",
    "            error_percent.append(error / target_sum)\n",
    "            print(f\"Ano-{i%12}: |Prediction{prediction_sum} - Target[{target_sum}]| =  Error: {error}; MAPE:{abs(prediction_sum - target_sum)/target_sum}\")\n",
    "            target_sum = 0\n",
    "            prediction_sum = 0\n",
    "            \n",
    "        prediction = model.predict(test_input.iloc[i:i+1])\n",
    "        target_sum += test_target[start_target + i]\n",
    "        prediction_sum += prediction\n",
    "        \n",
    "    error = np.abs(target_sum - prediction_sum)\n",
    "    errors.append(error)\n",
    "    error_percent.append(error / target_sum)\n",
    "    print(f\"Ano-{i%12}: |Prediction{prediction_sum} - Target[{target_sum}]| =  Error: {error}; MAPE:{abs(prediction_sum - target_sum)/target_sum}\")\n",
    "        \n",
    "    mae = np.mean(errors)\n",
    "    mape = np.mean(error_percent) \n",
    "\n",
    "    return errors, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04d784f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ano-0: |Prediction[[1024.2023]] - Target[1290.639]| =  Error: [[266.43677]]; MAPE:[[0.20643787]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Ano-0: |Prediction[[1076.7885]] - Target[1367.6490000000001]| =  Error: [[290.8606]]; MAPE:[[0.21267195]]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ano-5: |Prediction[[578.39453]] - Target[789.644]| =  Error: [[211.24945]]; MAPE:[[0.26752493]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[266.43677]], dtype=float32),\n",
       " array([[290.8606]], dtype=float32),\n",
       " array([[211.24945]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "256.18228"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.22887826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors, mae, mape = year_mae_mape_calculator(trained_model, \n",
    "                                             test_input, \n",
    "                                             test_target, \n",
    "                                             train_split)\n",
    "display(errors)\n",
    "display(mae)\n",
    "display(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee49d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
